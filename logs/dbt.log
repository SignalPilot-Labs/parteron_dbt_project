[0m14:23:35.891280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107aee2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad361e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad35a90>]}


============================== 14:23:35.895166 | 8b914c21-5fc9-440c-8c50-ac3ea93e9753 ==============================
[0m14:23:35.895166 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:23:35.895520 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'quiet': 'False', 'static_parser': 'True', 'no_print': 'None', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'log_format': 'default', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'partial_parse': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt debug', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'log_cache_events': 'False', 'warn_error': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'use_colors': 'True', 'debug': 'False'}
[0m14:23:35.907038 [info ] [MainThread]: dbt version: 1.11.0-rc3
[0m14:23:35.907331 [info ] [MainThread]: python version: 3.12.12
[0m14:23:35.907523 [info ] [MainThread]: python path: /Users/tarik/miniconda3/envs/sp2/bin/python3.12
[0m14:23:35.907669 [info ] [MainThread]: os info: macOS-26.1-arm64-arm-64bit
[0m14:23:35.911677 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_HOST'
[0m14:23:35.913419 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.072633795, "process_in_blocks": "0", "process_kernel_time": 0.16608, "process_mem_max_rss": "132808704", "process_out_blocks": "0", "process_user_time": 1.354975}
[0m14:23:35.913708 [debug] [MainThread]: Command `dbt debug` failed at 14:23:35.913656 after 0.07 seconds
[0m14:23:35.913903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad35dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8e3c80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10acce7b0>]}
[0m14:23:35.914091 [debug] [MainThread]: Flushing usage events
[0m14:23:36.388238 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:24:03.392556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1291545f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12942e7e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12942e0c0>]}


============================== 14:24:03.396234 | 9f24fad4-c8ce-4ef0-8441-bebf09789d0c ==============================
[0m14:24:03.396234 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:24:03.396566 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'no_print': 'None', 'static_parser': 'True', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'use_colors': 'True', 'quiet': 'False', 'introspect': 'True', 'use_experimental_parser': 'False', 'printer_width': '80', 'version_check': 'True', 'log_format': 'default', 'empty': 'None', 'target_path': 'None', 'fail_fast': 'False', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'debug': 'False', 'warn_error': 'None', 'indirect_selection': 'eager'}
[0m14:24:03.407533 [info ] [MainThread]: dbt version: 1.11.0-rc3
[0m14:24:03.407871 [info ] [MainThread]: python version: 3.12.12
[0m14:24:03.408083 [info ] [MainThread]: python path: /Users/tarik/miniconda3/envs/sp2/bin/python3.12
[0m14:24:03.408220 [info ] [MainThread]: os info: macOS-26.1-arm64-arm-64bit
[0m14:24:03.411675 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_HOST'
[0m14:24:03.413638 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.06412275, "process_in_blocks": "0", "process_kernel_time": 0.184678, "process_mem_max_rss": "133726208", "process_out_blocks": "0", "process_user_time": 1.386537}
[0m14:24:03.414015 [debug] [MainThread]: Command `dbt debug` failed at 14:24:03.413960 after 0.06 seconds
[0m14:24:03.414227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1280ffd40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129013470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1293cf320>]}
[0m14:24:03.414448 [debug] [MainThread]: Flushing usage events
[0m14:24:03.708552 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:00.251911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062c55b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10741f920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10741f1d0>]}


============================== 14:26:00.255753 | a910987d-5732-48da-afeb-5e88506019c0 ==============================
[0m14:26:00.255753 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:26:00.256147 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'log_format': 'default', 'use_colors': 'True', 'static_parser': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'quiet': 'False', 'target_path': 'None', 'invocation_command': 'dbt debug', 'warn_error': 'None', 'empty': 'None', 'indirect_selection': 'eager', 'no_print': 'None', 'printer_width': '80', 'fail_fast': 'False', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'debug': 'False', 'version_check': 'True', 'write_json': 'True'}
[0m14:26:00.267111 [info ] [MainThread]: dbt version: 1.11.0-rc3
[0m14:26:00.267410 [info ] [MainThread]: python version: 3.12.12
[0m14:26:00.267566 [info ] [MainThread]: python path: /Users/tarik/miniconda3/envs/sp2/bin/python3.12
[0m14:26:00.267708 [info ] [MainThread]: os info: macOS-26.1-arm64-arm-64bit
[0m14:26:00.769165 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:26:00.769500 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:26:00.769670 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:26:08.956789 [info ] [MainThread]: Using profiles dir at /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project
[0m14:26:08.957329 [info ] [MainThread]: Using profiles.yml file at /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/profiles.yml
[0m14:26:08.957644 [info ] [MainThread]: Using dbt_project.yml file at /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/dbt_project.yml
[0m14:26:08.957808 [info ] [MainThread]: adapter type: databricks
[0m14:26:08.957945 [info ] [MainThread]: adapter version: 1.11.3
[0m14:26:09.014566 [info ] [MainThread]: Configuration:
[0m14:26:09.014860 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:26:09.015021 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:26:09.015154 [info ] [MainThread]: Required dependencies:
[0m14:26:09.015359 [debug] [MainThread]: Executing "git --help"
[0m14:26:09.041221 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:26:09.042164 [debug] [MainThread]: STDERR: "b''"
[0m14:26:09.042744 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:26:09.043029 [info ] [MainThread]: Connection:
[0m14:26:09.043292 [info ] [MainThread]:   host: dbc-789165f8-da9f.cloud.databricks.com
[0m14:26:09.043430 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/55eab00736b60422
[0m14:26:09.043559 [info ] [MainThread]:   catalog: patreon_dev
[0m14:26:09.043686 [info ] [MainThread]:   schema: analytics
[0m14:26:09.044061 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:26:09.141473 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:26:09.141981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a910987d-5732-48da-afeb-5e88506019c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129539b20>]}
[0m14:26:09.142393 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m14:26:09.142544 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m14:26:09.142697 [debug] [MainThread]: Using databricks connection "debug"
[0m14:26:09.142842 [debug] [MainThread]: On debug: select 1 as id
[0m14:26:09.142969 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:26:09.929276 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0db75-dc03-1cdb-a43b-2f6d6a591ad1) - Created
[0m14:26:16.748682 [debug] [MainThread]: SQL status: OK in 7.600 seconds
[0m14:26:16.752526 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0db75-dc03-1cdb-a43b-2f6d6a591ad1, command-id=01f0db75-dc2f-1404-83f6-45d6fb07662c) - Closing
[0m14:26:17.020048 [debug] [MainThread]: On debug: Close
[0m14:26:17.020793 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0db75-dc03-1cdb-a43b-2f6d6a591ad1) - Closing
[0m14:26:17.224543 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:26:17.226475 [info ] [MainThread]: [32mAll checks passed![0m
[0m14:26:17.245432 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 17.045319, "process_in_blocks": "0", "process_kernel_time": 0.434783, "process_mem_max_rss": "255655936", "process_out_blocks": "0", "process_user_time": 2.484183}
[0m14:26:17.245898 [debug] [MainThread]: Command `dbt debug` succeeded at 14:26:17.245830 after 17.05 seconds
[0m14:26:17.246186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10627d9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ddc6b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ddd580>]}
[0m14:26:17.246409 [debug] [MainThread]: Flushing usage events
[0m14:26:17.693285 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:25.069919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065baf60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10862b5f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10862afc0>]}


============================== 14:26:25.076427 | 59ab49b4-cdf7-4544-91e4-e395b2433a57 ==============================
[0m14:26:25.076427 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:26:25.076865 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'log_cache_events': 'False', 'warn_error': 'None', 'target_path': 'None', 'indirect_selection': 'eager', 'empty': 'False', 'debug': 'False', 'partial_parse': 'True', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'write_json': 'True', 'fail_fast': 'False', 'version_check': 'True', 'quiet': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'introspect': 'True', 'static_parser': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run'}
[0m14:26:25.570782 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:26:25.571085 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:26:25.571249 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:26:26.195720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '59ab49b4-cdf7-4544-91e4-e395b2433a57', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12854a3c0>]}
[0m14:26:26.227929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '59ab49b4-cdf7-4544-91e4-e395b2433a57', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f42fb30>]}
[0m14:26:26.228465 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:26:26.313547 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:26:26.314172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '59ab49b4-cdf7-4544-91e4-e395b2433a57', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5f9370>]}
[0m14:26:26.314775 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 3 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m14:26:26.317614 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.3003422, "process_in_blocks": "0", "process_kernel_time": 0.349185, "process_mem_max_rss": "250789888", "process_out_blocks": "0", "process_user_time": 2.338386}
[0m14:26:26.318199 [debug] [MainThread]: Command `dbt run` failed at 14:26:26.318143 after 1.30 seconds
[0m14:26:26.318512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10862b2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1295da750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f5c2a20>]}
[0m14:26:26.318707 [debug] [MainThread]: Flushing usage events
[0m14:26:26.613826 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:51.582379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1212d9b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121f2eab0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121f2e3f0>]}


============================== 14:26:51.586103 | d7510fbd-8b9a-4273-9911-b710d108dc51 ==============================
[0m14:26:51.586103 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:26:51.586516 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'warn_error': 'None', 'invocation_command': 'dbt seed', 'quiet': 'False', 'static_parser': 'True', 'printer_width': '80', 'use_colors': 'True', 'partial_parse': 'True', 'log_format': 'default', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'fail_fast': 'False', 'debug': 'False', 'empty': 'None', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'no_print': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'cache_selected_only': 'False', 'introspect': 'True'}
[0m14:26:52.078424 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:26:52.078778 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:26:52.078965 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:26:52.717927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd7510fbd-8b9a-4273-9911-b710d108dc51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121ecf710>]}
[0m14:26:52.750418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd7510fbd-8b9a-4273-9911-b710d108dc51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x132fac740>]}
[0m14:26:52.750971 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:26:52.840907 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:26:52.841460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'd7510fbd-8b9a-4273-9911-b710d108dc51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124791970>]}
[0m14:26:52.841948 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 3 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m14:26:52.844872 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 1.3068702, "process_in_blocks": "0", "process_kernel_time": 0.363692, "process_mem_max_rss": "250658816", "process_out_blocks": "0", "process_user_time": 2.334977}
[0m14:26:52.845526 [debug] [MainThread]: Command `dbt seed` failed at 14:26:52.845461 after 1.31 seconds
[0m14:26:52.845938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102ad2f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x132ffe540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1312332c0>]}
[0m14:26:52.846231 [debug] [MainThread]: Flushing usage events
[0m14:26:53.129964 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:27:29.397396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118747200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c10a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a26540>]}


============================== 14:27:29.401301 | 8ca71847-9fe5-4f99-8a28-be3333589e5e ==============================
[0m14:27:29.401301 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:27:29.401682 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'debug': 'False', 'log_cache_events': 'False', 'printer_width': '80', 'invocation_command': 'dbt deps', 'version_check': 'True', 'quiet': 'False', 'use_colors': 'True', 'static_parser': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'no_print': 'None', 'write_json': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'warn_error': 'None', 'empty': 'None', 'introspect': 'True'}
[0m14:27:29.474690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8ca71847-9fe5-4f99-8a28-be3333589e5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c10a70>]}
[0m14:27:29.490143 [debug] [MainThread]: Set downloads directory='/var/folders/17/44fmtn0931x0k_v05ghc5pl40000gn/T/dbt-downloads-elt753bd'
[0m14:27:29.490542 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m14:27:29.907927 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m14:27:29.910035 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m14:27:30.055401 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m14:27:30.065697 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json
[0m14:27:30.209448 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json 200
[0m14:27:30.212474 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `calogica/dbt_expectations` package is deprecated in favor of
`metaplane/dbt_expectations`. Please update your `packages.yml` configuration to
use `metaplane/dbt_expectations` instead.
[0m14:27:30.212936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '8ca71847-9fe5-4f99-8a28-be3333589e5e', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a24200>]}
[0m14:27:30.215498 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json
[0m14:27:30.352982 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json 200
[0m14:27:30.354888 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m14:27:30.502430 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m14:27:30.520776 [info ] [MainThread]: Updating lock file in file path: /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/package-lock.yml
[0m14:27:30.524712 [debug] [MainThread]: Set downloads directory='/var/folders/17/44fmtn0931x0k_v05ghc5pl40000gn/T/dbt-downloads-4oh63jp6'
[0m14:27:30.530183 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m14:27:31.069436 [info ] [MainThread]: Installed from version 1.3.3
[0m14:27:31.069773 [info ] [MainThread]: Up to date!
[0m14:27:31.070028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8ca71847-9fe5-4f99-8a28-be3333589e5e', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1194a1280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c2bb90>]}
[0m14:27:31.070283 [info ] [MainThread]: Installing calogica/dbt_expectations
[0m14:27:31.858801 [info ] [MainThread]: Installed from version 0.10.4
[0m14:27:31.859088 [info ] [MainThread]: Up to date!
[0m14:27:31.859294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8ca71847-9fe5-4f99-8a28-be3333589e5e', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1195fa0c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119bc1820>]}
[0m14:27:31.859625 [info ] [MainThread]: Installing dbt-labs/codegen
[0m14:27:32.235485 [info ] [MainThread]: Installed from version 0.14.0
[0m14:27:32.235848 [info ] [MainThread]: Up to date!
[0m14:27:32.236130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8ca71847-9fe5-4f99-8a28-be3333589e5e', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119b15310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ad70e0>]}
[0m14:27:32.236427 [info ] [MainThread]: Installing calogica/dbt_date
[0m14:27:32.671818 [info ] [MainThread]: Installed from version 0.10.1
[0m14:27:32.672112 [info ] [MainThread]: Up to date!
[0m14:27:32.672331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8ca71847-9fe5-4f99-8a28-be3333589e5e', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a25550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119cbb6b0>]}
[0m14:27:32.673974 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PackageRedirectDeprecation: 2 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:27:32.676463 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 3.3253007, "process_in_blocks": "0", "process_kernel_time": 0.32223, "process_mem_max_rss": "151126016", "process_out_blocks": "0", "process_user_time": 1.810445}
[0m14:27:32.677083 [debug] [MainThread]: Command `dbt deps` succeeded at 14:27:32.676878 after 3.33 seconds
[0m14:27:32.677529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118747200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1191f13d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119401fa0>]}
[0m14:27:32.677892 [debug] [MainThread]: Flushing usage events
[0m14:27:32.983320 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:27:48.666063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1035a28a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10642ac30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10642a3f0>]}


============================== 14:27:48.668501 | 1c866f36-9fa8-4687-8413-babeb8b54ed1 ==============================
[0m14:27:48.668501 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:27:48.668826 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'printer_width': '80', 'fail_fast': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'write_json': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'version_check': 'True', 'indirect_selection': 'eager', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'use_colors': 'True', 'debug': 'False', 'cache_selected_only': 'False', 'empty': 'None', 'invocation_command': 'dbt seed', 'log_format': 'default', 'introspect': 'True', 'quiet': 'False'}
[0m14:27:49.160259 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:27:49.160576 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:27:49.160751 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:27:49.816393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1c866f36-9fa8-4687-8413-babeb8b54ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2aba70>]}
[0m14:27:49.852465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1c866f36-9fa8-4687-8413-babeb8b54ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051b5310>]}
[0m14:27:49.852968 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:27:49.942969 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:27:49.943551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1c866f36-9fa8-4687-8413-babeb8b54ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10821e450>]}
[0m14:27:49.964587 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m14:27:49.965189 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:27:49.965384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1c866f36-9fa8-4687-8413-babeb8b54ed1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e39b530>]}
[0m14:27:49.975260 [warn ] [MainThread]: [[33mWARNING[0m][PropertyMovedToConfigDeprecation]: Deprecated functionality
Found `meta` as a top-level property of `models[0].columns[0]` in file
`models/marts/core/schema.yml`. The `meta` top-level property should be moved
into the `config` of `models[0].columns[0]`.
[0m14:27:49.975548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '1c866f36-9fa8-4687-8413-babeb8b54ed1', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061fd760>]}
[0m14:27:51.299822 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on 'stg_creators' in
package 'patreon_analytics' (models/staging/schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m14:27:51.300142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '1c866f36-9fa8-4687-8413-babeb8b54ed1', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1198919a0>]}
[0m14:27:51.485609 [error] [MainThread]: Encountered an error:
Parsing Error
  The semantic layer requires a time spine model with granularity DAY or smaller in the project, but none was found. Guidance on creating this model can be found on our docs site (https://docs.getdbt.com/docs/build/metricflow-time-spine).
[0m14:27:51.486060 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PropertyMovedToConfigDeprecation: 42 occurrences
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:27:51.488875 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 2.8639047, "process_in_blocks": "0", "process_kernel_time": 0.400773, "process_mem_max_rss": "267272192", "process_out_blocks": "0", "process_user_time": 3.737272}
[0m14:27:51.489380 [debug] [MainThread]: Command `dbt seed` failed at 14:27:51.489323 after 2.86 seconds
[0m14:27:51.489637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1007a2f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104aadd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118793110>]}
[0m14:27:51.489832 [debug] [MainThread]: Flushing usage events
[0m14:27:51.850737 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:26.670006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1144cf290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11462ea50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11462e0f0>]}


============================== 14:34:26.673718 | d2ccd821-f5c7-4e7d-a010-10f10feb6e0c ==============================
[0m14:34:26.673718 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:34:26.674127 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'debug': 'False', 'write_json': 'True', 'empty': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'cache_selected_only': 'False', 'version_check': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'no_print': 'None', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'introspect': 'True', 'warn_error': 'None', 'printer_width': '80', 'invocation_command': 'dbt deps', 'partial_parse': 'True', 'log_format': 'default'}
[0m14:34:26.771208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd2ccd821-f5c7-4e7d-a010-10f10feb6e0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109439b0>]}
[0m14:34:26.850786 [debug] [MainThread]: Set downloads directory='/var/folders/17/44fmtn0931x0k_v05ghc5pl40000gn/T/dbt-downloads-gz6d0xv6'
[0m14:34:26.851100 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m14:34:27.120735 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m14:34:27.122221 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m14:34:27.264874 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m14:34:27.268314 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json
[0m14:34:27.439323 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json 200
[0m14:34:27.441755 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `calogica/dbt_expectations` package is deprecated in favor of
`metaplane/dbt_expectations`. Please update your `packages.yml` configuration to
use `metaplane/dbt_expectations` instead.
[0m14:34:27.442117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'd2ccd821-f5c7-4e7d-a010-10f10feb6e0c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113f793a0>]}
[0m14:34:27.443470 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json
[0m14:34:27.583320 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json 200
[0m14:34:27.584979 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m14:34:27.790647 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m14:34:27.793016 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m14:34:28.300296 [info ] [MainThread]: Installed from version 1.3.3
[0m14:34:28.300585 [info ] [MainThread]: Up to date!
[0m14:34:28.300795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'd2ccd821-f5c7-4e7d-a010-10f10feb6e0c', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1140e3ce0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11462c620>]}
[0m14:34:28.301024 [info ] [MainThread]: Installing calogica/dbt_expectations
[0m14:34:29.257888 [info ] [MainThread]: Installed from version 0.10.4
[0m14:34:29.258181 [info ] [MainThread]: Up to date!
[0m14:34:29.258385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'd2ccd821-f5c7-4e7d-a010-10f10feb6e0c', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148bf050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148bf1a0>]}
[0m14:34:29.258602 [info ] [MainThread]: Installing dbt-labs/codegen
[0m14:34:29.558254 [info ] [MainThread]: Installed from version 0.14.0
[0m14:34:29.558587 [info ] [MainThread]: Up to date!
[0m14:34:29.558818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'd2ccd821-f5c7-4e7d-a010-10f10feb6e0c', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113201070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113202570>]}
[0m14:34:29.559070 [info ] [MainThread]: Installing calogica/dbt_date
[0m14:34:29.934029 [info ] [MainThread]: Installed from version 0.10.1
[0m14:34:29.934319 [info ] [MainThread]: Up to date!
[0m14:34:29.934524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'd2ccd821-f5c7-4e7d-a010-10f10feb6e0c', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148bf830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148bf170>]}
[0m14:34:29.935847 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PackageRedirectDeprecation: 2 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:34:29.938289 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 3.3128986, "process_in_blocks": "0", "process_kernel_time": 0.350618, "process_mem_max_rss": "149602304", "process_out_blocks": "0", "process_user_time": 1.722269}
[0m14:34:29.938700 [debug] [MainThread]: Command `dbt deps` succeeded at 14:34:29.938640 after 3.31 seconds
[0m14:34:29.938918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10321f050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113fa1850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1146e5130>]}
[0m14:34:29.939130 [debug] [MainThread]: Flushing usage events
[0m14:34:30.348744 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:32.268290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1132da7b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114434f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1144346e0>]}


============================== 14:34:32.271375 | 712f75bb-a410-4074-890a-b97308f56d9c ==============================
[0m14:34:32.271375 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:34:32.271705 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'fail_fast': 'False', 'quiet': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'use_colors': 'True', 'debug': 'False', 'log_format': 'default', 'no_print': 'None', 'warn_error': 'None', 'use_experimental_parser': 'False', 'target_path': 'None', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'invocation_command': 'dbt seed', 'write_json': 'True', 'empty': 'None', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'indirect_selection': 'eager'}
[0m14:34:32.275306 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_HOST'
[0m14:34:32.276868 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 0.048020832, "process_in_blocks": "0", "process_kernel_time": 0.14973, "process_mem_max_rss": "132448256", "process_out_blocks": "0", "process_user_time": 1.336745}
[0m14:34:32.277212 [debug] [MainThread]: Command `dbt seed` failed at 14:34:32.277163 after 0.05 seconds
[0m14:34:32.277412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114434b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114437620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113c91dc0>]}
[0m14:34:32.277592 [debug] [MainThread]: Flushing usage events
[0m14:34:32.556008 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:17.954665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11437f980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114b35be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114b35550>]}


============================== 14:35:17.958390 | 84e0b9a7-3625-4ad4-baed-09f58dd259f3 ==============================
[0m14:35:17.958390 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:35:17.958749 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'log_format': 'default', 'use_colors': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'introspect': 'True', 'no_print': 'None', 'indirect_selection': 'eager', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'invocation_command': 'dbt deps', 'version_check': 'True', 'empty': 'None', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'static_parser': 'True', 'partial_parse': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs'}
[0m14:35:18.055694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '84e0b9a7-3625-4ad4-baed-09f58dd259f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114bd5b20>]}
[0m14:35:18.121887 [debug] [MainThread]: Set downloads directory='/var/folders/17/44fmtn0931x0k_v05ghc5pl40000gn/T/dbt-downloads-keczho6r'
[0m14:35:18.122243 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m14:35:18.268014 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m14:35:18.269034 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m14:35:18.488696 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m14:35:18.492628 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json
[0m14:35:18.659069 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json 200
[0m14:35:18.661125 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `calogica/dbt_expectations` package is deprecated in favor of
`metaplane/dbt_expectations`. Please update your `packages.yml` configuration to
use `metaplane/dbt_expectations` instead.
[0m14:35:18.661432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '84e0b9a7-3625-4ad4-baed-09f58dd259f3', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114b0b560>]}
[0m14:35:18.663012 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json
[0m14:35:18.907950 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json 200
[0m14:35:18.909736 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m14:35:19.082749 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m14:35:19.084589 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m14:35:19.562187 [info ] [MainThread]: Installed from version 1.3.3
[0m14:35:19.562491 [info ] [MainThread]: Up to date!
[0m14:35:19.562710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '84e0b9a7-3625-4ad4-baed-09f58dd259f3', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114e8bb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114eb2420>]}
[0m14:35:19.562936 [info ] [MainThread]: Installing calogica/dbt_expectations
[0m14:35:20.306735 [info ] [MainThread]: Installed from version 0.10.4
[0m14:35:20.307000 [info ] [MainThread]: Up to date!
[0m14:35:20.307190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '84e0b9a7-3625-4ad4-baed-09f58dd259f3', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c1b440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c19220>]}
[0m14:35:20.307388 [info ] [MainThread]: Installing dbt-labs/codegen
[0m14:35:20.768133 [info ] [MainThread]: Installed from version 0.14.0
[0m14:35:20.768452 [info ] [MainThread]: Up to date!
[0m14:35:20.768675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '84e0b9a7-3625-4ad4-baed-09f58dd259f3', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114e371a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114e37050>]}
[0m14:35:20.768915 [info ] [MainThread]: Installing calogica/dbt_date
[0m14:35:21.181472 [info ] [MainThread]: Installed from version 0.10.1
[0m14:35:21.181770 [info ] [MainThread]: Up to date!
[0m14:35:21.181968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '84e0b9a7-3625-4ad4-baed-09f58dd259f3', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114bd4680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114bd5df0>]}
[0m14:35:21.183229 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PackageRedirectDeprecation: 2 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:35:21.185225 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 3.2746265, "process_in_blocks": "0", "process_kernel_time": 0.343906, "process_mem_max_rss": "149028864", "process_out_blocks": "0", "process_user_time": 1.725064}
[0m14:35:21.185566 [debug] [MainThread]: Command `dbt deps` succeeded at 14:35:21.185506 after 3.28 seconds
[0m14:35:21.185801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113cd91c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114b0b560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11189f200>]}
[0m14:35:21.186009 [debug] [MainThread]: Flushing usage events
[0m14:35:21.527539 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:23.318018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069fd640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f2ecc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f2e5a0>]}


============================== 14:35:23.320842 | b0051998-d266-4201-8692-087d981e1a08 ==============================
[0m14:35:23.320842 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:35:23.321184 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'partial_parse': 'True', 'use_colors': 'True', 'introspect': 'True', 'indirect_selection': 'eager', 'debug': 'False', 'log_cache_events': 'False', 'warn_error': 'None', 'target_path': 'None', 'write_json': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'invocation_command': 'dbt seed', 'log_format': 'default', 'cache_selected_only': 'False', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'quiet': 'False', 'no_print': 'None', 'static_parser': 'True', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80'}
[0m14:35:23.807824 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:35:23.808182 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:35:23.808370 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:35:24.496659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b0051998-d266-4201-8692-087d981e1a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1196ffc80>]}
[0m14:35:24.532130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b0051998-d266-4201-8692-087d981e1a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11915ddf0>]}
[0m14:35:24.532637 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:35:24.620846 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:35:24.621429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b0051998-d266-4201-8692-087d981e1a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e28500>]}
[0m14:35:24.638692 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m14:35:24.639244 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:35:24.639432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b0051998-d266-4201-8692-087d981e1a08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11967a2d0>]}
[0m14:35:24.648986 [warn ] [MainThread]: [[33mWARNING[0m][PropertyMovedToConfigDeprecation]: Deprecated functionality
Found `meta` as a top-level property of `models[0].columns[0]` in file
`models/marts/core/schema.yml`. The `meta` top-level property should be moved
into the `config` of `models[0].columns[0]`.
[0m14:35:24.649318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'b0051998-d266-4201-8692-087d981e1a08', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11978ba40>]}
[0m14:35:25.941068 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on 'stg_creators' in
package 'patreon_analytics' (models/staging/schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m14:35:25.941424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'b0051998-d266-4201-8692-087d981e1a08', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a2fdd60>]}
[0m14:35:26.123483 [error] [MainThread]: Encountered an error:
Parsing Error
  The semantic layer requires a time spine model with granularity DAY or smaller in the project, but none was found. Guidance on creating this model can be found on our docs site (https://docs.getdbt.com/docs/build/metricflow-time-spine).
[0m14:35:26.123951 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PropertyMovedToConfigDeprecation: 42 occurrences
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:35:26.126375 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 2.8481545, "process_in_blocks": "0", "process_kernel_time": 0.367117, "process_mem_max_rss": "268124160", "process_out_blocks": "0", "process_user_time": 3.660052}
[0m14:35:26.126744 [debug] [MainThread]: Command `dbt seed` failed at 14:35:26.126697 after 2.85 seconds
[0m14:35:26.126977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1008af080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a376240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a2b2900>]}
[0m14:35:26.127164 [debug] [MainThread]: Flushing usage events
[0m14:35:26.468404 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:36:50.107815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100fdca70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059b66f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059b5f40>]}


============================== 14:36:50.112532 | 3cb6abf9-ecb1-4de7-b489-55e87d675e3a ==============================
[0m14:36:50.112532 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:36:50.112996 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'introspect': 'True', 'warn_error': 'None', 'no_print': 'None', 'debug': 'False', 'fail_fast': 'False', 'version_check': 'True', 'write_json': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'invocation_command': 'dbt deps', 'log_format': 'default', 'empty': 'None', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'printer_width': '80', 'indirect_selection': 'eager', 'target_path': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'quiet': 'False'}
[0m14:36:50.241304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3cb6abf9-ecb1-4de7-b489-55e87d675e3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a6a0c0>]}
[0m14:36:50.320595 [debug] [MainThread]: Set downloads directory='/var/folders/17/44fmtn0931x0k_v05ghc5pl40000gn/T/dbt-downloads-tmhxfcka'
[0m14:36:50.320906 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m14:36:50.591793 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m14:36:50.592988 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m14:36:50.815177 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m14:36:50.818953 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json
[0m14:36:50.961154 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json 200
[0m14:36:50.963515 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `calogica/dbt_expectations` package is deprecated in favor of
`metaplane/dbt_expectations`. Please update your `packages.yml` configuration to
use `metaplane/dbt_expectations` instead.
[0m14:36:50.963935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '3cb6abf9-ecb1-4de7-b489-55e87d675e3a', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105af8c20>]}
[0m14:36:50.965622 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json
[0m14:36:51.126382 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json 200
[0m14:36:51.128084 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m14:36:51.372450 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m14:36:51.374418 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m14:36:52.147665 [info ] [MainThread]: Installed from version 1.3.3
[0m14:36:52.147973 [info ] [MainThread]: Up to date!
[0m14:36:52.148195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '3cb6abf9-ecb1-4de7-b489-55e87d675e3a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d8bf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a59490>]}
[0m14:36:52.148443 [info ] [MainThread]: Installing calogica/dbt_expectations
[0m14:36:53.408823 [info ] [MainThread]: Installed from version 0.10.4
[0m14:36:53.409112 [info ] [MainThread]: Up to date!
[0m14:36:53.409323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '3cb6abf9-ecb1-4de7-b489-55e87d675e3a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d36b40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d36e10>]}
[0m14:36:53.409558 [info ] [MainThread]: Installing dbt-labs/codegen
[0m14:36:53.703266 [info ] [MainThread]: Installed from version 0.14.0
[0m14:36:53.703552 [info ] [MainThread]: Up to date!
[0m14:36:53.703759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '3cb6abf9-ecb1-4de7-b489-55e87d675e3a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dbe330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dbe180>]}
[0m14:36:53.703977 [info ] [MainThread]: Installing calogica/dbt_date
[0m14:36:56.143630 [info ] [MainThread]: Installed from version 0.10.1
[0m14:36:56.144033 [info ] [MainThread]: Up to date!
[0m14:36:56.144331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '3cb6abf9-ecb1-4de7-b489-55e87d675e3a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dbdee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dbe360>]}
[0m14:36:56.145999 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PackageRedirectDeprecation: 2 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:36:56.148705 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 6.0978827, "process_in_blocks": "0", "process_kernel_time": 0.374734, "process_mem_max_rss": "149618688", "process_out_blocks": "0", "process_user_time": 1.778868}
[0m14:36:56.149199 [debug] [MainThread]: Command `dbt deps` succeeded at 14:36:56.149120 after 6.10 seconds
[0m14:36:56.149496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a596d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105256cf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b6c590>]}
[0m14:36:56.149770 [debug] [MainThread]: Flushing usage events
[0m14:36:56.475306 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:36:58.309504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065c5760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107032a80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070323f0>]}


============================== 14:36:58.312171 | 90506413-25f7-4dee-827e-8e24c84ad52a ==============================
[0m14:36:58.312171 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:36:58.312525 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'partial_parse': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'quiet': 'False', 'fail_fast': 'False', 'invocation_command': 'dbt seed', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'write_json': 'True', 'target_path': 'None', 'printer_width': '80', 'no_print': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'warn_error': 'None'}
[0m14:36:58.792114 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:36:58.792468 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:36:58.792651 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:36:59.417939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '90506413-25f7-4dee-827e-8e24c84ad52a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f21e930>]}
[0m14:36:59.448352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '90506413-25f7-4dee-827e-8e24c84ad52a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119272c30>]}
[0m14:36:59.448824 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:36:59.532838 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:36:59.533395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '90506413-25f7-4dee-827e-8e24c84ad52a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1192d3d40>]}
[0m14:36:59.551300 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m14:36:59.551822 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:36:59.552042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '90506413-25f7-4dee-827e-8e24c84ad52a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ba42c0>]}
[0m14:36:59.562092 [warn ] [MainThread]: [[33mWARNING[0m][PropertyMovedToConfigDeprecation]: Deprecated functionality
Found `meta` as a top-level property of `models[0].columns[0]` in file
`models/marts/core/schema.yml`. The `meta` top-level property should be moved
into the `config` of `models[0].columns[0]`.
[0m14:36:59.562440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '90506413-25f7-4dee-827e-8e24c84ad52a', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1195f4bf0>]}
[0m14:37:00.922973 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on 'stg_creators' in
package 'patreon_analytics' (models/staging/schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m14:37:00.923323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '90506413-25f7-4dee-827e-8e24c84ad52a', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b129cd0>]}
[0m14:37:01.059733 [error] [MainThread]: Encountered an error:
Parsing Error
  Time spine standard granularity column must be defined on the model. Got invalid column name 'date_day' for model 'metricflow_time_spine'. Valid names: [].
[0m14:37:01.060156 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PropertyMovedToConfigDeprecation: 42 occurrences
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:37:01.062684 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 2.7907476, "process_in_blocks": "0", "process_kernel_time": 0.364968, "process_mem_max_rss": "265076736", "process_out_blocks": "0", "process_user_time": 3.638436}
[0m14:37:01.063132 [debug] [MainThread]: Command `dbt seed` failed at 14:37:01.063073 after 2.79 seconds
[0m14:37:01.063414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b00ef90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a7a9a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b05f350>]}
[0m14:37:01.063636 [debug] [MainThread]: Flushing usage events
[0m14:37:01.406544 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:37:43.875248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2e5b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b32acf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b32a5a0>]}


============================== 14:37:43.878533 | 0b82fc3c-08c3-4284-9e63-3f1f987eaefb ==============================
[0m14:37:43.878533 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:37:43.878887 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'write_json': 'True', 'use_colors': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'warn_error': 'None', 'invocation_command': 'dbt seed', 'static_parser': 'True', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'introspect': 'True', 'log_format': 'default', 'log_cache_events': 'False', 'quiet': 'False', 'fail_fast': 'False', 'empty': 'None', 'printer_width': '80', 'target_path': 'None', 'no_print': 'None', 'cache_selected_only': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'True'}
[0m14:37:44.356555 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:37:44.356894 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:37:44.357071 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:37:44.992983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0b82fc3c-08c3-4284-9e63-3f1f987eaefb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d3afbc0>]}
[0m14:37:45.025052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0b82fc3c-08c3-4284-9e63-3f1f987eaefb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cecc620>]}
[0m14:37:45.025550 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:37:45.115182 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:37:45.115775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0b82fc3c-08c3-4284-9e63-3f1f987eaefb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d477d40>]}
[0m14:37:45.135017 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m14:37:45.135630 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:37:45.135845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0b82fc3c-08c3-4284-9e63-3f1f987eaefb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d3814f0>]}
[0m14:37:45.145853 [warn ] [MainThread]: [[33mWARNING[0m][PropertyMovedToConfigDeprecation]: Deprecated functionality
Found `meta` as a top-level property of `models[0].columns[0]` in file
`models/marts/core/schema.yml`. The `meta` top-level property should be moved
into the `config` of `models[0].columns[0]`.
[0m14:37:45.146249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '0b82fc3c-08c3-4284-9e63-3f1f987eaefb', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d70dcd0>]}
[0m14:37:46.527950 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on 'stg_creators' in
package 'patreon_analytics' (models/staging/schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m14:37:46.528285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '0b82fc3c-08c3-4284-9e63-3f1f987eaefb', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11deade80>]}
[0m14:37:46.716553 [error] [MainThread]: No input metrics found for derived metric 'platform_take_rate'. Please add metrics to type_params.metrics.
[0m14:37:46.716859 [error] [MainThread]: Invalid name `month` - names cannot match reserved time granularity keywords (['NANOSECOND', 'MICROSECOND', 'MILLISECOND', 'SECOND', 'MINUTE', 'HOUR', 'DAY', 'WEEK', 'MONTH', 'QUARTER', 'YEAR'])
[0m14:37:46.717026 [error] [MainThread]: An error occurred while checking aggregation time dimension for a semantic model - AssertionError: Aggregation time dimension for measure mrr is not set! This should either be set directly on the measure specification in the model, or else defaulted to the primary time dimension in the data source containing the measure.

[0m14:37:46.717217 [error] [MainThread]: Encountered an error:
Parsing Error
  Semantic Manifest validation failed.
[0m14:37:46.717509 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PropertyMovedToConfigDeprecation: 42 occurrences
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:37:46.720659 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 2.885322, "process_in_blocks": "0", "process_kernel_time": 0.394348, "process_mem_max_rss": "268009472", "process_out_blocks": "0", "process_user_time": 3.759682}
[0m14:37:46.721080 [debug] [MainThread]: Command `dbt seed` failed at 14:37:46.721033 after 2.89 seconds
[0m14:37:46.721346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102acb080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11df76120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11df6caa0>]}
[0m14:37:46.721560 [debug] [MainThread]: Flushing usage events
[0m14:37:47.206193 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:38:44.276225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10839bb30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b48ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b481d0>]}


============================== 14:38:44.279571 | 300d5257-699e-41d0-9d25-1591df791008 ==============================
[0m14:38:44.279571 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:38:44.279919 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'no_print': 'None', 'static_parser': 'True', 'debug': 'False', 'warn_error': 'None', 'invocation_command': 'dbt seed', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'empty': 'None', 'log_format': 'default', 'fail_fast': 'False', 'printer_width': '80', 'log_cache_events': 'False', 'use_colors': 'True', 'write_json': 'True', 'target_path': 'None', 'partial_parse': 'True', 'quiet': 'False', 'cache_selected_only': 'False'}
[0m14:38:44.758585 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:38:44.758925 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:38:44.759106 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:38:45.423661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '300d5257-699e-41d0-9d25-1591df791008', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b924950>]}
[0m14:38:45.455756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '300d5257-699e-41d0-9d25-1591df791008', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc63b30>]}
[0m14:38:45.456260 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:38:45.542010 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:38:45.542609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '300d5257-699e-41d0-9d25-1591df791008', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c7f3c20>]}
[0m14:38:45.563791 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m14:38:45.564422 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:38:45.564639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '300d5257-699e-41d0-9d25-1591df791008', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c79ad50>]}
[0m14:38:45.573756 [warn ] [MainThread]: [[33mWARNING[0m][PropertyMovedToConfigDeprecation]: Deprecated functionality
Found `meta` as a top-level property of `models[0].columns[0]` in file
`models/marts/core/schema.yml`. The `meta` top-level property should be moved
into the `config` of `models[0].columns[0]`.
[0m14:38:45.574075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '300d5257-699e-41d0-9d25-1591df791008', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cb02150>]}
[0m14:38:46.936915 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on 'stg_creators' in
package 'patreon_analytics' (models/staging/schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m14:38:46.937239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '300d5257-699e-41d0-9d25-1591df791008', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d2aa0f0>]}
[0m14:38:47.163834 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.intermediate
- models.patreon_analytics.marts.finance
[0m14:38:47.168022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '300d5257-699e-41d0-9d25-1591df791008', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d2fca70>]}
[0m14:38:47.242796 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m14:38:47.245263 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m14:38:47.256679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '300d5257-699e-41d0-9d25-1591df791008', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cec77d0>]}
[0m14:38:47.257038 [info ] [MainThread]: Found 9 models, 7 seeds, 36 data tests, 4 metrics, 1125 macros, 1 semantic model
[0m14:38:47.257240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '300d5257-699e-41d0-9d25-1591df791008', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cdb2540>]}
[0m14:38:47.258745 [info ] [MainThread]: 
[0m14:38:47.258989 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:38:47.259143 [info ] [MainThread]: 
[0m14:38:47.259436 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m14:38:47.259593 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:38:47.263298 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m14:38:47.263552 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m14:38:47.267827 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m14:38:47.268056 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m14:38:47.268210 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:38:48.635370 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-a030-16a8-ab9c-3f640eae739e) - Created
[0m14:38:54.718232 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'patreon_dev' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'patreon_dev' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'patreon_dev' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1389)
	at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7997)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:145)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7996)
	at com.databricks.sql.managedcatalog.client.ErrorDetailsHandlerImpl.wrapServiceException(ErrorDetailsHandler.scala:96)
	at com.databricks.sql.managedcatalog.client.ErrorDetailsHandlerImpl.wrapServiceException$(ErrorDetailsHandler.scala:88)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:44)
	at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7975)
	at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7959)
	at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1355)
	at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1338)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:892)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:196)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:74)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:145)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:73)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:196)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:1108)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
	at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:431)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:411)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:782)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:701)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:687)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:701)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:847)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:847)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1389)
		at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7997)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:145)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7996)
		at com.databricks.sql.managedcatalog.client.ErrorDetailsHandlerImpl.wrapServiceException(ErrorDetailsHandler.scala:96)
		at com.databricks.sql.managedcatalog.client.ErrorDetailsHandlerImpl.wrapServiceException$(ErrorDetailsHandler.scala:88)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:44)
		at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7975)
		at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7959)
		at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1355)
		at com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1338)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:892)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:196)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2144)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:74)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:145)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:73)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:196)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:1108)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)
		at org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:56)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		... 65 more
, operation-id=01f0db77-a067-1bf2-8244-d44a6939bb43
[0m14:38:54.720701 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro list_schemas
: Database Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'patreon_dev' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m14:38:54.721241 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m14:38:54.721558 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-a030-16a8-ab9c-3f640eae739e) - Closing
[0m14:38:54.927182 [info ] [MainThread]: 
[0m14:38:54.927486 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 7.67 seconds (7.67s).
[0m14:38:54.927754 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    [NO_SUCH_CATALOG_EXCEPTION] Catalog 'patreon_dev' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m14:38:54.928084 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PropertyMovedToConfigDeprecation: 42 occurrences
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:38:54.932651 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 10.697953, "process_in_blocks": "0", "process_kernel_time": 0.413898, "process_mem_max_rss": "275644416", "process_out_blocks": "0", "process_user_time": 3.96183}
[0m14:38:54.933195 [debug] [MainThread]: Command `dbt seed` failed at 14:38:54.933142 after 10.70 seconds
[0m14:38:54.933420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d282510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cff8e60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cff8e00>]}
[0m14:38:54.933620 [debug] [MainThread]: Flushing usage events
[0m14:38:55.232569 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:39:35.682993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10596a510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10702a960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10702a1e0>]}


============================== 14:39:35.686354 | 69bafaa9-c75f-47d7-ae00-6233649724bc ==============================
[0m14:39:35.686354 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:39:35.686696 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'static_parser': 'True', 'log_format': 'default', 'write_json': 'True', 'empty': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'cache_selected_only': 'False', 'printer_width': '80', 'target_path': 'None', 'invocation_command': 'dbt run-operation --args {"sql": "CREATE CATALOG IF NOT EXISTS patreon_dev"} run_query', 'indirect_selection': 'eager', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'warn_error': 'None', 'introspect': 'True', 'fail_fast': 'False', 'no_print': 'None', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'use_colors': 'True', 'debug': 'False'}
[0m14:39:36.172621 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:39:36.172973 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:39:36.173151 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:39:36.815986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '69bafaa9-c75f-47d7-ae00-6233649724bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118ff530>]}
[0m14:39:36.850943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '69bafaa9-c75f-47d7-ae00-6233649724bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106554620>]}
[0m14:39:36.851453 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:39:36.942058 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:39:36.942628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '69bafaa9-c75f-47d7-ae00-6233649724bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x131061340>]}
[0m14:39:36.962740 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m14:39:37.099980 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:39:37.100302 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:39:37.100445 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:39:37.105374 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.marts.finance
- models.patreon_analytics.intermediate
[0m14:39:37.137063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '69bafaa9-c75f-47d7-ae00-6233649724bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1314c6db0>]}
[0m14:39:37.209493 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m14:39:37.212333 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m14:39:37.219420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '69bafaa9-c75f-47d7-ae00-6233649724bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x131851a30>]}
[0m14:39:37.219738 [info ] [MainThread]: Found 9 models, 7 seeds, 36 data tests, 4 metrics, 1125 macros, 1 semantic model
[0m14:39:37.219931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69bafaa9-c75f-47d7-ae00-6233649724bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111069460>]}
[0m14:39:37.220294 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_run_query) - Creating connection
[0m14:39:37.220464 [debug] [MainThread]: Acquiring new databricks connection 'macro_run_query'
[0m14:39:37.226286 [debug] [MainThread]: Using databricks connection "macro_run_query"
[0m14:39:37.226572 [debug] [MainThread]: On macro_run_query: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "macro_run_query"} */

    CREATE CATALOG IF NOT EXISTS patreon_dev
  
[0m14:39:37.226735 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:39:37.855345 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0db77-bda4-1815-a562-24001e9eea5c) - Created
[0m14:39:39.344826 [debug] [MainThread]: SQL status: OK in 2.120 seconds
[0m14:39:39.352214 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0db77-bda4-1815-a562-24001e9eea5c, command-id=01f0db77-bdbe-106c-8d62-58519da269f2) - Closing
[0m14:39:39.352640 [debug] [MainThread]: On macro_run_query: Close
[0m14:39:39.352888 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0db77-bda4-1815-a562-24001e9eea5c) - Closing
[0m14:39:39.535797 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m14:39:39.541566 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 3.9000962, "process_in_blocks": "0", "process_kernel_time": 0.403276, "process_mem_max_rss": "270254080", "process_out_blocks": "0", "process_user_time": 2.568727}
[0m14:39:39.542261 [debug] [MainThread]: Command `dbt run-operation` succeeded at 14:39:39.542150 after 3.90 seconds
[0m14:39:39.542719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10148f0e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10596a510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10702a960>]}
[0m14:39:39.543111 [debug] [MainThread]: Flushing usage events
[0m14:39:39.992312 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:40:17.394268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f6a600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11342aed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11342a5d0>]}


============================== 14:40:17.397706 | d4c596a6-af71-41af-81e6-95d3d581253c ==============================
[0m14:40:17.397706 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:40:17.398078 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'partial_parse': 'True', 'printer_width': '80', 'invocation_command': 'dbt run-operation create_catalog --args {"catalog_name": "patreon_dev"}', 'no_print': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'use_experimental_parser': 'False', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'static_parser': 'True', 'empty': 'None', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'log_cache_events': 'False'}
[0m14:40:17.914098 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:40:17.914420 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:40:17.914590 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:40:18.582908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd4c596a6-af71-41af-81e6-95d3d581253c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x134f6f4a0>]}
[0m14:40:18.617235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd4c596a6-af71-41af-81e6-95d3d581253c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x133da71a0>]}
[0m14:40:18.617756 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:40:18.703602 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:40:18.704119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'd4c596a6-af71-41af-81e6-95d3d581253c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136c670b0>]}
[0m14:40:18.724833 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m14:40:18.866215 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m14:40:18.866689 [debug] [MainThread]: Partial parsing: added file: patreon_analytics://macros/create_catalog.sql
[0m14:40:18.932515 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.intermediate
- models.patreon_analytics.marts.finance
[0m14:40:18.936352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd4c596a6-af71-41af-81e6-95d3d581253c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111a5fad0>]}
[0m14:40:19.009924 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m14:40:19.012227 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m14:40:19.019304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd4c596a6-af71-41af-81e6-95d3d581253c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13766d520>]}
[0m14:40:19.019652 [info ] [MainThread]: Found 9 models, 7 seeds, 36 data tests, 4 metrics, 1126 macros, 1 semantic model
[0m14:40:19.019854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd4c596a6-af71-41af-81e6-95d3d581253c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117f8e9c0>]}
[0m14:40:19.020242 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=macro_create_catalog) - Creating connection
[0m14:40:19.020407 [debug] [MainThread]: Acquiring new databricks connection 'macro_create_catalog'
[0m14:40:19.026905 [debug] [MainThread]: Using databricks connection "macro_create_catalog"
[0m14:40:19.027161 [debug] [MainThread]: On macro_create_catalog: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "macro_create_catalog"} */

    
        CREATE CATALOG IF NOT EXISTS patreon_dev
    
  
[0m14:40:19.027326 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:40:19.662318 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0db77-d68b-1267-9c04-8f9e0f0e259a) - Created
[0m14:40:20.164716 [debug] [MainThread]: SQL status: OK in 1.140 seconds
[0m14:40:20.168200 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0db77-d68b-1267-9c04-8f9e0f0e259a, command-id=01f0db77-d6a9-13fe-bf12-75a369f318a4) - Closing
[0m14:40:20.168477 [info ] [MainThread]: Catalog patreon_dev created or already exists
[0m14:40:20.168731 [debug] [MainThread]: On macro_create_catalog: Close
[0m14:40:20.168892 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0db77-d68b-1267-9c04-8f9e0f0e259a) - Closing
[0m14:40:20.363162 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m14:40:20.366195 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 3.014988, "process_in_blocks": "0", "process_kernel_time": 0.402127, "process_mem_max_rss": "270155776", "process_out_blocks": "0", "process_user_time": 2.596371}
[0m14:40:20.366529 [debug] [MainThread]: Command `dbt run-operation` succeeded at 14:40:20.366469 after 3.02 seconds
[0m14:40:20.366767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10323b0e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1117a5160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11342aed0>]}
[0m14:40:20.366975 [debug] [MainThread]: Flushing usage events
[0m14:40:20.659402 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:40:32.439248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2b9d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd2b620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd2af00>]}


============================== 14:40:32.442355 | ffb9479b-238a-4a4f-9e5d-51b4027e3978 ==============================
[0m14:40:32.442355 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:40:32.442700 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'write_json': 'True', 'invocation_command': 'dbt seed', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'debug': 'False', 'target_path': 'None', 'static_parser': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'warn_error': 'None', 'introspect': 'True', 'partial_parse': 'True', 'quiet': 'False'}
[0m14:40:32.905955 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:40:32.906289 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:40:32.906461 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:40:33.538646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d0797f0>]}
[0m14:40:33.571091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de66960>]}
[0m14:40:33.571615 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:40:33.663128 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:40:33.663727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d0a49b0>]}
[0m14:40:33.684266 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m14:40:33.816214 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:40:33.816546 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:40:33.816701 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:40:33.821298 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.marts.finance
- models.patreon_analytics.intermediate
[0m14:40:33.852189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d30d4f0>]}
[0m14:40:33.923002 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m14:40:33.925857 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m14:40:33.935076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d800b30>]}
[0m14:40:33.935387 [info ] [MainThread]: Found 9 models, 7 seeds, 36 data tests, 4 metrics, 1126 macros, 1 semantic model
[0m14:40:33.935567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d85ac90>]}
[0m14:40:33.936933 [info ] [MainThread]: 
[0m14:40:33.937109 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:40:33.937243 [info ] [MainThread]: 
[0m14:40:33.937538 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m14:40:33.937683 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:40:33.940919 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m14:40:33.941218 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m14:40:33.949284 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m14:40:33.949577 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m14:40:33.949758 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:40:34.570493 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-df71-1037-acf2-d56ff4dfbc6d) - Created
[0m14:40:35.040430 [debug] [ThreadPool]: SQL status: OK in 1.090 seconds
[0m14:40:35.047322 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db77-df71-1037-acf2-d56ff4dfbc6d, command-id=01f0db77-df8c-1132-b6dd-868a9d387179) - Closing
[0m14:40:35.047797 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m14:40:35.048529 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-df71-1037-acf2-d56ff4dfbc6d) - Closing
[0m14:40:35.236791 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_patreon_dev_analytics_raw) - Creating connection
[0m14:40:35.237070 [debug] [ThreadPool]: Acquiring new databricks connection 'create_patreon_dev_analytics_raw'
[0m14:40:35.237309 [debug] [ThreadPool]: Creating schema "database: "patreon_dev"
schema: "analytics_raw"
"
[0m14:40:35.240746 [debug] [ThreadPool]: Using databricks connection "create_patreon_dev_analytics_raw"
[0m14:40:35.241122 [debug] [ThreadPool]: On create_patreon_dev_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "create_patreon_dev_analytics_raw"} */
create schema if not exists `patreon_dev`.`analytics_raw`
  
[0m14:40:35.241329 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:40:35.852629 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-e032-19d9-bfb1-af91d8558b13) - Created
[0m14:40:36.454136 [debug] [ThreadPool]: SQL status: OK in 1.210 seconds
[0m14:40:36.456387 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db77-e032-19d9-bfb1-af91d8558b13, command-id=01f0db77-e04c-19a4-9e40-db5292ff0ac6) - Closing
[0m14:40:36.457738 [debug] [ThreadPool]: On create_patreon_dev_analytics_raw: Close
[0m14:40:36.458429 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-e032-19d9-bfb1-af91d8558b13) - Closing
[0m14:40:36.614288 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_marts) - Creating connection
[0m14:40:36.614696 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_staging) - Creating connection
[0m14:40:36.614911 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_marts'
[0m14:40:36.615317 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics) - Creating connection
[0m14:40:36.615722 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_raw) - Creating connection
[0m14:40:36.615923 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_staging'
[0m14:40:36.620365 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_marts"
[0m14:40:36.620604 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics'
[0m14:40:36.620864 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_raw'
[0m14:40:36.622561 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_staging"
[0m14:40:36.622887 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_marts'

  
[0m14:40:36.624547 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics"
[0m14:40:36.628643 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_raw"
[0m14:40:36.628979 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_staging'

  
[0m14:40:36.629335 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:40:36.629652 [debug] [ThreadPool]: On list_patreon_dev_analytics: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics'

  
[0m14:40:36.629970 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_raw'

  
[0m14:40:36.630243 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:40:36.630598 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:40:36.630899 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:40:37.414927 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-e123-169a-b05b-65d4e3605c3e) - Created
[0m14:40:37.415266 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-e121-1828-b948-d14e14ca08d8) - Created
[0m14:40:37.424385 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-e128-1863-88be-11525dc60083) - Created
[0m14:40:37.425079 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-e128-13e8-bc6b-2d96f335b07c) - Created
[0m14:40:38.385888 [debug] [ThreadPool]: SQL status: OK in 1.760 seconds
[0m14:40:38.386265 [debug] [ThreadPool]: SQL status: OK in 1.760 seconds
[0m14:40:38.388358 [debug] [ThreadPool]: SQL status: OK in 1.760 seconds
[0m14:40:38.389258 [debug] [ThreadPool]: SQL status: OK in 1.760 seconds
[0m14:40:38.391041 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db77-e123-169a-b05b-65d4e3605c3e, command-id=01f0db77-e13b-1e16-ae0b-7da9af82e9ac) - Closing
[0m14:40:38.391921 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db77-e128-13e8-bc6b-2d96f335b07c, command-id=01f0db77-e13f-14dc-9ae5-71369322e1e2) - Closing
[0m14:40:38.392594 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db77-e121-1828-b948-d14e14ca08d8, command-id=01f0db77-e13b-163b-ba3d-cee84ef34c78) - Closing
[0m14:40:38.393286 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db77-e128-1863-88be-11525dc60083, command-id=01f0db77-e13f-1c25-8125-0074f8e48ac7) - Closing
[0m14:40:38.393554 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: Close
[0m14:40:38.393966 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-e123-169a-b05b-65d4e3605c3e) - Closing
[0m14:40:38.557434 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: Close
[0m14:40:38.558821 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-e128-13e8-bc6b-2d96f335b07c) - Closing
[0m14:40:38.731078 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: Close
[0m14:40:38.731517 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-e121-1828-b948-d14e14ca08d8) - Closing
[0m14:40:38.888892 [debug] [ThreadPool]: On list_patreon_dev_analytics: Close
[0m14:40:38.889399 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db77-e128-1863-88be-11525dc60083) - Closing
[0m14:40:39.065134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d84f3b0>]}
[0m14:40:39.068573 [debug] [Thread-4 (]: Began running node seed.patreon_analytics.creators
[0m14:40:39.068898 [debug] [Thread-5 (]: Began running node seed.patreon_analytics.engagement_events
[0m14:40:39.069097 [debug] [Thread-6 (]: Began running node seed.patreon_analytics.patrons
[0m14:40:39.069404 [info ] [Thread-4 (]: 1 of 7 START seed file analytics_raw.creators .................................. [RUN]
[0m14:40:39.069698 [debug] [Thread-7 (]: Began running node seed.patreon_analytics.pledges
[0m14:40:39.070051 [info ] [Thread-5 (]: 2 of 7 START seed file analytics_raw.engagement_events ......................... [RUN]
[0m14:40:39.070422 [info ] [Thread-6 (]: 3 of 7 START seed file analytics_raw.patrons ................................... [RUN]
[0m14:40:39.070809 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.creators) - Creating connection
[0m14:40:39.071090 [info ] [Thread-7 (]: 4 of 7 START seed file analytics_raw.pledges ................................... [RUN]
[0m14:40:39.071365 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.engagement_events) - Creating connection
[0m14:40:39.071627 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.patrons) - Creating connection
[0m14:40:39.071965 [debug] [Thread-4 (]: Acquiring new databricks connection 'seed.patreon_analytics.creators'
[0m14:40:39.072364 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.pledges) - Creating connection
[0m14:40:39.072585 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.patreon_analytics.engagement_events'
[0m14:40:39.072817 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.patreon_analytics.patrons'
[0m14:40:39.073068 [debug] [Thread-4 (]: Began compiling node seed.patreon_analytics.creators
[0m14:40:39.073371 [debug] [Thread-7 (]: Acquiring new databricks connection 'seed.patreon_analytics.pledges'
[0m14:40:39.073581 [debug] [Thread-5 (]: Began compiling node seed.patreon_analytics.engagement_events
[0m14:40:39.073772 [debug] [Thread-6 (]: Began compiling node seed.patreon_analytics.patrons
[0m14:40:39.074014 [debug] [Thread-4 (]: Began executing node seed.patreon_analytics.creators
[0m14:40:39.074341 [debug] [Thread-7 (]: Began compiling node seed.patreon_analytics.pledges
[0m14:40:39.074641 [debug] [Thread-5 (]: Began executing node seed.patreon_analytics.engagement_events
[0m14:40:39.074891 [debug] [Thread-6 (]: Began executing node seed.patreon_analytics.patrons
[0m14:40:39.078402 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:40:39.078901 [debug] [Thread-7 (]: Began executing node seed.patreon_analytics.pledges
[0m14:40:39.080836 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:40:39.082616 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:40:39.083000 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d90aba0>]}
[0m14:40:39.084741 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:40:39.085131 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d96dd00>]}
[0m14:40:39.085421 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d988ec0>]}
[0m14:40:39.092116 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d9a8440>]}
[0m14:40:39.241673 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m14:40:39.242573 [debug] [Thread-7 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m14:40:39.243690 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.creators"
[0m14:40:39.246034 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m14:40:39.246321 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.engagement_events"} */

    create  table `patreon_dev`.`analytics_raw`.`engagement_events` (event_id string ,patron_id string ,creator_id string ,post_id string ,event_type string ,event_at timestamp )
    
    using delta
  
    
    
    
    
    
  
[0m14:40:39.246923 [debug] [Thread-7 (]: On seed.patreon_analytics.pledges: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.pledges"} */

    create  table `patreon_dev`.`analytics_raw`.`pledges` (pledge_id string ,patron_id string ,creator_id string ,tier_id string ,pledge_amount_usd decimal(10,2) ,pledge_status string ,is_first_pledge boolean ,started_at timestamp ,ended_at timestamp ,pause_started_at timestamp ,churn_reason string )
    
    using delta
  
    
    
    
    
    
  
[0m14:40:39.247205 [debug] [Thread-4 (]: On seed.patreon_analytics.creators: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.creators"} */

    create  table `patreon_dev`.`analytics_raw`.`creators` (creator_id string ,creator_name string ,email string ,category string ,subcategory string ,country_code string ,currency_code string ,plan_type string ,is_nsfw boolean ,is_verified boolean ,created_at timestamp ,first_pledge_received_at timestamp ,last_post_at timestamp ,status string )
    
    using delta
  
    
    
    
    
    
  
[0m14:40:39.247430 [debug] [Thread-6 (]: On seed.patreon_analytics.patrons: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.patrons"} */

    create  table `patreon_dev`.`analytics_raw`.`patrons` (patron_id string ,patron_name string ,email string ,country_code string ,created_at timestamp ,first_pledge_at timestamp ,lifetime_spend_usd double ,status string )
    
    using delta
  
    
    
    
    
    
  
[0m14:40:39.247618 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m14:40:39.247780 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m14:40:39.247935 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:40:39.248086 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:40:40.003833 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0db77-e2b1-1878-bbd7-77b3481189f3) - Created
[0m14:40:40.024381 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db77-e2b4-14b6-a4d4-6885a85ac28e) - Created
[0m14:40:40.053355 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db77-e2b9-1605-9494-9458ace7d006) - Created
[0m14:40:40.072666 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db77-e2b8-19ff-8198-022ac92b5b09) - Created
[0m14:40:42.942053 [debug] [Thread-6 (]: SQL status: OK in 3.690 seconds
[0m14:40:42.943402 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0db77-e2b4-14b6-a4d4-6885a85ac28e, command-id=01f0db77-e2c9-1ed5-8b61-dca34df251b3) - Closing
[0m14:40:42.973748 [debug] [Thread-5 (]: SQL status: OK in 3.730 seconds
[0m14:40:42.975130 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m14:40:42.975963 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db77-e2b8-19ff-8198-022ac92b5b09, command-id=01f0db77-e2d3-1f50-ab8d-b712bc1fff3c) - Closing
[0m14:40:42.976342 [debug] [Thread-6 (]: On seed.patreon_analytics.patrons: 
          insert overwrite `patreon_dev`.`analytics_raw`.`patrons` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%...
[0m14:40:42.993636 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m14:40:42.995409 [debug] [Thread-4 (]: SQL status: OK in 3.750 seconds
[0m14:40:42.995833 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: 
          insert overwrite `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:40:42.996558 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db77-e2b9-1605-9494-9458ace7d006, command-id=01f0db77-e2d0-1da4-bd99-d5d78e1d6a60) - Closing
[0m14:40:43.003758 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.creators"
[0m14:40:43.004104 [debug] [Thread-4 (]: On seed.patreon_analytics.creators: 
          insert overwrite `patreon_dev`.`analytics_raw`.`creators` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s...
[0m14:40:43.006956 [debug] [Thread-7 (]: SQL status: OK in 3.760 seconds
[0m14:40:43.007501 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0db77-e2b1-1878-bbd7-77b3481189f3, command-id=01f0db77-e2c6-1ced-9bb5-588f18b671ad) - Closing
[0m14:40:43.024628 [debug] [Thread-7 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m14:40:43.024998 [debug] [Thread-7 (]: On seed.patreon_analytics.pledges: 
          insert overwrite `patreon_dev`.`analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m14:40:47.500296 [debug] [Thread-7 (]: SQL status: OK in 4.470 seconds
[0m14:40:47.500772 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0db77-e2b1-1878-bbd7-77b3481189f3, command-id=01f0db77-e493-1859-986d-826dca72bec9) - Closing
[0m14:40:47.504294 [debug] [Thread-7 (]: Writing runtime SQL for node "seed.patreon_analytics.pledges"
[0m14:40:47.514215 [debug] [Thread-7 (]: On seed.patreon_analytics.pledges: Close
[0m14:40:47.514517 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0db77-e2b1-1878-bbd7-77b3481189f3) - Closing
[0m14:40:47.528059 [debug] [Thread-4 (]: SQL status: OK in 4.520 seconds
[0m14:40:47.528390 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db77-e2b9-1605-9494-9458ace7d006, command-id=01f0db77-e493-11d5-b801-7a174949fcd6) - Closing
[0m14:40:47.528805 [debug] [Thread-4 (]: Writing runtime SQL for node "seed.patreon_analytics.creators"
[0m14:40:47.675235 [debug] [Thread-4 (]: On seed.patreon_analytics.creators: Close
[0m14:40:47.677214 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db77-e2b9-1605-9494-9458ace7d006) - Closing
[0m14:40:47.867231 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d8dd850>]}
[0m14:40:47.867555 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d938bf0>]}
[0m14:40:47.868081 [info ] [Thread-7 (]: 4 of 7 OK loaded seed file analytics_raw.pledges ............................... [[32mINSERT 60[0m in 8.79s]
[0m14:40:47.868438 [info ] [Thread-4 (]: 1 of 7 OK loaded seed file analytics_raw.creators .............................. [[32mINSERT 15[0m in 8.80s]
[0m14:40:47.868900 [debug] [Thread-7 (]: Finished running node seed.patreon_analytics.pledges
[0m14:40:47.869239 [debug] [Thread-4 (]: Finished running node seed.patreon_analytics.creators
[0m14:40:47.869533 [debug] [Thread-7 (]: Began running node seed.patreon_analytics.posts
[0m14:40:47.869850 [debug] [Thread-4 (]: Began running node seed.patreon_analytics.tiers
[0m14:40:47.870198 [info ] [Thread-7 (]: 5 of 7 START seed file analytics_raw.posts ..................................... [RUN]
[0m14:40:47.870578 [info ] [Thread-4 (]: 6 of 7 START seed file analytics_raw.tiers ..................................... [RUN]
[0m14:40:47.870949 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.posts) - Creating connection
[0m14:40:47.871232 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.tiers) - Creating connection
[0m14:40:47.871438 [debug] [Thread-7 (]: Acquiring new databricks connection 'seed.patreon_analytics.posts'
[0m14:40:47.871704 [debug] [Thread-4 (]: Acquiring new databricks connection 'seed.patreon_analytics.tiers'
[0m14:40:47.872027 [debug] [Thread-7 (]: Began compiling node seed.patreon_analytics.posts
[0m14:40:47.872374 [debug] [Thread-4 (]: Began compiling node seed.patreon_analytics.tiers
[0m14:40:47.872662 [debug] [Thread-7 (]: Began executing node seed.patreon_analytics.posts
[0m14:40:47.873055 [debug] [Thread-4 (]: Began executing node seed.patreon_analytics.tiers
[0m14:40:47.880042 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.tiers"
[0m14:40:47.881072 [debug] [Thread-4 (]: On seed.patreon_analytics.tiers: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.tiers"} */

    create  table `patreon_dev`.`analytics_raw`.`tiers` (tier_id string ,creator_id string ,tier_name string ,tier_rank bigint ,price_usd decimal(10,2) ,description string ,is_active boolean ,created_at timestamp ,archived_at bigint )
    
    using delta
  
    
    
    
    
    
  
[0m14:40:47.882066 [debug] [Thread-7 (]: Using databricks connection "seed.patreon_analytics.posts"
[0m14:40:47.882295 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:40:47.882609 [debug] [Thread-7 (]: On seed.patreon_analytics.posts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.posts"} */

    create  table `patreon_dev`.`analytics_raw`.`posts` (post_id string ,creator_id string ,title string ,post_type string ,access_level string ,minimum_tier_id string ,published_at timestamp ,is_pinned boolean )
    
    using delta
  
    
    
    
    
    
  
[0m14:40:47.882965 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m14:40:48.580482 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db77-e7ce-1390-8133-42773aa0159c) - Created
[0m14:40:48.679982 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0db77-e7cf-175d-8524-9aaf8dca39f7) - Created
[0m14:40:48.681934 [debug] [Thread-6 (]: SQL status: OK in 5.690 seconds
[0m14:40:48.682378 [debug] [Thread-5 (]: SQL status: OK in 5.690 seconds
[0m14:40:48.682615 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0db77-e2b4-14b6-a4d4-6885a85ac28e, command-id=01f0db77-e48f-12f3-aad8-3bb18cd52a3f) - Closing
[0m14:40:48.682964 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db77-e2b8-19ff-8198-022ac92b5b09, command-id=01f0db77-e492-102e-8123-ba378c065820) - Closing
[0m14:40:48.867934 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.patreon_analytics.patrons"
[0m14:40:48.869464 [debug] [Thread-6 (]: On seed.patreon_analytics.patrons: Close
[0m14:40:48.869707 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db77-e2b4-14b6-a4d4-6885a85ac28e) - Closing
[0m14:40:48.899289 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.patreon_analytics.engagement_events"
[0m14:40:49.024581 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: Close
[0m14:40:49.024943 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db77-e2b8-19ff-8198-022ac92b5b09) - Closing
[0m14:40:49.201663 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d925190>]}
[0m14:40:49.203968 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d96da30>]}
[0m14:40:49.204821 [info ] [Thread-6 (]: 3 of 7 OK loaded seed file analytics_raw.patrons ............................... [[32mINSERT 40[0m in 10.13s]
[0m14:40:49.205619 [info ] [Thread-5 (]: 2 of 7 OK loaded seed file analytics_raw.engagement_events ..................... [[32mINSERT 83[0m in 10.13s]
[0m14:40:49.206088 [debug] [Thread-6 (]: Finished running node seed.patreon_analytics.patrons
[0m14:40:49.206382 [debug] [Thread-5 (]: Finished running node seed.patreon_analytics.engagement_events
[0m14:40:49.206748 [debug] [Thread-6 (]: Began running node seed.patreon_analytics.transactions
[0m14:40:49.207240 [info ] [Thread-6 (]: 7 of 7 START seed file analytics_raw.transactions .............................. [RUN]
[0m14:40:49.207692 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.transactions) - Creating connection
[0m14:40:49.207924 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.patreon_analytics.transactions'
[0m14:40:49.208115 [debug] [Thread-6 (]: Began compiling node seed.patreon_analytics.transactions
[0m14:40:49.208296 [debug] [Thread-6 (]: Began executing node seed.patreon_analytics.transactions
[0m14:40:49.218056 [debug] [Thread-6 (]: Compilation Error in seed transactions (seeds/transactions.csv)
  Row 57 has 14 values, but Table only has 13 columns.
  
  > in macro create_seed_v1 (macros/materializations/seeds/seeds.sql)
  > called by macro materialization_seed_databricks (macros/materializations/seeds/seeds.sql)
  > called by seed transactions (seeds/transactions.csv)
[0m14:40:49.218608 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cf8ef00>]}
[0m14:40:49.218915 [error] [Thread-6 (]: 7 of 7 ERROR loading seed file analytics_raw.transactions ...................... [[31mERROR[0m in 0.01s]
[0m14:40:49.219194 [debug] [Thread-6 (]: Finished running node seed.patreon_analytics.transactions
[0m14:40:49.219439 [debug] [Thread-10 ]: Marking all children of 'seed.patreon_analytics.transactions' to be skipped because of status 'error'.  Reason: Compilation Error in seed transactions (seeds/transactions.csv)
  Row 57 has 14 values, but Table only has 13 columns.
  
  > in macro create_seed_v1 (macros/materializations/seeds/seeds.sql)
  > called by macro materialization_seed_databricks (macros/materializations/seeds/seeds.sql)
  > called by seed transactions (seeds/transactions.csv).
[0m14:40:50.497536 [debug] [Thread-4 (]: SQL status: OK in 2.620 seconds
[0m14:40:50.498297 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db77-e7ce-1390-8133-42773aa0159c, command-id=01f0db77-e7f2-1f3a-b098-cef7fe7c566c) - Closing
[0m14:40:50.511001 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.tiers"
[0m14:40:50.511410 [debug] [Thread-4 (]: On seed.patreon_analytics.tiers: 
          insert overwrite `patreon_dev`.`analytics_raw`.`tiers` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%...
[0m14:40:50.523101 [debug] [Thread-7 (]: SQL status: OK in 2.640 seconds
[0m14:40:50.523752 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0db77-e7cf-175d-8524-9aaf8dca39f7, command-id=01f0db77-e7f6-14b0-88ed-e16b0ce922e4) - Closing
[0m14:40:50.534147 [debug] [Thread-7 (]: Using databricks connection "seed.patreon_analytics.posts"
[0m14:40:50.534592 [debug] [Thread-7 (]: On seed.patreon_analytics.posts: 
          insert overwrite `patreon_dev`.`analytics_raw`.`posts` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,...
[0m14:40:52.026261 [debug] [Thread-4 (]: SQL status: OK in 1.510 seconds
[0m14:40:52.026936 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db77-e7ce-1390-8133-42773aa0159c, command-id=01f0db77-e90c-181f-af72-28e02d75d043) - Closing
[0m14:40:52.027723 [debug] [Thread-4 (]: Writing runtime SQL for node "seed.patreon_analytics.tiers"
[0m14:40:52.030164 [debug] [Thread-4 (]: On seed.patreon_analytics.tiers: Close
[0m14:40:52.030603 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db77-e7ce-1390-8133-42773aa0159c) - Closing
[0m14:40:52.137898 [debug] [Thread-7 (]: SQL status: OK in 1.600 seconds
[0m14:40:52.138463 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0db77-e7cf-175d-8524-9aaf8dca39f7, command-id=01f0db77-e910-1166-8049-5b8270b31b7f) - Closing
[0m14:40:52.139078 [debug] [Thread-7 (]: Writing runtime SQL for node "seed.patreon_analytics.posts"
[0m14:40:52.210735 [debug] [Thread-7 (]: On seed.patreon_analytics.posts: Close
[0m14:40:52.211157 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0db77-e7cf-175d-8524-9aaf8dca39f7) - Closing
[0m14:40:52.373737 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd09c40>]}
[0m14:40:52.374133 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffb9479b-238a-4a4f-9e5d-51b4027e3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd09190>]}
[0m14:40:52.374453 [info ] [Thread-4 (]: 6 of 7 OK loaded seed file analytics_raw.tiers ................................. [[32mINSERT 39[0m in 4.50s]
[0m14:40:52.374732 [info ] [Thread-7 (]: 5 of 7 OK loaded seed file analytics_raw.posts ................................. [[32mINSERT 46[0m in 4.50s]
[0m14:40:52.375006 [debug] [Thread-4 (]: Finished running node seed.patreon_analytics.tiers
[0m14:40:52.375242 [debug] [Thread-7 (]: Finished running node seed.patreon_analytics.posts
[0m14:40:52.376222 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m14:40:52.376489 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:40:52.376762 [info ] [MainThread]: 
[0m14:40:52.376955 [info ] [MainThread]: Finished running 7 seeds in 0 hours 0 minutes and 18.44 seconds (18.44s).
[0m14:40:52.377698 [debug] [MainThread]: Command end result
[0m14:40:52.407811 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m14:40:52.410797 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m14:40:52.414836 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m14:40:52.415111 [info ] [MainThread]: 
[0m14:40:52.415382 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:40:52.415592 [info ] [MainThread]: 
[0m14:40:52.415850 [error] [MainThread]: [31mFailure in seed transactions (seeds/transactions.csv)[0m
[0m14:40:52.416089 [error] [MainThread]:   Compilation Error in seed transactions (seeds/transactions.csv)
  Row 57 has 14 values, but Table only has 13 columns.
  
  > in macro create_seed_v1 (macros/materializations/seeds/seeds.sql)
  > called by macro materialization_seed_databricks (macros/materializations/seeds/seeds.sql)
  > called by seed transactions (seeds/transactions.csv)
[0m14:40:52.416288 [info ] [MainThread]: 
[0m14:40:52.416497 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=7
[0m14:40:52.419741 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 20.0205, "process_in_blocks": "0", "process_kernel_time": 0.424739, "process_mem_max_rss": "284229632", "process_out_blocks": "0", "process_user_time": 5.164897}
[0m14:40:52.420096 [debug] [MainThread]: Command `dbt seed` failed at 14:40:52.420043 after 20.02 seconds
[0m14:40:52.420359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b350dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d84cd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d9d7fb0>]}
[0m14:40:52.420590 [debug] [MainThread]: Flushing usage events
[0m14:40:52.771933 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:44:40.578301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108952750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b36240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b35940>]}


============================== 14:44:40.581967 | bc497619-feca-4218-a5ba-7bcc820edfea ==============================
[0m14:44:40.581967 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:44:40.582430 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'quiet': 'False', 'printer_width': '80', 'invocation_command': 'dbt deps', 'static_parser': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'indirect_selection': 'eager', 'introspect': 'True', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'empty': 'None', 'log_cache_events': 'False', 'target_path': 'None', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'version_check': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m14:44:40.677184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc497619-feca-4218-a5ba-7bcc820edfea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10833e900>]}
[0m14:44:40.732022 [debug] [MainThread]: Set downloads directory='/var/folders/17/44fmtn0931x0k_v05ghc5pl40000gn/T/dbt-downloads-725549vz'
[0m14:44:40.732329 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m14:44:41.164648 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m14:44:41.166036 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m14:44:41.305207 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m14:44:41.307739 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json
[0m14:44:41.558749 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json 200
[0m14:44:41.560762 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `calogica/dbt_expectations` package is deprecated in favor of
`metaplane/dbt_expectations`. Please update your `packages.yml` configuration to
use `metaplane/dbt_expectations` instead.
[0m14:44:41.561065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'bc497619-feca-4218-a5ba-7bcc820edfea', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b34e60>]}
[0m14:44:41.562802 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json
[0m14:44:41.810644 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json 200
[0m14:44:41.813039 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m14:44:42.046279 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m14:44:42.048578 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m14:44:42.631367 [info ] [MainThread]: Installed from version 1.3.3
[0m14:44:42.631649 [info ] [MainThread]: Up to date!
[0m14:44:42.631858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'bc497619-feca-4218-a5ba-7bcc820edfea', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109dae960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10993ac30>]}
[0m14:44:42.632068 [info ] [MainThread]: Installing calogica/dbt_expectations
[0m14:44:43.668281 [info ] [MainThread]: Installed from version 0.10.4
[0m14:44:43.668594 [info ] [MainThread]: Up to date!
[0m14:44:43.668807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'bc497619-feca-4218-a5ba-7bcc820edfea', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c77290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c765a0>]}
[0m14:44:43.669034 [info ] [MainThread]: Installing dbt-labs/codegen
[0m14:44:44.101107 [info ] [MainThread]: Installed from version 0.14.0
[0m14:44:44.101392 [info ] [MainThread]: Up to date!
[0m14:44:44.101598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'bc497619-feca-4218-a5ba-7bcc820edfea', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c3d0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d39310>]}
[0m14:44:44.101811 [info ] [MainThread]: Installing calogica/dbt_date
[0m14:44:44.621455 [info ] [MainThread]: Installed from version 0.10.1
[0m14:44:44.621763 [info ] [MainThread]: Up to date!
[0m14:44:44.621990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'bc497619-feca-4218-a5ba-7bcc820edfea', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bd58b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bd54c0>]}
[0m14:44:44.623334 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PackageRedirectDeprecation: 2 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:44:44.625992 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 4.0912156, "process_in_blocks": "0", "process_kernel_time": 0.328034, "process_mem_max_rss": "149946368", "process_out_blocks": "0", "process_user_time": 1.691471}
[0m14:44:44.626411 [debug] [MainThread]: Command `dbt deps` succeeded at 14:44:44.626347 after 4.09 seconds
[0m14:44:44.626665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109be9dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ad2ff0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096a5430>]}
[0m14:44:44.626910 [debug] [MainThread]: Flushing usage events
[0m14:44:45.124649 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:44:47.058857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12132e9f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124c2b110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124c2a990>]}


============================== 14:44:47.062022 | 6bea731b-ebd5-428c-9c26-42ad4d4de120 ==============================
[0m14:44:47.062022 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:44:47.062574 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'empty': 'None', 'indirect_selection': 'eager', 'warn_error': 'None', 'static_parser': 'True', 'write_json': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'use_colors': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'fail_fast': 'False', 'quiet': 'False', 'no_print': 'None', 'debug': 'False', 'invocation_command': 'dbt seed'}
[0m14:44:47.547889 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:44:47.548247 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:44:47.548417 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:44:48.175855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1214afec0>]}
[0m14:44:48.208080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135e6f530>]}
[0m14:44:48.208526 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:44:48.298945 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:44:48.299537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136367d40>]}
[0m14:44:48.318898 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m14:44:48.449278 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 7 files changed.
[0m14:44:48.449796 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://seeds/tiers.csv
[0m14:44:48.450022 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://seeds/pledges.csv
[0m14:44:48.450219 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://seeds/creators.csv
[0m14:44:48.450398 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://seeds/posts.csv
[0m14:44:48.450577 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://seeds/transactions.csv
[0m14:44:48.450751 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://seeds/patrons.csv
[0m14:44:48.450920 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://seeds/engagement_events.csv
[0m14:44:48.595372 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.intermediate
- models.patreon_analytics.marts.finance
[0m14:44:48.599053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136dfdac0>]}
[0m14:44:48.671979 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m14:44:48.674137 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m14:44:48.764177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136e6d790>]}
[0m14:44:48.764490 [info ] [MainThread]: Found 9 models, 36 data tests, 7 seeds, 4 metrics, 1126 macros, 1 semantic model
[0m14:44:48.764675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e67890>]}
[0m14:44:48.766098 [info ] [MainThread]: 
[0m14:44:48.766336 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:44:48.766489 [info ] [MainThread]: 
[0m14:44:48.766801 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m14:44:48.766963 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:44:48.770376 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m14:44:48.770684 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m14:44:48.777122 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m14:44:48.777366 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m14:44:48.777529 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:49.463428 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-7760-1d28-b0ea-070087fbcbdc) - Created
[0m14:44:49.891474 [debug] [ThreadPool]: SQL status: OK in 1.110 seconds
[0m14:44:49.900195 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-7760-1d28-b0ea-070087fbcbdc, command-id=01f0db78-777a-1a7b-a803-8eac2a72ddc2) - Closing
[0m14:44:49.901080 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m14:44:49.901490 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-7760-1d28-b0ea-070087fbcbdc) - Closing
[0m14:44:50.082236 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_marts) - Creating connection
[0m14:44:50.082702 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics) - Creating connection
[0m14:44:50.082897 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_marts'
[0m14:44:50.083162 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_staging) - Creating connection
[0m14:44:50.083490 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_raw) - Creating connection
[0m14:44:50.083695 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics'
[0m14:44:50.087761 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_marts"
[0m14:44:50.088074 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_staging'
[0m14:44:50.088313 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_raw'
[0m14:44:50.090292 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_marts'

  
[0m14:44:50.093127 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics"
[0m14:44:50.095465 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_staging"
[0m14:44:50.097620 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_raw"
[0m14:44:50.097851 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:50.098086 [debug] [ThreadPool]: On list_patreon_dev_analytics: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics'

  
[0m14:44:50.098334 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_staging'

  
[0m14:44:50.098573 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_raw'

  
[0m14:44:50.098852 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:50.099246 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:50.099506 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:50.973551 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-7847-14c6-8c01-819f3b6c59dd) - Created
[0m14:44:50.977898 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-7842-1674-b558-32925d842519) - Created
[0m14:44:51.052009 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-7853-1e3d-9d6e-a1e94f0b8f7e) - Created
[0m14:44:51.057761 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-7853-16a9-a238-412ffa9aa048) - Created
[0m14:44:51.458210 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m14:44:51.459006 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m14:44:51.460794 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-7853-16a9-a238-412ffa9aa048, command-id=01f0db78-786a-12fd-93b5-8daee8e2ce2c) - Closing
[0m14:44:51.461169 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: Close
[0m14:44:51.461411 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-7853-16a9-a238-412ffa9aa048) - Closing
[0m14:44:51.462436 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-7842-1674-b558-32925d842519, command-id=01f0db78-785d-14a7-bd27-9281511d82d0) - Closing
[0m14:44:51.484270 [debug] [ThreadPool]: SQL status: OK in 1.390 seconds
[0m14:44:51.485638 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-7853-1e3d-9d6e-a1e94f0b8f7e, command-id=01f0db78-786b-1958-a04c-3f8fcbb66976) - Closing
[0m14:44:51.634297 [debug] [ThreadPool]: SQL status: OK in 1.530 seconds
[0m14:44:51.635852 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-7847-14c6-8c01-819f3b6c59dd, command-id=01f0db78-785d-11cb-84bc-202100e51196) - Closing
[0m14:44:51.640021 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: Close
[0m14:44:51.640344 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-7842-1674-b558-32925d842519) - Closing
[0m14:44:51.801756 [debug] [ThreadPool]: On list_patreon_dev_analytics: Close
[0m14:44:51.802547 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-7853-1e3d-9d6e-a1e94f0b8f7e) - Closing
[0m14:44:51.968579 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: Close
[0m14:44:51.969363 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-7847-14c6-8c01-819f3b6c59dd) - Closing
[0m14:44:52.137504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122015f10>]}
[0m14:44:52.141160 [debug] [Thread-3 (]: Began running node seed.patreon_analytics.creators
[0m14:44:52.141495 [debug] [Thread-4 (]: Began running node seed.patreon_analytics.engagement_events
[0m14:44:52.141725 [debug] [Thread-5 (]: Began running node seed.patreon_analytics.patrons
[0m14:44:52.142044 [info ] [Thread-3 (]: 1 of 7 START seed file analytics_raw.creators .................................. [RUN]
[0m14:44:52.142323 [debug] [Thread-6 (]: Began running node seed.patreon_analytics.pledges
[0m14:44:52.142648 [info ] [Thread-4 (]: 2 of 7 START seed file analytics_raw.engagement_events ......................... [RUN]
[0m14:44:52.142979 [info ] [Thread-5 (]: 3 of 7 START seed file analytics_raw.patrons ................................... [RUN]
[0m14:44:52.143398 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.creators) - Creating connection
[0m14:44:52.143900 [info ] [Thread-6 (]: 4 of 7 START seed file analytics_raw.pledges ................................... [RUN]
[0m14:44:52.144488 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.engagement_events) - Creating connection
[0m14:44:52.145002 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.patrons) - Creating connection
[0m14:44:52.145387 [debug] [Thread-3 (]: Acquiring new databricks connection 'seed.patreon_analytics.creators'
[0m14:44:52.145796 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.pledges) - Creating connection
[0m14:44:52.146115 [debug] [Thread-4 (]: Acquiring new databricks connection 'seed.patreon_analytics.engagement_events'
[0m14:44:52.146370 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.patreon_analytics.patrons'
[0m14:44:52.147003 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.patreon_analytics.pledges'
[0m14:44:52.147911 [debug] [Thread-4 (]: Began compiling node seed.patreon_analytics.engagement_events
[0m14:44:52.148963 [debug] [Thread-5 (]: Began compiling node seed.patreon_analytics.patrons
[0m14:44:52.146628 [debug] [Thread-3 (]: Began compiling node seed.patreon_analytics.creators
[0m14:44:52.151899 [debug] [Thread-6 (]: Began compiling node seed.patreon_analytics.pledges
[0m14:44:52.152260 [debug] [Thread-4 (]: Began executing node seed.patreon_analytics.engagement_events
[0m14:44:52.152538 [debug] [Thread-5 (]: Began executing node seed.patreon_analytics.patrons
[0m14:44:52.152774 [debug] [Thread-3 (]: Began executing node seed.patreon_analytics.creators
[0m14:44:52.152998 [debug] [Thread-6 (]: Began executing node seed.patreon_analytics.pledges
[0m14:44:52.156440 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:44:52.158921 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:44:52.161587 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:44:52.163244 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136e7c200>]}
[0m14:44:52.166773 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136e93530>]}
[0m14:44:52.166502 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:44:52.167173 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136ed16a0>]}
[0m14:44:52.177354 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1364605f0>]}
[0m14:44:52.257618 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.creators"
[0m14:44:52.281668 [debug] [Thread-3 (]: On seed.patreon_analytics.creators: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.creators"} */

    create or replace table `patreon_dev`.`analytics_raw`.`creators` (creator_id string ,creator_name string ,email string ,category string ,subcategory string ,country_code string ,currency_code string ,plan_type string ,is_nsfw boolean ,is_verified boolean ,created_at timestamp ,first_pledge_received_at timestamp ,last_post_at timestamp ,status string )
    
    using delta
  
    
    
    
    
    
  
[0m14:44:52.312110 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:44:52.557522 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m14:44:52.558318 [debug] [Thread-6 (]: On seed.patreon_analytics.pledges: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.pledges"} */

    create or replace table `patreon_dev`.`analytics_raw`.`pledges` (pledge_id string ,patron_id string ,creator_id string ,tier_id string ,pledge_amount_usd decimal(10,2) ,pledge_status string ,is_first_pledge boolean ,started_at timestamp ,ended_at timestamp ,pause_started_at bigint ,churn_reason string )
    
    using delta
  
    
    
    
    
    
  
[0m14:44:52.558762 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:44:52.806972 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m14:44:52.813440 [debug] [Thread-5 (]: On seed.patreon_analytics.patrons: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.patrons"} */

    create or replace table `patreon_dev`.`analytics_raw`.`patrons` (patron_id string ,patron_name string ,email string ,country_code string ,created_at timestamp ,first_pledge_at timestamp ,lifetime_spend_usd double ,status string )
    
    using delta
  
    
    
    
    
    
  
[0m14:44:52.813852 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m14:44:53.247256 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0db78-7997-144b-9f99-109e0337913c) - Created
[0m14:44:53.252657 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m14:44:53.253356 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.engagement_events"} */

    create or replace table `patreon_dev`.`analytics_raw`.`engagement_events` (event_id string ,patron_id string ,creator_id string ,post_id string ,event_type string ,event_at timestamp )
    
    using delta
  
    
    
    
    
    
  
[0m14:44:53.253601 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:44:53.344678 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db78-79ac-1581-a6fd-a619c0627225) - Created
[0m14:44:53.514550 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db78-79c8-1fb6-9814-621db08ed72b) - Created
[0m14:44:54.042721 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db78-7a0a-1f60-84b5-7db38ce56301) - Created
[0m14:44:54.844390 [debug] [Thread-5 (]: SQL status: OK in 2.030 seconds
[0m14:44:54.845186 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-79c8-1fb6-9814-621db08ed72b, command-id=01f0db78-79e1-10c0-ac8d-ad3d5ffb7b23) - Closing
[0m14:44:54.961653 [debug] [Thread-6 (]: SQL status: OK in 2.400 seconds
[0m14:44:54.968591 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0db78-79ac-1581-a6fd-a619c0627225, command-id=01f0db78-79c8-1d59-a491-beb93cfef2c8) - Closing
[0m14:44:55.104215 [debug] [Thread-3 (]: SQL status: OK in 2.790 seconds
[0m14:44:55.117509 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0db78-7997-144b-9f99-109e0337913c, command-id=01f0db78-79ba-1f4c-997f-62fa063c512b) - Closing
[0m14:44:55.389107 [debug] [Thread-4 (]: SQL status: OK in 2.140 seconds
[0m14:44:55.413973 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db78-7a0a-1f60-84b5-7db38ce56301, command-id=01f0db78-7a33-1f57-9bb7-593eb6e87671) - Closing
[0m14:44:55.723761 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.creators"
[0m14:44:55.730758 [debug] [Thread-3 (]: On seed.patreon_analytics.creators: 
          insert overwrite `patreon_dev`.`analytics_raw`.`creators` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s...
[0m14:44:57.752841 [debug] [Thread-3 (]: SQL status: OK in 2.020 seconds
[0m14:44:57.765638 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0db78-7997-144b-9f99-109e0337913c, command-id=01f0db78-7b5e-1b0f-ad0e-ab96eae38d80) - Closing
[0m14:44:57.792887 [debug] [Thread-3 (]: Writing runtime SQL for node "seed.patreon_analytics.creators"
[0m14:44:57.903726 [debug] [Thread-3 (]: On seed.patreon_analytics.creators: Close
[0m14:44:57.945828 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0db78-7997-144b-9f99-109e0337913c) - Closing
[0m14:44:58.310263 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1214af170>]}
[0m14:44:58.328573 [info ] [Thread-3 (]: 1 of 7 OK loaded seed file analytics_raw.creators .............................. [[32mINSERT 500[0m in 6.00s]
[0m14:44:58.352508 [debug] [Thread-3 (]: Finished running node seed.patreon_analytics.creators
[0m14:44:58.359195 [debug] [Thread-3 (]: Began running node seed.patreon_analytics.posts
[0m14:44:58.378168 [info ] [Thread-3 (]: 5 of 7 START seed file analytics_raw.posts ..................................... [RUN]
[0m14:44:58.384977 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.posts) - Creating connection
[0m14:44:58.402809 [debug] [Thread-3 (]: Acquiring new databricks connection 'seed.patreon_analytics.posts'
[0m14:44:58.414909 [debug] [Thread-3 (]: Began compiling node seed.patreon_analytics.posts
[0m14:44:58.434081 [debug] [Thread-3 (]: Began executing node seed.patreon_analytics.posts
[0m14:44:58.869547 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.posts"
[0m14:44:58.888285 [debug] [Thread-3 (]: On seed.patreon_analytics.posts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.posts"} */

    create or replace table `patreon_dev`.`analytics_raw`.`posts` (post_id string ,creator_id string ,title string ,post_type string ,access_level string ,minimum_tier_id string ,published_at timestamp ,is_pinned boolean )
    
    using delta
  
    
    
    
    
    
  
[0m14:44:58.906388 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:44:59.989617 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0db78-7d9f-10c9-ab9b-28bacda79fd8) - Created
[0m14:45:00.034838 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m14:45:00.043260 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert overwrite `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:45:00.567343 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m14:45:00.577817 [debug] [Thread-5 (]: On seed.patreon_analytics.patrons: 
          insert overwrite `patreon_dev`.`analytics_raw`.`patrons` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%...
[0m14:45:01.416805 [debug] [Thread-3 (]: SQL status: OK in 2.510 seconds
[0m14:45:01.423771 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0db78-7d9f-10c9-ab9b-28bacda79fd8, command-id=01f0db78-7dc4-1c06-83fb-e4ea30709d4c) - Closing
[0m14:45:01.550676 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m14:45:01.562525 [debug] [Thread-6 (]: On seed.patreon_analytics.pledges: 
          insert overwrite `patreon_dev`.`analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m14:45:03.293432 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.posts"
[0m14:45:03.297221 [debug] [Thread-3 (]: On seed.patreon_analytics.posts: 
          insert overwrite `patreon_dev`.`analytics_raw`.`posts` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,...
[0m14:45:03.479079 [debug] [Thread-4 (]: SQL status: OK in 3.420 seconds
[0m14:45:03.479431 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db78-7a0a-1f60-84b5-7db38ce56301, command-id=01f0db78-7e4c-1984-8e6d-d450e4b18199) - Closing
[0m14:45:04.262067 [debug] [Thread-5 (]: SQL status: OK in 3.670 seconds
[0m14:45:04.268240 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-79c8-1fb6-9814-621db08ed72b, command-id=01f0db78-7e9e-1f5f-8d02-b5770f4b5055) - Closing
[0m14:45:05.210610 [debug] [Thread-6 (]: SQL status: OK in 3.640 seconds
[0m14:45:05.229216 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0db78-79ac-1581-a6fd-a619c0627225, command-id=01f0db78-7f44-18b1-8805-c50a6835c936) - Closing
[0m14:45:06.206976 [debug] [Thread-3 (]: SQL status: OK in 2.910 seconds
[0m14:45:06.226300 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0db78-7d9f-10c9-ab9b-28bacda79fd8, command-id=01f0db78-8025-10dd-9f32-8d3431132bff) - Closing
[0m14:45:06.233377 [debug] [Thread-3 (]: Writing runtime SQL for node "seed.patreon_analytics.posts"
[0m14:45:06.341947 [debug] [Thread-3 (]: On seed.patreon_analytics.posts: Close
[0m14:45:06.374714 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0db78-7d9f-10c9-ab9b-28bacda79fd8) - Closing
[0m14:45:06.645693 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x162435cd0>]}
[0m14:45:06.651401 [info ] [Thread-3 (]: 5 of 7 OK loaded seed file analytics_raw.posts ................................. [[32mINSERT 8000[0m in 8.26s]
[0m14:45:06.715721 [debug] [Thread-3 (]: Finished running node seed.patreon_analytics.posts
[0m14:45:06.734382 [debug] [Thread-3 (]: Began running node seed.patreon_analytics.tiers
[0m14:45:06.747548 [info ] [Thread-3 (]: 6 of 7 START seed file analytics_raw.tiers ..................................... [RUN]
[0m14:45:06.790301 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.tiers) - Creating connection
[0m14:45:06.827185 [debug] [Thread-3 (]: Acquiring new databricks connection 'seed.patreon_analytics.tiers'
[0m14:45:06.827585 [debug] [Thread-3 (]: Began compiling node seed.patreon_analytics.tiers
[0m14:45:06.840579 [debug] [Thread-3 (]: Began executing node seed.patreon_analytics.tiers
[0m14:45:07.003593 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.tiers"
[0m14:45:07.004021 [debug] [Thread-3 (]: On seed.patreon_analytics.tiers: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.tiers"} */

    create or replace table `patreon_dev`.`analytics_raw`.`tiers` (tier_id string ,creator_id string ,tier_name string ,tier_rank bigint ,price_usd decimal(10,2) ,description string ,is_active boolean ,created_at timestamp ,archived_at bigint )
    
    using delta
  
    
    
    
    
    
  
[0m14:45:07.004243 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:45:07.224556 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m14:45:07.238953 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m14:45:07.407071 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m14:45:07.410260 [debug] [Thread-5 (]: On seed.patreon_analytics.patrons: 
          insert into `patreon_dev`.`analytics_raw`.`patrons` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,...
[0m14:45:07.964513 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0db78-8264-1659-be13-14559547b10a) - Created
[0m14:45:09.390046 [debug] [Thread-3 (]: SQL status: OK in 2.390 seconds
[0m14:45:09.397331 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0db78-8264-1659-be13-14559547b10a, command-id=01f0db78-827f-1f89-b3f5-9365ffdefb1e) - Closing
[0m14:45:09.625634 [debug] [Thread-5 (]: SQL status: OK in 2.210 seconds
[0m14:45:09.631177 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-79c8-1fb6-9814-621db08ed72b, command-id=01f0db78-823a-16b1-8ff4-3a1a0dc96c7a) - Closing
[0m14:45:09.645023 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.patreon_analytics.patrons"
[0m14:45:09.645750 [debug] [Thread-4 (]: SQL status: OK in 2.410 seconds
[0m14:45:09.671107 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db78-7a0a-1f60-84b5-7db38ce56301, command-id=01f0db78-8243-13f2-9529-c72e9c3c1398) - Closing
[0m14:45:09.719984 [debug] [Thread-5 (]: On seed.patreon_analytics.patrons: Close
[0m14:45:09.726619 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db78-79c8-1fb6-9814-621db08ed72b) - Closing
[0m14:45:09.938941 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1444bbd40>]}
[0m14:45:09.945674 [info ] [Thread-5 (]: 3 of 7 OK loaded seed file analytics_raw.patrons ............................... [[32mINSERT 15000[0m in 17.79s]
[0m14:45:09.957153 [debug] [Thread-5 (]: Finished running node seed.patreon_analytics.patrons
[0m14:45:09.976379 [debug] [Thread-5 (]: Began running node seed.patreon_analytics.transactions
[0m14:45:09.982696 [info ] [Thread-5 (]: 7 of 7 START seed file analytics_raw.transactions .............................. [RUN]
[0m14:45:10.007440 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.transactions) - Creating connection
[0m14:45:10.014093 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.patreon_analytics.transactions'
[0m14:45:10.020316 [debug] [Thread-5 (]: Began compiling node seed.patreon_analytics.transactions
[0m14:45:10.026242 [debug] [Thread-5 (]: Began executing node seed.patreon_analytics.transactions
[0m14:45:10.409373 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.tiers"
[0m14:45:10.422072 [debug] [Thread-3 (]: On seed.patreon_analytics.tiers: 
          insert overwrite `patreon_dev`.`analytics_raw`.`tiers` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%...
[0m14:45:12.599771 [debug] [Thread-3 (]: SQL status: OK in 2.130 seconds
[0m14:45:12.612822 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0db78-8264-1659-be13-14559547b10a, command-id=01f0db78-8446-153b-933e-6465f6d8809d) - Closing
[0m14:45:12.631988 [debug] [Thread-3 (]: Writing runtime SQL for node "seed.patreon_analytics.tiers"
[0m14:45:12.746211 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m14:45:12.757452 [debug] [Thread-6 (]: On seed.patreon_analytics.pledges: 
          insert into `patreon_dev`.`analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m14:45:12.758642 [debug] [Thread-3 (]: On seed.patreon_analytics.tiers: Close
[0m14:45:12.800622 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0db78-8264-1659-be13-14559547b10a) - Closing
[0m14:45:13.097281 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:45:13.097669 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x17bb686e0>]}
[0m14:45:13.109372 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.transactions"} */

    create  table `patreon_dev`.`analytics_raw`.`transactions` (transaction_id string ,pledge_id string ,patron_id string ,creator_id string ,transaction_type string ,transaction_status string ,gross_amount_usd decimal(10,2) ,platform_fee_usd decimal(10,2) ,processing_fee_usd decimal(10,2) ,net_amount_usd decimal(10,2) ,payment_method string ,failure_reason string ,transaction_at timestamp )
    
    using delta
  
    
    
    
    
    
  
[0m14:45:13.128363 [info ] [Thread-3 (]: 6 of 7 OK loaded seed file analytics_raw.tiers ................................. [[32mINSERT 1412[0m in 6.31s]
[0m14:45:13.147097 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m14:45:13.166132 [debug] [Thread-3 (]: Finished running node seed.patreon_analytics.tiers
[0m14:45:13.761333 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m14:45:13.764585 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m14:45:13.861491 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922) - Created
[0m14:45:15.292396 [debug] [Thread-6 (]: SQL status: OK in 2.520 seconds
[0m14:45:15.292812 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0db78-79ac-1581-a6fd-a619c0627225, command-id=01f0db78-8587-1ec3-be01-0c8ad8b64b37) - Closing
[0m14:45:15.742124 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m14:45:15.743143 [debug] [Thread-6 (]: On seed.patreon_analytics.pledges: 
          insert into `patreon_dev`.`analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m14:45:15.750840 [debug] [Thread-4 (]: SQL status: OK in 1.990 seconds
[0m14:45:15.751302 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db78-7a0a-1f60-84b5-7db38ce56301, command-id=01f0db78-8602-1455-b391-b5e20c193b88) - Closing
[0m14:45:15.911428 [debug] [Thread-5 (]: SQL status: OK in 2.760 seconds
[0m14:45:15.918592 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-8601-1421-9bbe-820e45d832c6) - Closing
[0m14:45:16.989768 [debug] [Thread-6 (]: SQL status: OK in 1.250 seconds
[0m14:45:17.001144 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0db78-79ac-1581-a6fd-a619c0627225, command-id=01f0db78-8724-15cf-acd8-3e8098fa284b) - Closing
[0m14:45:17.008601 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.patreon_analytics.pledges"
[0m14:45:17.060394 [debug] [Thread-6 (]: On seed.patreon_analytics.pledges: Close
[0m14:45:17.071348 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db78-79ac-1581-a6fd-a619c0627225) - Closing
[0m14:45:17.276959 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x161b025d0>]}
[0m14:45:17.304628 [info ] [Thread-6 (]: 4 of 7 OK loaded seed file analytics_raw.pledges ............................... [[32mINSERT 21310[0m in 25.13s]
[0m14:45:17.317990 [debug] [Thread-6 (]: Finished running node seed.patreon_analytics.pledges
[0m14:45:19.200655 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m14:45:19.210333 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m14:45:21.230937 [debug] [Thread-4 (]: SQL status: OK in 2.010 seconds
[0m14:45:21.236596 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:45:21.236899 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db78-7a0a-1f60-84b5-7db38ce56301, command-id=01f0db78-894d-1a6e-b3bb-33b2cbc309f9) - Closing
[0m14:45:21.242878 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert overwrite `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m14:45:22.940589 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m14:45:22.944030 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m14:45:25.008236 [debug] [Thread-5 (]: SQL status: OK in 3.750 seconds
[0m14:45:25.009184 [debug] [Thread-4 (]: SQL status: OK in 2.060 seconds
[0m14:45:25.009548 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-8af6-1690-98d2-4a6195ef3f8d) - Closing
[0m14:45:25.009808 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db78-7a0a-1f60-84b5-7db38ce56301, command-id=01f0db78-8b79-11e9-b2fb-5075adfd8cf5) - Closing
[0m14:45:28.133596 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m14:45:28.143264 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m14:45:29.922410 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:45:29.928482 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:45:30.271143 [debug] [Thread-4 (]: SQL status: OK in 2.120 seconds
[0m14:45:30.271574 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db78-7a0a-1f60-84b5-7db38ce56301, command-id=01f0db78-8ea0-12dd-affa-c2823380d257) - Closing
[0m14:45:30.319740 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m14:45:30.320181 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m14:45:32.750213 [debug] [Thread-5 (]: SQL status: OK in 2.820 seconds
[0m14:45:32.750778 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-8faf-1628-a412-09a14ff0f87a) - Closing
[0m14:45:33.513084 [debug] [Thread-4 (]: SQL status: OK in 3.190 seconds
[0m14:45:33.519845 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db78-7a0a-1f60-84b5-7db38ce56301, command-id=01f0db78-8fd3-1959-afca-27f2ed77fa8e) - Closing
[0m14:45:33.527029 [debug] [Thread-4 (]: Writing runtime SQL for node "seed.patreon_analytics.engagement_events"
[0m14:45:33.577521 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: Close
[0m14:45:33.584159 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db78-7a0a-1f60-84b5-7db38ce56301) - Closing
[0m14:45:33.790662 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1369b66f0>]}
[0m14:45:33.796620 [info ] [Thread-4 (]: 2 of 7 OK loaded seed file analytics_raw.engagement_events ..................... [[32mINSERT 60226[0m in 41.65s]
[0m14:45:33.802331 [debug] [Thread-4 (]: Finished running node seed.patreon_analytics.engagement_events
[0m14:45:36.370934 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:45:36.377092 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:45:38.848487 [debug] [Thread-5 (]: SQL status: OK in 2.470 seconds
[0m14:45:38.850527 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-937d-1b93-ba46-a5776f0208cd) - Closing
[0m14:45:42.293323 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:45:42.299521 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:45:44.875136 [debug] [Thread-5 (]: SQL status: OK in 2.580 seconds
[0m14:45:44.875858 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-9703-1f40-9542-81bc160ee14d) - Closing
[0m14:45:48.050213 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:45:48.056298 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:45:50.916804 [debug] [Thread-5 (]: SQL status: OK in 2.860 seconds
[0m14:45:50.918172 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-9a76-13a7-b487-c43cb9678c76) - Closing
[0m14:45:54.250616 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:45:54.256816 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:45:56.748624 [debug] [Thread-5 (]: SQL status: OK in 2.490 seconds
[0m14:45:56.749014 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-9e22-1acf-8b6f-488e694702e5) - Closing
[0m14:46:00.074457 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:46:00.081173 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:46:04.375268 [debug] [Thread-5 (]: SQL status: OK in 4.290 seconds
[0m14:46:04.376298 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-a19b-1040-bda1-a1b07e83cfe7) - Closing
[0m14:46:07.699295 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:46:07.705684 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:46:10.635007 [debug] [Thread-5 (]: SQL status: OK in 2.930 seconds
[0m14:46:10.635367 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-a62b-1234-ba26-b7dc2a530629) - Closing
[0m14:46:14.156261 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:46:14.162944 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:46:17.635605 [debug] [Thread-5 (]: SQL status: OK in 3.470 seconds
[0m14:46:17.635986 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-aa00-12c2-8590-61154f69ae14) - Closing
[0m14:46:21.252351 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:46:21.259281 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:46:25.115012 [debug] [Thread-5 (]: SQL status: OK in 3.860 seconds
[0m14:46:25.115345 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-ae3a-1981-8266-296a33492a05) - Closing
[0m14:46:28.767767 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:46:28.774544 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:46:32.543906 [debug] [Thread-5 (]: SQL status: OK in 3.770 seconds
[0m14:46:32.544466 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-b2b4-1ed3-a50b-22c2b8cb3e03) - Closing
[0m14:46:35.958782 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:46:35.966040 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:46:39.127033 [debug] [Thread-5 (]: SQL status: OK in 3.160 seconds
[0m14:46:39.127909 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-b6fe-17f5-bf79-2047cd3481bf) - Closing
[0m14:46:42.571060 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:46:42.577514 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:46:45.398737 [debug] [Thread-5 (]: SQL status: OK in 2.820 seconds
[0m14:46:45.400648 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-baee-14eb-8ebd-715580ea0f3d) - Closing
[0m14:46:48.723787 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:46:48.729993 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:46:51.518207 [debug] [Thread-5 (]: SQL status: OK in 2.790 seconds
[0m14:46:51.519133 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-bea4-134f-b79b-7a28b5182436) - Closing
[0m14:46:54.786780 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:46:54.793919 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:46:57.719964 [debug] [Thread-5 (]: SQL status: OK in 2.930 seconds
[0m14:46:57.721146 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-c241-1872-9ad4-d471fc23f98a) - Closing
[0m14:47:01.181370 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:47:01.187621 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:47:04.137783 [debug] [Thread-5 (]: SQL status: OK in 2.950 seconds
[0m14:47:04.138139 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-c612-1b7a-8a75-a4b351c91b4c) - Closing
[0m14:47:08.092987 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:47:08.099475 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:47:11.775037 [debug] [Thread-5 (]: SQL status: OK in 3.680 seconds
[0m14:47:11.775448 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-ca30-1089-bcfc-e321d0f44d40) - Closing
[0m14:47:15.218258 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:47:15.224899 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:47:19.068758 [debug] [Thread-5 (]: SQL status: OK in 3.840 seconds
[0m14:47:19.069788 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-ce6e-1e16-bfa8-b974cb7a90a7) - Closing
[0m14:47:20.891001 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m14:47:20.894416 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m14:47:23.819543 [debug] [Thread-5 (]: SQL status: OK in 2.920 seconds
[0m14:47:23.820791 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922, command-id=01f0db78-d1c4-15c8-a086-c6745996f948) - Closing
[0m14:47:23.822626 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.patreon_analytics.transactions"
[0m14:47:23.827113 [debug] [Thread-5 (]: On seed.patreon_analytics.transactions: Close
[0m14:47:23.827439 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db78-85eb-188e-b2f0-38e1166f0922) - Closing
[0m14:47:24.005012 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bea731b-ebd5-428c-9c26-42ad4d4de120', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x17aa815e0>]}
[0m14:47:24.005492 [info ] [Thread-5 (]: 7 of 7 OK loaded seed file analytics_raw.transactions .......................... [[32mINSERT 185141[0m in 134.00s]
[0m14:47:24.005848 [debug] [Thread-5 (]: Finished running node seed.patreon_analytics.transactions
[0m14:47:24.006872 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m14:47:24.007219 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:47:24.007653 [info ] [MainThread]: 
[0m14:47:24.008346 [info ] [MainThread]: Finished running 7 seeds in 0 hours 2 minutes and 35.24 seconds (155.24s).
[0m14:47:24.010206 [debug] [MainThread]: Command end result
[0m14:47:24.042598 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m14:47:24.046199 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m14:47:24.049762 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m14:47:24.050007 [info ] [MainThread]: 
[0m14:47:24.050247 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:47:24.050420 [info ] [MainThread]: 
[0m14:47:24.050614 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=7
[0m14:47:24.054647 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 157.03491, "process_in_blocks": "0", "process_kernel_time": 1.20933, "process_mem_max_rss": "706592768", "process_out_blocks": "0", "process_user_time": 91.42953}
[0m14:47:24.055006 [debug] [MainThread]: Command `dbt seed` succeeded at 14:47:24.054955 after 157.04 seconds
[0m14:47:24.055256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1234182f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124afaae0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123813ce0>]}
[0m14:47:24.055470 [debug] [MainThread]: Flushing usage events
[0m14:47:24.454464 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:47:26.791752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058deb70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111514e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111514f20>]}


============================== 14:47:26.795090 | a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5 ==============================
[0m14:47:26.795090 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:47:26.795477 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'no_print': 'None', 'static_parser': 'True', 'invocation_command': 'dbt run', 'warn_error': 'None', 'debug': 'False', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'empty': 'False', 'log_format': 'default', 'printer_width': '80', 'fail_fast': 'False', 'log_cache_events': 'False', 'use_colors': 'True', 'write_json': 'True', 'target_path': 'None', 'partial_parse': 'True', 'quiet': 'False', 'send_anonymous_usage_stats': 'True'}
[0m14:47:27.283012 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:47:27.283334 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:47:27.283508 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:47:27.909329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1247f85c0>]}
[0m14:47:27.942200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1144bc0e0>]}
[0m14:47:27.942721 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:47:28.065487 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:47:28.066062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12505b980>]}
[0m14:47:28.088852 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m14:47:28.242725 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:47:28.243040 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:47:28.243191 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:47:28.247799 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.intermediate
- models.patreon_analytics.marts.finance
[0m14:47:28.278685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12508b380>]}
[0m14:47:28.350481 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m14:47:28.353307 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m14:47:28.363018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125802960>]}
[0m14:47:28.363336 [info ] [MainThread]: Found 9 models, 36 data tests, 7 seeds, 4 metrics, 1126 macros, 1 semantic model
[0m14:47:28.363538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125807f20>]}
[0m14:47:28.365059 [info ] [MainThread]: 
[0m14:47:28.365311 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:47:28.365460 [info ] [MainThread]: 
[0m14:47:28.365813 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m14:47:28.365978 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:47:28.369632 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m14:47:28.369934 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m14:47:28.370197 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m14:47:28.372140 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m14:47:28.378579 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m14:47:28.378920 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m14:47:28.379180 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m14:47:28.379378 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m14:47:28.380993 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m14:47:28.382455 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m14:47:28.382654 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:28.382819 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m14:47:28.383092 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m14:47:28.383514 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:28.383832 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:29.020501 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d67a-14d1-aadc-3aa11ed07580) - Created
[0m14:47:29.089043 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d685-1f60-93ac-ea6f101b1267) - Created
[0m14:47:29.092419 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d685-1f87-b52b-5254bb63bdf2) - Created
[0m14:47:29.366931 [debug] [ThreadPool]: SQL status: OK in 0.980 seconds
[0m14:47:29.376175 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-d67a-14d1-aadc-3aa11ed07580, command-id=01f0db78-d690-1eae-95e5-8317e9829aa1) - Closing
[0m14:47:29.376606 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m14:47:29.376787 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d67a-14d1-aadc-3aa11ed07580) - Closing
[0m14:47:29.421667 [debug] [ThreadPool]: SQL status: OK in 1.040 seconds
[0m14:47:29.422938 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-d685-1f87-b52b-5254bb63bdf2, command-id=01f0db78-d69e-1332-9d58-9fde6eec46c5) - Closing
[0m14:47:29.423231 [debug] [ThreadPool]: SQL status: OK in 1.040 seconds
[0m14:47:29.423885 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-d685-1f60-93ac-ea6f101b1267, command-id=01f0db78-d69d-16ff-bee0-61c74c2ed771) - Closing
[0m14:47:29.533646 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m14:47:29.533958 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d685-1f87-b52b-5254bb63bdf2) - Closing
[0m14:47:29.709672 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m14:47:29.710583 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d685-1f60-93ac-ea6f101b1267) - Closing
[0m14:47:29.888959 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_patreon_dev_analytics) - Creating connection
[0m14:47:29.890373 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_patreon_dev_analytics_staging) - Creating connection
[0m14:47:29.894760 [debug] [ThreadPool]: Acquiring new databricks connection 'create_patreon_dev_analytics'
[0m14:47:29.896165 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_patreon_dev_analytics_marts) - Creating connection
[0m14:47:29.899572 [debug] [ThreadPool]: Acquiring new databricks connection 'create_patreon_dev_analytics_staging'
[0m14:47:29.901597 [debug] [ThreadPool]: Creating schema "database: "patreon_dev"
schema: "analytics"
"
[0m14:47:29.902516 [debug] [ThreadPool]: Acquiring new databricks connection 'create_patreon_dev_analytics_marts'
[0m14:47:29.903279 [debug] [ThreadPool]: Creating schema "database: "patreon_dev"
schema: "analytics_staging"
"
[0m14:47:29.920385 [debug] [ThreadPool]: Creating schema "database: "patreon_dev"
schema: "analytics_marts"
"
[0m14:47:29.921188 [debug] [ThreadPool]: Using databricks connection "create_patreon_dev_analytics"
[0m14:47:29.926871 [debug] [ThreadPool]: Using databricks connection "create_patreon_dev_analytics_staging"
[0m14:47:29.928243 [debug] [ThreadPool]: Using databricks connection "create_patreon_dev_analytics_marts"
[0m14:47:29.928471 [debug] [ThreadPool]: On create_patreon_dev_analytics: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "create_patreon_dev_analytics"} */
create schema if not exists `patreon_dev`.`analytics`
  
[0m14:47:29.928646 [debug] [ThreadPool]: On create_patreon_dev_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "create_patreon_dev_analytics_staging"} */
create schema if not exists `patreon_dev`.`analytics_staging`
  
[0m14:47:29.928807 [debug] [ThreadPool]: On create_patreon_dev_analytics_marts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "create_patreon_dev_analytics_marts"} */
create schema if not exists `patreon_dev`.`analytics_marts`
  
[0m14:47:29.928950 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:29.929081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:29.929212 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:30.556532 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d766-1500-88c9-7ac0dba836e9) - Created
[0m14:47:30.566274 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d766-17db-8503-55bec58906c0) - Created
[0m14:47:30.621552 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d76f-1698-a043-8787ba943e4d) - Created
[0m14:47:31.030235 [debug] [ThreadPool]: SQL status: OK in 1.100 seconds
[0m14:47:31.032529 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-d766-1500-88c9-7ac0dba836e9, command-id=01f0db78-d77b-13ae-9fea-5681a3defdca) - Closing
[0m14:47:31.033317 [debug] [ThreadPool]: On create_patreon_dev_analytics_staging: Close
[0m14:47:31.033829 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d766-1500-88c9-7ac0dba836e9) - Closing
[0m14:47:31.057996 [debug] [ThreadPool]: SQL status: OK in 1.130 seconds
[0m14:47:31.058624 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-d766-17db-8503-55bec58906c0, command-id=01f0db78-d77d-1189-9fe3-003edb0c4bec) - Closing
[0m14:47:31.115131 [debug] [ThreadPool]: SQL status: OK in 1.190 seconds
[0m14:47:31.115815 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-d76f-1698-a043-8787ba943e4d, command-id=01f0db78-d787-1d3c-a614-061a40e4e354) - Closing
[0m14:47:31.190857 [debug] [ThreadPool]: On create_patreon_dev_analytics_marts: Close
[0m14:47:31.191195 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d766-17db-8503-55bec58906c0) - Closing
[0m14:47:31.359439 [debug] [ThreadPool]: On create_patreon_dev_analytics: Close
[0m14:47:31.360170 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d76f-1698-a043-8787ba943e4d) - Closing
[0m14:47:31.535344 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_raw) - Creating connection
[0m14:47:31.535987 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics) - Creating connection
[0m14:47:31.536269 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_raw'
[0m14:47:31.536661 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_staging) - Creating connection
[0m14:47:31.537213 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_marts) - Creating connection
[0m14:47:31.537607 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics'
[0m14:47:31.538433 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_staging'
[0m14:47:31.539438 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_marts'
[0m14:47:31.544938 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics"
[0m14:47:31.550009 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_raw"
[0m14:47:31.552689 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_staging"
[0m14:47:31.555289 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_marts"
[0m14:47:31.555689 [debug] [ThreadPool]: On list_patreon_dev_analytics: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics'

  
[0m14:47:31.556096 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_raw'

  
[0m14:47:31.556466 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_staging'

  
[0m14:47:31.556786 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_marts'

  
[0m14:47:31.557054 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:31.557281 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:31.557502 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:31.557714 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:32.268510 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d86b-1210-a11a-affdf96fe193) - Created
[0m14:47:32.289800 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d86e-1a80-b9ab-9f85d1f2e6d6) - Created
[0m14:47:32.368161 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d87a-11b6-a7bd-3a19b8f3882d) - Created
[0m14:47:32.376002 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d879-1951-9a62-0554f71375c4) - Created
[0m14:47:32.721736 [debug] [ThreadPool]: SQL status: OK in 1.160 seconds
[0m14:47:32.725326 [debug] [ThreadPool]: SQL status: OK in 1.170 seconds
[0m14:47:32.727659 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-d86b-1210-a11a-affdf96fe193, command-id=01f0db78-d880-18c0-81d2-2bae2a099b28) - Closing
[0m14:47:32.729077 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-d86e-1a80-b9ab-9f85d1f2e6d6, command-id=01f0db78-d884-11db-9127-c8af443046e7) - Closing
[0m14:47:32.729567 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: Close
[0m14:47:32.730044 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d86b-1210-a11a-affdf96fe193) - Closing
[0m14:47:32.803774 [debug] [ThreadPool]: SQL status: OK in 1.250 seconds
[0m14:47:32.805544 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-d87a-11b6-a7bd-3a19b8f3882d, command-id=01f0db78-d893-1092-8546-fd43c26bd36b) - Closing
[0m14:47:32.845325 [debug] [ThreadPool]: SQL status: OK in 1.290 seconds
[0m14:47:32.847325 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db78-d879-1951-9a62-0554f71375c4, command-id=01f0db78-d893-1169-aa12-5eae959a7ae3) - Closing
[0m14:47:32.904228 [debug] [ThreadPool]: On list_patreon_dev_analytics: Close
[0m14:47:32.905102 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d86e-1a80-b9ab-9f85d1f2e6d6) - Closing
[0m14:47:33.066004 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: Close
[0m14:47:33.066354 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d87a-11b6-a7bd-3a19b8f3882d) - Closing
[0m14:47:33.245877 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: Close
[0m14:47:33.246300 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db78-d879-1951-9a62-0554f71375c4) - Closing
[0m14:47:33.419713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12585f950>]}
[0m14:47:33.422324 [debug] [Thread-4 (]: Began running node model.patreon_analytics.metricflow_time_spine
[0m14:47:33.422828 [debug] [Thread-5 (]: Began running node model.patreon_analytics.stg_creators
[0m14:47:33.423151 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_engagement_events
[0m14:47:33.423379 [debug] [Thread-7 (]: Began running node model.patreon_analytics.stg_patrons
[0m14:47:33.423757 [info ] [Thread-4 (]: 1 of 9 START sql table model analytics.metricflow_time_spine ................... [RUN]
[0m14:47:33.424468 [info ] [Thread-5 (]: 2 of 9 START sql view model analytics_staging.stg_creators ..................... [RUN]
[0m14:47:33.425083 [info ] [Thread-6 (]: 3 of 9 START sql view model analytics_staging.stg_engagement_events ............ [RUN]
[0m14:47:33.425614 [info ] [Thread-7 (]: 4 of 9 START sql view model analytics_staging.stg_patrons ...................... [RUN]
[0m14:47:33.426036 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.metricflow_time_spine) - Creating connection
[0m14:47:33.426518 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_creators) - Creating connection
[0m14:47:33.426954 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_engagement_events) - Creating connection
[0m14:47:33.427326 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_patrons) - Creating connection
[0m14:47:33.427632 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.metricflow_time_spine'
[0m14:47:33.427932 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_creators'
[0m14:47:33.428218 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_engagement_events'
[0m14:47:33.428450 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_patrons'
[0m14:47:33.428709 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.metricflow_time_spine
[0m14:47:33.428933 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.stg_creators
[0m14:47:33.429140 [debug] [Thread-6 (]: Began compiling node model.patreon_analytics.stg_engagement_events
[0m14:47:33.429343 [debug] [Thread-7 (]: Began compiling node model.patreon_analytics.stg_patrons
[0m14:47:33.436361 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.stg_creators"
[0m14:47:33.439736 [debug] [Thread-6 (]: Writing injected SQL for node "model.patreon_analytics.stg_engagement_events"
[0m14:47:33.442840 [debug] [Thread-7 (]: Writing injected SQL for node "model.patreon_analytics.stg_patrons"
[0m14:47:33.470735 [debug] [Thread-4 (]: Using databricks connection "model.patreon_analytics.metricflow_time_spine"
[0m14:47:33.471366 [debug] [Thread-4 (]: On model.patreon_analytics.metricflow_time_spine: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.metricflow_time_spine"} */


        select timestampdiff(day, date_trunc('day', cast('2020-01-01' as timestamp)), date_trunc('day', cast('2030-12-31' as timestamp)))
[0m14:47:33.471797 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:47:33.473219 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.stg_creators
[0m14:47:33.473560 [debug] [Thread-6 (]: Began executing node model.patreon_analytics.stg_engagement_events
[0m14:47:33.473856 [debug] [Thread-7 (]: Began executing node model.patreon_analytics.stg_patrons
[0m14:47:33.483554 [debug] [Thread-5 (]: MATERIALIZING VIEW
[0m14:47:33.485256 [debug] [Thread-6 (]: MATERIALIZING VIEW
[0m14:47:33.486800 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m14:47:33.488222 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:47:33.488696 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:47:33.489048 [warn ] [Thread-7 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:47:33.489284 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12580d850>]}
[0m14:47:33.489546 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12580d8b0>]}
[0m14:47:33.489818 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12580d940>]}
[0m14:47:33.497357 [debug] [Thread-6 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_engagement_events`
[0m14:47:33.497868 [debug] [Thread-5 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_creators`
[0m14:47:33.498487 [debug] [Thread-7 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_patrons`
[0m14:47:33.504956 [debug] [Thread-6 (]: Writing runtime sql for node "model.patreon_analytics.stg_engagement_events"
[0m14:47:33.505465 [debug] [Thread-5 (]: Writing runtime sql for node "model.patreon_analytics.stg_creators"
[0m14:47:33.505799 [debug] [Thread-7 (]: Writing runtime sql for node "model.patreon_analytics.stg_patrons"
[0m14:47:33.506510 [debug] [Thread-6 (]: Using databricks connection "model.patreon_analytics.stg_engagement_events"
[0m14:47:33.506756 [debug] [Thread-6 (]: On model.patreon_analytics.stg_engagement_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_engagement_events"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_engagement_events`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`engagement_events`
),

staged as (
    select
        event_id,
        patron_id,
        creator_id,
        post_id,
        event_type,
        event_at,
        
        -- Derived fields
        date_trunc('month', event_at) as event_month,
        date_trunc('day', event_at) as event_date,
        
        -- Engagement weighting (for composite scores)
        case event_type
            when 'view' then 1
            when 'like' then 3
            when 'comment' then 5
            when 'share' then 7
            else 1
        end as engagement_weight,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m14:47:33.507529 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:47:33.510440 [debug] [Thread-7 (]: Using databricks connection "model.patreon_analytics.stg_patrons"
[0m14:47:33.513173 [debug] [Thread-7 (]: On model.patreon_analytics.stg_patrons: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_patrons"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_patrons`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`patrons`
),

staged as (
    select
        patron_id,
        patron_name,
        email,
        country_code,
        created_at,
        first_pledge_at,
        coalesce(lifetime_spend_usd, 0) as lifetime_spend_usd,
        coalesce(status, 'active') as status,
        
        -- Derived fields
        datediff(month, created_at, current_timestamp()) as account_age_months,
        case 
            when lifetime_spend_usd >= 1000 then 'whale'
            when lifetime_spend_usd >= 500 then 'high_value'
            when lifetime_spend_usd >= 100 then 'regular'
            else 'casual'
        end as patron_value_tier,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m14:47:33.516358 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m14:47:33.511909 [debug] [Thread-5 (]: Using databricks connection "model.patreon_analytics.stg_creators"
[0m14:47:33.588518 [debug] [Thread-5 (]: On model.patreon_analytics.stg_creators: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_creators"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_creators`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`creators`
),

staged as (
    select
        creator_id,
        creator_name,
        email,
        category,
        subcategory,
        country_code,
        coalesce(currency_code, 'USD') as currency_code,
        coalesce(plan_type, 'pro') as plan_type,
        coalesce(is_nsfw, false) as is_nsfw,
        coalesce(is_verified, false) as is_verified,
        created_at,
        first_pledge_received_at,
        last_post_at,
        coalesce(status, 'active') as status,
        
        -- Derived fields
        datediff(day, created_at, coalesce(first_pledge_received_at, current_timestamp())) as days_to_first_pledge,
        datediff(month, created_at, current_timestamp()) as account_age_months,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m14:47:33.600476 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m14:47:34.164968 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db78-d98b-1ee8-b1d8-0b2518c2d677) - Created
[0m14:47:34.216566 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db78-d994-15b7-b693-81ba3571b6d8) - Created
[0m14:47:34.292294 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db78-d9a0-1089-a831-bb63392fed89) - Created
[0m14:47:34.298890 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0db78-d9a0-13a4-b74e-a48a212818de) - Created
[0m14:47:34.476172 [debug] [Thread-4 (]: SQL status: OK in 1.000 seconds
[0m14:47:34.482270 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db78-d98b-1ee8-b1d8-0b2518c2d677, command-id=01f0db78-d9a1-1fe2-b214-ce924e1eca4a) - Closing
[0m14:47:34.527915 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.metricflow_time_spine"
[0m14:47:34.528764 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.metricflow_time_spine
[0m14:47:34.540126 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m14:47:34.558126 [debug] [Thread-4 (]: Writing runtime sql for node "model.patreon_analytics.metricflow_time_spine"
[0m14:47:34.558811 [debug] [Thread-4 (]: Using databricks connection "model.patreon_analytics.metricflow_time_spine"
[0m14:47:34.559242 [debug] [Thread-4 (]: On model.patreon_analytics.metricflow_time_spine: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.metricflow_time_spine"} */

  
    
        create or replace table `patreon_dev`.`analytics`.`metricflow_time_spine`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      with days as (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
    
    

    )

    select *
    from unioned
    where generated_number <= 4017
    order by generated_number



),

all_periods as (

    select (
        timestampadd(day, (row_number() over (order by 1) - 1), cast('2020-01-01' as timestamp))
    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(timestampadd(year, -1, d.date_day) as date) as prior_year_date_day,
        cast(timestampadd(day, -364, d.date_day) as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(timestampadd(day, -1, d.date_day) as date) as prior_date_day,
    cast(timestampadd(day, 1, d.date_day) as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    date_part('dayofweek', d.date_day) as day_of_week,
    date_part('dayofweek_iso', d.date_day) as day_of_week_iso,
    date_format(d.date_day, 'EEEE') as day_of_week_name,
    date_format(d.date_day, 'E') as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    dayofyear(d.date_day) as day_of_year,

    cast(date_trunc('week', d.date_day) as date) as week_start_date,
    cast(
        timestampadd(day, -1, timestampadd(week, 1, date_trunc('week', d.date_day)))
        as date) as week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_week_start_date,
    cast(
        timestampadd(day, -1, timestampadd(week, 1, date_trunc('week', d.prior_year_over_year_date_day)))
        as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(timestampadd(day, 6, cast(date_trunc('week', d.date_day) as date)) as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(timestampadd(day, 6, cast(date_trunc('week', d.prior_year_over_year_date_day) as date)) as date) as prior_year_iso_week_end_date,
    cast(date_part('week', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    date_format(d.date_day, 'MMMM')  as month_name,
    date_format(d.date_day, 'MMM')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(month, 1, date_trunc('month', d.date_day)))
        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(month, 1, date_trunc('month', d.prior_year_date_day)))
        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(quarter, 1, date_trunc('quarter', d.date_day)))
        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(year, 1, date_trunc('year', d.date_day)))
        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

select
    date_day as date_day
from days
  
[0m14:47:34.908121 [debug] [Thread-5 (]: SQL status: OK in 1.320 seconds
[0m14:47:34.910135 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-d994-15b7-b693-81ba3571b6d8, command-id=01f0db78-d9aa-122f-9f2b-6ac6b946cd55) - Closing
[0m14:47:34.920247 [debug] [Thread-5 (]: Applying tags to relation None
[0m14:47:34.923251 [debug] [Thread-5 (]: On model.patreon_analytics.stg_creators: Close
[0m14:47:34.923602 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db78-d994-15b7-b693-81ba3571b6d8) - Closing
[0m14:47:34.928927 [debug] [Thread-7 (]: SQL status: OK in 1.410 seconds
[0m14:47:34.929727 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0db78-d9a0-13a4-b74e-a48a212818de, command-id=01f0db78-d9b9-148c-b913-db5aca17e96d) - Closing
[0m14:47:34.930201 [debug] [Thread-7 (]: Applying tags to relation None
[0m14:47:35.023600 [debug] [Thread-6 (]: SQL status: OK in 1.520 seconds
[0m14:47:35.025405 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0db78-d9a0-1089-a831-bb63392fed89, command-id=01f0db78-d9b8-1578-863b-174aa4f3904c) - Closing
[0m14:47:35.026449 [debug] [Thread-6 (]: Applying tags to relation None
[0m14:47:35.083300 [debug] [Thread-7 (]: On model.patreon_analytics.stg_patrons: Close
[0m14:47:35.084461 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0db78-d9a0-13a4-b74e-a48a212818de) - Closing
[0m14:47:35.252342 [debug] [Thread-6 (]: On model.patreon_analytics.stg_engagement_events: Close
[0m14:47:35.252674 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db78-d9a0-1089-a831-bb63392fed89) - Closing
[0m14:47:35.428363 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12580cc20>]}
[0m14:47:35.428855 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e9ff20>]}
[0m14:47:35.429208 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1259327e0>]}
[0m14:47:35.429831 [info ] [Thread-7 (]: 4 of 9 OK created sql view model analytics_staging.stg_patrons ................. [[32mOK[0m in 2.00s]
[0m14:47:35.430475 [info ] [Thread-5 (]: 2 of 9 OK created sql view model analytics_staging.stg_creators ................ [[32mOK[0m in 2.00s]
[0m14:47:35.432066 [debug] [Thread-7 (]: Finished running node model.patreon_analytics.stg_patrons
[0m14:47:35.430993 [info ] [Thread-6 (]: 3 of 9 OK created sql view model analytics_staging.stg_engagement_events ....... [[32mOK[0m in 2.00s]
[0m14:47:35.432710 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.stg_creators
[0m14:47:35.433087 [debug] [Thread-7 (]: Began running node model.patreon_analytics.stg_pledges
[0m14:47:35.433783 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_engagement_events
[0m14:47:35.434280 [debug] [Thread-5 (]: Began running node model.patreon_analytics.stg_posts
[0m14:47:35.434798 [info ] [Thread-7 (]: 5 of 9 START sql view model analytics_staging.stg_pledges ...................... [RUN]
[0m14:47:35.435252 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_tiers
[0m14:47:35.435718 [info ] [Thread-5 (]: 6 of 9 START sql view model analytics_staging.stg_posts ........................ [RUN]
[0m14:47:35.436300 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_pledges) - Creating connection
[0m14:47:35.436713 [info ] [Thread-6 (]: 7 of 9 START sql view model analytics_staging.stg_tiers ........................ [RUN]
[0m14:47:35.437145 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_posts) - Creating connection
[0m14:47:35.437476 [debug] [Thread-7 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_pledges'
[0m14:47:35.437884 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_tiers) - Creating connection
[0m14:47:35.438210 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_posts'
[0m14:47:35.438507 [debug] [Thread-7 (]: Began compiling node model.patreon_analytics.stg_pledges
[0m14:47:35.438809 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_tiers'
[0m14:47:35.439088 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.stg_posts
[0m14:47:35.443327 [debug] [Thread-7 (]: Writing injected SQL for node "model.patreon_analytics.stg_pledges"
[0m14:47:35.443741 [debug] [Thread-6 (]: Began compiling node model.patreon_analytics.stg_tiers
[0m14:47:35.533884 [debug] [Thread-6 (]: Writing injected SQL for node "model.patreon_analytics.stg_tiers"
[0m14:47:35.540419 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.stg_posts"
[0m14:47:35.540744 [debug] [Thread-7 (]: Began executing node model.patreon_analytics.stg_pledges
[0m14:47:35.542212 [debug] [Thread-7 (]: MATERIALIZING VIEW
[0m14:47:35.542742 [debug] [Thread-7 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_pledges`
[0m14:47:35.543079 [debug] [Thread-7 (]: Writing runtime sql for node "model.patreon_analytics.stg_pledges"
[0m14:47:35.543294 [debug] [Thread-6 (]: Began executing node model.patreon_analytics.stg_tiers
[0m14:47:35.544709 [debug] [Thread-6 (]: MATERIALIZING VIEW
[0m14:47:35.545156 [debug] [Thread-6 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_tiers`
[0m14:47:35.545511 [debug] [Thread-6 (]: Writing runtime sql for node "model.patreon_analytics.stg_tiers"
[0m14:47:35.545976 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.stg_posts
[0m14:47:35.546223 [debug] [Thread-7 (]: Using databricks connection "model.patreon_analytics.stg_pledges"
[0m14:47:35.547929 [debug] [Thread-5 (]: MATERIALIZING VIEW
[0m14:47:35.548257 [debug] [Thread-7 (]: On model.patreon_analytics.stg_pledges: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_pledges"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_pledges`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`pledges`
),

staged as (
    select
        pledge_id,
        patron_id,
        creator_id,
        tier_id,
        pledge_amount_usd,
        pledge_status,
        coalesce(is_first_pledge, false) as is_first_pledge,
        started_at,
        ended_at,
        pause_started_at,
        churn_reason,
        
        -- Derived fields
        date_trunc('month', started_at) as pledge_month,
        case 
            when pledge_status = 'active' then null
            else datediff(day, started_at, coalesce(ended_at, current_timestamp()))
        end as pledge_duration_days,
        
        -- Is currently active (for point-in-time analysis)
        case 
            when pledge_status = 'active' and pause_started_at is null then true
            else false
        end as is_currently_active,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m14:47:35.548812 [debug] [Thread-5 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_posts`
[0m14:47:35.549033 [debug] [Thread-6 (]: Using databricks connection "model.patreon_analytics.stg_tiers"
[0m14:47:35.549344 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m14:47:35.549772 [debug] [Thread-5 (]: Writing runtime sql for node "model.patreon_analytics.stg_posts"
[0m14:47:35.550057 [debug] [Thread-6 (]: On model.patreon_analytics.stg_tiers: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_tiers"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_tiers`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`tiers`
),

staged as (
    select
        tier_id,
        creator_id,
        tier_name,
        tier_rank,
        price_usd,
        description,
        coalesce(is_active, true) as is_active,
        created_at,
        archived_at,
        
        -- Price bucket for analysis
        case 
            when price_usd <= 5 then 'micro'
            when price_usd <= 15 then 'standard'
            when price_usd <= 30 then 'premium'
            else 'whale'
        end as price_bucket,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m14:47:35.550464 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:47:35.551168 [debug] [Thread-5 (]: Using databricks connection "model.patreon_analytics.stg_posts"
[0m14:47:35.551506 [debug] [Thread-5 (]: On model.patreon_analytics.stg_posts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_posts"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_posts`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`posts`
),

staged as (
    select
        post_id,
        creator_id,
        title,
        post_type,
        access_level,
        minimum_tier_id,
        published_at,
        coalesce(is_pinned, false) as is_pinned,
        
        -- Derived fields
        date_trunc('month', published_at) as published_month,
        date_trunc('day', published_at) as published_date,
        
        -- Content categorization
        case 
            when access_level = 'public' then 'free'
            when access_level = 'patrons_only' then 'paywalled'
            when access_level = 'tier_specific' then 'premium'
            else 'unknown'
        end as content_access_type,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m14:47:35.551766 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m14:47:36.154464 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db78-dabc-1cf3-9138-43fa94eb7b9c) - Created
[0m14:47:36.232013 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db78-dac8-1544-917e-fa69b2aa9e65) - Created
[0m14:47:36.238367 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0db78-dac9-16f0-a747-a9421edaefe4) - Created
[0m14:47:36.740896 [debug] [Thread-5 (]: SQL status: OK in 1.190 seconds
[0m14:47:36.743449 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-dabc-1cf3-9138-43fa94eb7b9c, command-id=01f0db78-dad1-15f9-bd9b-0df5e096e4ab) - Closing
[0m14:47:36.744737 [debug] [Thread-5 (]: Applying tags to relation None
[0m14:47:36.746416 [debug] [Thread-5 (]: On model.patreon_analytics.stg_posts: Close
[0m14:47:36.747083 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db78-dabc-1cf3-9138-43fa94eb7b9c) - Closing
[0m14:47:36.833290 [debug] [Thread-6 (]: SQL status: OK in 1.280 seconds
[0m14:47:36.834115 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0db78-dac8-1544-917e-fa69b2aa9e65, command-id=01f0db78-dae2-1b72-aca9-955724fdd31e) - Closing
[0m14:47:36.834587 [debug] [Thread-6 (]: Applying tags to relation None
[0m14:47:36.858938 [debug] [Thread-7 (]: SQL status: OK in 1.310 seconds
[0m14:47:36.859721 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0db78-dac9-16f0-a747-a9421edaefe4, command-id=01f0db78-dae0-1652-8500-8e0064c1d97b) - Closing
[0m14:47:36.860169 [debug] [Thread-7 (]: Applying tags to relation None
[0m14:47:36.910432 [debug] [Thread-6 (]: On model.patreon_analytics.stg_tiers: Close
[0m14:47:36.910778 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db78-dac8-1544-917e-fa69b2aa9e65) - Closing
[0m14:47:37.076087 [debug] [Thread-7 (]: On model.patreon_analytics.stg_pledges: Close
[0m14:47:37.076410 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0db78-dac9-16f0-a747-a9421edaefe4) - Closing
[0m14:47:37.250580 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125999d30>]}
[0m14:47:37.252534 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1258e2f30>]}
[0m14:47:37.253683 [info ] [Thread-6 (]: 7 of 9 OK created sql view model analytics_staging.stg_tiers ................... [[32mOK[0m in 1.81s]
[0m14:47:37.254466 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_tiers
[0m14:47:37.251971 [info ] [Thread-5 (]: 6 of 9 OK created sql view model analytics_staging.stg_posts ................... [[32mOK[0m in 1.81s]
[0m14:47:37.257231 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.stg_posts
[0m14:47:37.255778 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_transactions
[0m14:47:37.256382 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1258e1af0>]}
[0m14:47:37.258795 [info ] [Thread-6 (]: 8 of 9 START sql view model analytics_staging.stg_transactions ................. [RUN]
[0m14:47:37.260822 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_transactions) - Creating connection
[0m14:47:37.261378 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_transactions'
[0m14:47:37.260089 [info ] [Thread-7 (]: 5 of 9 OK created sql view model analytics_staging.stg_pledges ................. [[32mOK[0m in 1.82s]
[0m14:47:37.262080 [debug] [Thread-6 (]: Began compiling node model.patreon_analytics.stg_transactions
[0m14:47:37.262467 [debug] [Thread-7 (]: Finished running node model.patreon_analytics.stg_pledges
[0m14:47:37.265149 [debug] [Thread-6 (]: Writing injected SQL for node "model.patreon_analytics.stg_transactions"
[0m14:47:37.265843 [debug] [Thread-6 (]: Began executing node model.patreon_analytics.stg_transactions
[0m14:47:37.273032 [debug] [Thread-6 (]: MATERIALIZING VIEW
[0m14:47:37.274369 [debug] [Thread-6 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_transactions`
[0m14:47:37.275114 [debug] [Thread-6 (]: Writing runtime sql for node "model.patreon_analytics.stg_transactions"
[0m14:47:37.275899 [debug] [Thread-6 (]: Using databricks connection "model.patreon_analytics.stg_transactions"
[0m14:47:37.276349 [debug] [Thread-6 (]: On model.patreon_analytics.stg_transactions: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_transactions"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_transactions`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`transactions`
),

staged as (
    select
        transaction_id,
        pledge_id,
        patron_id,
        creator_id,
        transaction_type,
        transaction_status,
        gross_amount_usd,
        platform_fee_usd,
        processing_fee_usd,
        net_amount_usd,
        payment_method,
        failure_reason,
        transaction_at,
        
        -- Derived fields
        date_trunc('month', transaction_at) as transaction_month,
        date_trunc('day', transaction_at) as transaction_date,
        
        -- Fee analysis
        case 
            when gross_amount_usd > 0 and transaction_status = 'succeeded'
            then round((platform_fee_usd / gross_amount_usd) * 100, 2)
            else null
        end as platform_fee_rate_pct,
        
        case 
            when gross_amount_usd > 0 and transaction_status = 'succeeded'
            then round((processing_fee_usd / gross_amount_usd) * 100, 2)
            else null
        end as processing_fee_rate_pct,
        
        -- Success flag
        case when transaction_status = 'succeeded' then 1 else 0 end as is_successful,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m14:47:37.276637 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:47:37.822640 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db78-dbba-18e6-9624-fcac11232a62) - Created
[0m14:47:38.537339 [debug] [Thread-6 (]: SQL status: OK in 1.260 seconds
[0m14:47:38.539055 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0db78-dbba-18e6-9624-fcac11232a62, command-id=01f0db78-dbd0-1270-a8ed-b4a1d130aba7) - Closing
[0m14:47:38.540045 [debug] [Thread-6 (]: Applying tags to relation None
[0m14:47:38.541313 [debug] [Thread-6 (]: On model.patreon_analytics.stg_transactions: Close
[0m14:47:38.541873 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db78-dbba-18e6-9624-fcac11232a62) - Closing
[0m14:47:38.696605 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1258f7b30>]}
[0m14:47:38.698007 [info ] [Thread-6 (]: 8 of 9 OK created sql view model analytics_staging.stg_transactions ............ [[32mOK[0m in 1.44s]
[0m14:47:38.698875 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_transactions
[0m14:47:38.699769 [debug] [Thread-5 (]: Began running node model.patreon_analytics.fct_creator_monthly_performance
[0m14:47:38.700662 [info ] [Thread-5 (]: 9 of 9 START sql table model analytics_marts.fct_creator_monthly_performance ... [RUN]
[0m14:47:38.701585 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.fct_creator_monthly_performance) - Creating connection
[0m14:47:38.702170 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.fct_creator_monthly_performance'
[0m14:47:38.702693 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.fct_creator_monthly_performance
[0m14:47:38.724280 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.fct_creator_monthly_performance"
[0m14:47:38.725860 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.fct_creator_monthly_performance
[0m14:47:38.728555 [debug] [Thread-5 (]: MATERIALIZING TABLE
[0m14:47:38.730137 [debug] [Thread-5 (]: Writing runtime sql for node "model.patreon_analytics.fct_creator_monthly_performance"
[0m14:47:38.731464 [debug] [Thread-5 (]: Using databricks connection "model.patreon_analytics.fct_creator_monthly_performance"
[0m14:47:38.732487 [debug] [Thread-5 (]: On model.patreon_analytics.fct_creator_monthly_performance: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.fct_creator_monthly_performance"} */

  
    
        create or replace table `patreon_dev`.`analytics_marts`.`fct_creator_monthly_performance`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      with creators as (
    select * from `patreon_dev`.`analytics_staging`.`stg_creators`
),

pledges as (
    select * from `patreon_dev`.`analytics_staging`.`stg_pledges`
),

transactions as (
    select * from `patreon_dev`.`analytics_staging`.`stg_transactions`
),

tiers as (
    select * from `patreon_dev`.`analytics_staging`.`stg_tiers`
),

posts as (
    select * from `patreon_dev`.`analytics_staging`.`stg_posts`
),

engagement as (
    select * from `patreon_dev`.`analytics_staging`.`stg_engagement_events`
),

-- Generate month spine from earliest pledge to current month
months as (
    select distinct date_trunc('month', transaction_at)::date as month_start_date
    from transactions
),

-- Create creator-month combinations
creator_months as (
    select 
        c.creator_id,
        m.month_start_date,
        md5(cast(concat(coalesce(cast(c.creator_id as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(m.month_start_date as string), '_dbt_utils_surrogate_key_null_')) as string)) as creator_month_key
    from creators c
    cross join months m
    where m.month_start_date >= date_trunc('month', c.first_pledge_received_at)
),

-- Monthly pledge metrics
pledge_metrics as (
    select
        p.creator_id,
        date_trunc('month', t.transaction_at)::date as month_start_date,
        
        -- Patron counts
        count(distinct p.patron_id) as total_patrons,
        count(distinct case when p.pledge_status = 'active' then p.patron_id end) as active_patrons,
        count(distinct case when p.is_first_pledge = true then p.patron_id end) as new_patrons,
        count(distinct case when p.pledge_status = 'churned' then p.patron_id end) as churned_patrons,
        
        -- MRR
        sum(case when p.pledge_status = 'active' then p.pledge_amount_usd else 0 end) as gross_mrr_usd,
        sum(case when p.is_first_pledge = true then p.pledge_amount_usd else 0 end) as new_mrr_usd,
        sum(case when p.pledge_status = 'churned' then p.pledge_amount_usd else 0 end) as churned_mrr_usd,
        
        -- Tier distribution
        count(distinct case when ti.tier_rank = 1 then p.patron_id end) as tier_1_patrons,
        count(distinct case when ti.tier_rank = 2 then p.patron_id end) as tier_2_patrons,
        count(distinct case when ti.tier_rank >= 3 then p.patron_id end) as tier_3_plus_patrons,
        
        -- ARPP
        avg(p.pledge_amount_usd) as avg_pledge_amount_usd
        
    from pledges p
    inner join transactions t on p.pledge_id = t.pledge_id
    left join tiers ti on p.tier_id = ti.tier_id
    group by 1, 2
),

-- Monthly revenue (actual collections)
revenue_metrics as (
    select
        creator_id,
        date_trunc('month', transaction_at)::date as month_start_date,
        
        sum(case when transaction_status = 'succeeded' then gross_amount_usd else 0 end) as gross_revenue_usd,
        sum(case when transaction_status = 'succeeded' then platform_fee_usd else 0 end) as platform_fees_usd,
        sum(case when transaction_status = 'succeeded' then processing_fee_usd else 0 end) as processing_fees_usd,
        sum(case when transaction_status = 'succeeded' then net_amount_usd else 0 end) as net_creator_earnings_usd,
        
        count(case when transaction_status = 'succeeded' then 1 end) as successful_transactions,
        count(case when transaction_status = 'failed' then 1 end) as failed_transactions,
        sum(case when transaction_status = 'failed' then gross_amount_usd else 0 end) as declined_amount_usd
        
    from transactions
    where transaction_type = 'pledge_payment'
    group by 1, 2
),

-- Monthly content metrics
content_metrics as (
    select
        creator_id,
        published_month as month_start_date,
        
        count(distinct post_id) as posts_published,
        count(distinct case when content_access_type = 'paywalled' then post_id end) as paywalled_posts,
        count(distinct case when content_access_type = 'free' then post_id end) as free_posts
        
    from posts
    group by 1, 2
),

-- Monthly engagement metrics
engagement_metrics as (
    select
        creator_id,
        event_month as month_start_date,
        
        count(case when event_type = 'view' then 1 end) as total_views,
        count(case when event_type = 'like' then 1 end) as total_likes,
        count(case when event_type = 'comment' then 1 end) as total_comments,
        count(distinct patron_id) as engaged_patrons,
        sum(engagement_weight) as total_engagement_score
        
    from engagement
    group by 1, 2
),

-- Previous month for growth calculations
lagged as (
    select
        creator_id,
        month_start_date,
        lag(gross_mrr_usd) over (partition by creator_id order by month_start_date) as prev_mrr,
        lag(active_patrons) over (partition by creator_id order by month_start_date) as prev_patrons
    from pledge_metrics
)

select
    cm.creator_month_key,
    cm.creator_id,
    cm.month_start_date,
    
    -- Creator attributes
    c.creator_name,
    c.category as creator_category,
    c.plan_type,
    c.country_code as creator_country,
    
    -- Patron metrics
    coalesce(pm.total_patrons, 0) as total_patrons,
    coalesce(pm.active_patrons, 0) as active_patrons,
    coalesce(pm.new_patrons, 0) as new_patrons,
    coalesce(pm.churned_patrons, 0) as churned_patrons,
    coalesce(pm.active_patrons, 0) - coalesce(l.prev_patrons, 0) as net_patron_change,
    
    case 
        when coalesce(l.prev_patrons, 0) > 0 
        then round((pm.churned_patrons * 100.0 / l.prev_patrons), 2)
        else 0 
    end as patron_churn_rate_pct,
    
    -- MRR metrics
    coalesce(pm.gross_mrr_usd, 0) as gross_mrr_usd,
    coalesce(pm.new_mrr_usd, 0) as new_mrr_usd,
    coalesce(pm.churned_mrr_usd, 0) as churned_mrr_usd,
    coalesce(pm.gross_mrr_usd, 0) - coalesce(l.prev_mrr, 0) as mrr_change_usd,
    
    case 
        when coalesce(l.prev_mrr, 0) > 0 
        then round(((pm.gross_mrr_usd - l.prev_mrr) * 100.0 / l.prev_mrr), 2)
        else null 
    end as mrr_growth_rate_pct,
    
    -- Revenue metrics
    coalesce(rm.gross_revenue_usd, 0) as gross_revenue_usd,
    coalesce(rm.platform_fees_usd, 0) as platform_fees_usd,
    coalesce(rm.processing_fees_usd, 0) as processing_fees_usd,
    coalesce(rm.net_creator_earnings_usd, 0) as net_creator_earnings_usd,
    
    case 
        when coalesce(pm.gross_mrr_usd, 0) > 0 
        then round((rm.gross_revenue_usd * 100.0 / pm.gross_mrr_usd), 2)
        else null 
    end as collection_rate_pct,
    
    -- Payment health
    coalesce(rm.successful_transactions, 0) as successful_transactions,
    coalesce(rm.failed_transactions, 0) as failed_transactions,
    coalesce(rm.declined_amount_usd, 0) as declined_amount_usd,
    
    case 
        when coalesce(rm.successful_transactions, 0) + coalesce(rm.failed_transactions, 0) > 0
        then round((rm.failed_transactions * 100.0 / (rm.successful_transactions + rm.failed_transactions)), 2)
        else 0 
    end as decline_rate_pct,
    
    -- Tier distribution
    coalesce(pm.tier_1_patrons, 0) as tier_1_patrons,
    coalesce(pm.tier_2_patrons, 0) as tier_2_patrons,
    coalesce(pm.tier_3_plus_patrons, 0) as tier_3_plus_patrons,
    coalesce(pm.avg_pledge_amount_usd, 0) as avg_pledge_amount_usd,
    
    -- Content metrics
    coalesce(cnt.posts_published, 0) as posts_published,
    coalesce(cnt.paywalled_posts, 0) as paywalled_posts,
    coalesce(cnt.free_posts, 0) as free_posts,
    
    -- Engagement metrics
    coalesce(eng.total_views, 0) as total_views,
    coalesce(eng.total_likes, 0) as total_likes,
    coalesce(eng.total_comments, 0) as total_comments,
    coalesce(eng.engaged_patrons, 0) as engaged_patrons,
    coalesce(eng.total_engagement_score, 0) as total_engagement_score,
    
    case 
        when coalesce(pm.active_patrons, 0) > 0 
        then round((eng.engaged_patrons * 100.0 / pm.active_patrons), 2)
        else null 
    end as patron_engagement_rate_pct,
    
    current_timestamp() as updated_at

from creator_months cm
left join creators c on cm.creator_id = c.creator_id
left join pledge_metrics pm on cm.creator_id = pm.creator_id and cm.month_start_date = pm.month_start_date
left join revenue_metrics rm on cm.creator_id = rm.creator_id and cm.month_start_date = rm.month_start_date
left join content_metrics cnt on cm.creator_id = cnt.creator_id and cm.month_start_date = cnt.month_start_date
left join engagement_metrics eng on cm.creator_id = eng.creator_id and cm.month_start_date = eng.month_start_date
left join lagged l on cm.creator_id = l.creator_id and cm.month_start_date = l.month_start_date
  
[0m14:47:38.733149 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m14:47:39.060432 [debug] [Thread-4 (]: SQL status: OK in 4.500 seconds
[0m14:47:39.061130 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db78-d98b-1ee8-b1d8-0b2518c2d677, command-id=01f0db78-d9de-1471-99d5-eae4e4bd2294) - Closing
[0m14:47:39.061520 [debug] [Thread-4 (]: Applying tags to relation None
[0m14:47:39.075000 [debug] [Thread-4 (]: On model.patreon_analytics.metricflow_time_spine: Close
[0m14:47:39.075384 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db78-d98b-1ee8-b1d8-0b2518c2d677) - Closing
[0m14:47:39.238466 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1259c3f80>]}
[0m14:47:39.239045 [info ] [Thread-4 (]: 1 of 9 OK created sql table model analytics.metricflow_time_spine .............. [[32mOK[0m in 5.81s]
[0m14:47:39.239394 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.metricflow_time_spine
[0m14:47:39.267163 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db78-dc96-16a7-9475-f9c6419b9093) - Created
[0m14:47:47.372714 [debug] [Thread-5 (]: SQL status: OK in 8.640 seconds
[0m14:47:47.378648 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db78-dc96-16a7-9475-f9c6419b9093, command-id=01f0db78-dcac-11fc-a123-79d045cbfa7d) - Closing
[0m14:47:47.567985 [debug] [Thread-5 (]: Applying tags to relation None
[0m14:47:47.570296 [debug] [Thread-5 (]: On model.patreon_analytics.fct_creator_monthly_performance: Close
[0m14:47:47.570622 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db78-dc96-16a7-9475-f9c6419b9093) - Closing
[0m14:47:47.738427 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a88b65a4-9ec7-4bc8-bd66-28414ef8f5b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125994770>]}
[0m14:47:47.739429 [info ] [Thread-5 (]: 9 of 9 OK created sql table model analytics_marts.fct_creator_monthly_performance  [[32mOK[0m in 9.04s]
[0m14:47:47.739952 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.fct_creator_monthly_performance
[0m14:47:47.741643 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m14:47:47.742002 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:47:47.742482 [info ] [MainThread]: 
[0m14:47:47.742735 [info ] [MainThread]: Finished running 2 table models, 7 view models in 0 hours 0 minutes and 19.38 seconds (19.38s).
[0m14:47:47.743684 [debug] [MainThread]: Command end result
[0m14:47:47.779662 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m14:47:47.783339 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m14:47:47.787388 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m14:47:47.787600 [info ] [MainThread]: 
[0m14:47:47.787820 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:47:47.787976 [info ] [MainThread]: 
[0m14:47:47.788184 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=9
[0m14:47:47.791481 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 21.044615, "process_in_blocks": "0", "process_kernel_time": 0.508248, "process_mem_max_rss": "283082752", "process_out_blocks": "0", "process_user_time": 6.261385}
[0m14:47:47.791969 [debug] [MainThread]: Command `dbt run` succeeded at 14:47:47.791915 after 21.05 seconds
[0m14:47:47.792349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111392900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1250b2390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1259685c0>]}
[0m14:47:47.792575 [debug] [MainThread]: Flushing usage events
[0m14:47:48.332578 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:20:32.018952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121b89280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1247323f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124731b80>]}


============================== 15:20:32.023219 | 381af38c-eb06-486a-a388-af42513eafd3 ==============================
[0m15:20:32.023219 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m15:20:32.023596 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'use_colors': 'True', 'no_print': 'None', 'log_cache_events': 'False', 'warn_error': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'target_path': 'None', 'indirect_selection': 'eager', 'quiet': 'False', 'debug': 'False', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'partial_parse': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run', 'version_check': 'True', 'use_experimental_parser': 'False', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'empty': 'False', 'static_parser': 'True'}
[0m15:20:32.618862 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:20:32.619313 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:20:32.619506 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:20:33.644741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x164f8e0c0>]}
[0m15:20:33.677691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1664a35c0>]}
[0m15:20:33.678158 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m15:20:33.819959 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:20:33.820541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123807170>]}
[0m15:20:33.843120 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m15:20:33.944542 [warn ] [MainThread]: [[33mWARNING[0m][PropertyMovedToConfigDeprecation]: Deprecated functionality
Found `meta` as a top-level property of `models[0].columns[0]` in file
`models/marts/core/schema.yml`. The `meta` top-level property should be moved
into the `config` of `models[0].columns[0]`.
[0m15:20:33.944996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123ff5370>]}
[0m15:20:34.021217 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:20:34.021526 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m15:20:34.021672 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:20:34.026432 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.marts.finance
- models.patreon_analytics.intermediate
[0m15:20:34.059233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1669a66f0>]}
[0m15:20:34.142002 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m15:20:34.145587 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m15:20:34.157370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169c089e0>]}
[0m15:20:34.157715 [info ] [MainThread]: Found 9 models, 36 data tests, 7 seeds, 4 metrics, 1126 macros, 1 semantic model
[0m15:20:34.157911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169c63e90>]}
[0m15:20:34.159329 [info ] [MainThread]: 
[0m15:20:34.159526 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:20:34.159673 [info ] [MainThread]: 
[0m15:20:34.159993 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:20:34.160146 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:20:34.163652 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m15:20:34.164042 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m15:20:34.164432 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m15:20:34.170582 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m15:20:34.173008 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m15:20:34.173501 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m15:20:34.173909 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m15:20:34.174127 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m15:20:34.175771 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m15:20:34.177081 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m15:20:34.177253 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:34.177407 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m15:20:34.177554 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m15:20:34.177921 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:34.178261 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:35.215932 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-764e-1a67-a21b-b023239261e6) - Created
[0m15:20:35.227629 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7650-17a3-a827-9f1d216be395) - Created
[0m15:20:35.229018 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7651-1caa-977e-8474421e6bc4) - Created
[0m15:20:42.391544 [debug] [ThreadPool]: SQL status: OK in 8.210 seconds
[0m15:20:42.391971 [debug] [ThreadPool]: SQL status: OK in 8.210 seconds
[0m15:20:42.392519 [debug] [ThreadPool]: SQL status: OK in 8.210 seconds
[0m15:20:42.400524 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db7d-7651-1caa-977e-8474421e6bc4, command-id=01f0db7d-767b-1fee-9432-7b477ec0edb4) - Closing
[0m15:20:42.401520 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db7d-764e-1a67-a21b-b023239261e6, command-id=01f0db7d-767a-194a-8661-b481fe79c319) - Closing
[0m15:20:42.402188 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db7d-7650-17a3-a827-9f1d216be395, command-id=01f0db7d-767b-1311-b943-5e175c14282d) - Closing
[0m15:20:42.605196 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m15:20:42.605904 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7651-1caa-977e-8474421e6bc4) - Closing
[0m15:20:42.781229 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m15:20:42.782133 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-764e-1a67-a21b-b023239261e6) - Closing
[0m15:20:42.971294 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m15:20:42.973447 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7650-17a3-a827-9f1d216be395) - Closing
[0m15:20:43.140084 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_marts) - Creating connection
[0m15:20:43.141599 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics) - Creating connection
[0m15:20:43.142423 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_marts'
[0m15:20:43.146248 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_raw) - Creating connection
[0m15:20:43.147407 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics'
[0m15:20:43.148344 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_staging) - Creating connection
[0m15:20:43.161061 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_marts"
[0m15:20:43.161722 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_raw'
[0m15:20:43.167351 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics"
[0m15:20:43.167675 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_staging'
[0m15:20:43.167934 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_marts'

  
[0m15:20:43.170492 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_raw"
[0m15:20:43.170824 [debug] [ThreadPool]: On list_patreon_dev_analytics: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics'

  
[0m15:20:43.172518 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_staging"
[0m15:20:43.172687 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:43.172984 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_raw'

  
[0m15:20:43.173320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:43.173710 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_staging'

  
[0m15:20:43.174259 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:43.175488 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:44.179671 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7bb4-1605-8888-572e99120d22) - Created
[0m15:20:44.180471 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7bb4-1ac2-bfae-b224a2cd0911) - Created
[0m15:20:44.181940 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7bb5-12aa-9cba-f8a39dff1ce3) - Created
[0m15:20:44.182776 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7bb6-11c5-8e74-53d953cfa2a3) - Created
[0m15:20:46.757395 [debug] [ThreadPool]: SQL status: OK in 3.580 seconds
[0m15:20:46.758952 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db7d-7bb5-12aa-9cba-f8a39dff1ce3, command-id=01f0db7d-7bce-13e8-9a9b-efc0b198282c) - Closing
[0m15:20:46.759362 [debug] [ThreadPool]: On list_patreon_dev_analytics: Close
[0m15:20:46.759540 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7bb5-12aa-9cba-f8a39dff1ce3) - Closing
[0m15:20:47.108117 [debug] [ThreadPool]: SQL status: OK in 3.940 seconds
[0m15:20:47.109936 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db7d-7bb4-1ac2-bfae-b224a2cd0911, command-id=01f0db7d-7bce-18bc-8f59-3253c2c960c1) - Closing
[0m15:20:47.110362 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: Close
[0m15:20:47.110544 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7bb4-1ac2-bfae-b224a2cd0911) - Closing
[0m15:20:47.112906 [debug] [ThreadPool]: SQL status: OK in 3.940 seconds
[0m15:20:47.114420 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db7d-7bb4-1605-8888-572e99120d22, command-id=01f0db7d-7bce-1377-acf6-2d15c7d4905e) - Closing
[0m15:20:47.125285 [debug] [ThreadPool]: SQL status: OK in 3.950 seconds
[0m15:20:47.127900 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0db7d-7bb6-11c5-8e74-53d953cfa2a3, command-id=01f0db7d-7bce-1275-85ca-e74e0d4d518e) - Closing
[0m15:20:47.278078 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: Close
[0m15:20:47.278764 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7bb4-1605-8888-572e99120d22) - Closing
[0m15:20:47.467675 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: Close
[0m15:20:47.467911 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0db7d-7bb6-11c5-8e74-53d953cfa2a3) - Closing
[0m15:20:47.625970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x166ca1850>]}
[0m15:20:47.628821 [debug] [Thread-3 (]: Began running node model.patreon_analytics.metricflow_time_spine
[0m15:20:47.629128 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_creators
[0m15:20:47.629449 [info ] [Thread-3 (]: 1 of 9 START sql table model analytics.metricflow_time_spine ................... [RUN]
[0m15:20:47.629739 [info ] [Thread-4 (]: 2 of 9 START sql view model analytics_staging.stg_creators ..................... [RUN]
[0m15:20:47.629972 [debug] [Thread-5 (]: Began running node model.patreon_analytics.stg_engagement_events
[0m15:20:47.630299 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.metricflow_time_spine) - Creating connection
[0m15:20:47.630494 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_patrons
[0m15:20:47.630764 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_creators) - Creating connection
[0m15:20:47.631174 [info ] [Thread-5 (]: 3 of 9 START sql view model analytics_staging.stg_engagement_events ............ [RUN]
[0m15:20:47.631432 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.patreon_analytics.metricflow_time_spine'
[0m15:20:47.631844 [info ] [Thread-6 (]: 4 of 9 START sql view model analytics_staging.stg_patrons ...................... [RUN]
[0m15:20:47.632207 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_creators'
[0m15:20:47.632807 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_engagement_events) - Creating connection
[0m15:20:47.633093 [debug] [Thread-3 (]: Began compiling node model.patreon_analytics.metricflow_time_spine
[0m15:20:47.633416 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_patrons) - Creating connection
[0m15:20:47.633631 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.stg_creators
[0m15:20:47.633869 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_engagement_events'
[0m15:20:47.640260 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_patrons'
[0m15:20:47.649530 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.stg_creators"
[0m15:20:47.665866 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.stg_engagement_events
[0m15:20:47.668857 [debug] [Thread-3 (]: Using databricks connection "model.patreon_analytics.metricflow_time_spine"
[0m15:20:47.669626 [debug] [Thread-6 (]: Began compiling node model.patreon_analytics.stg_patrons
[0m15:20:47.673075 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.stg_engagement_events"
[0m15:20:47.673641 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.metricflow_time_spine"} */


        select timestampdiff(day, date_trunc('day', cast('2020-01-01' as timestamp)), date_trunc('day', cast('2030-12-31' as timestamp)))
[0m15:20:47.676181 [debug] [Thread-6 (]: Writing injected SQL for node "model.patreon_analytics.stg_patrons"
[0m15:20:47.676639 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.stg_creators
[0m15:20:47.676953 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:20:47.683275 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.stg_engagement_events
[0m15:20:47.686408 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m15:20:47.688359 [debug] [Thread-5 (]: MATERIALIZING VIEW
[0m15:20:47.688710 [debug] [Thread-6 (]: Began executing node model.patreon_analytics.stg_patrons
[0m15:20:47.691321 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:20:47.691965 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:20:47.693788 [debug] [Thread-6 (]: MATERIALIZING VIEW
[0m15:20:47.694391 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169cd4530>]}
[0m15:20:47.694690 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16aa11c10>]}
[0m15:20:47.695057 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:20:47.703443 [debug] [Thread-4 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_creators`
[0m15:20:47.704052 [debug] [Thread-5 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_engagement_events`
[0m15:20:47.704259 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169d59610>]}
[0m15:20:47.709877 [debug] [Thread-4 (]: Writing runtime sql for node "model.patreon_analytics.stg_creators"
[0m15:20:47.710330 [debug] [Thread-5 (]: Writing runtime sql for node "model.patreon_analytics.stg_engagement_events"
[0m15:20:47.710745 [debug] [Thread-6 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_patrons`
[0m15:20:47.711585 [debug] [Thread-6 (]: Writing runtime sql for node "model.patreon_analytics.stg_patrons"
[0m15:20:47.712044 [debug] [Thread-5 (]: Using databricks connection "model.patreon_analytics.stg_engagement_events"
[0m15:20:47.712388 [debug] [Thread-5 (]: On model.patreon_analytics.stg_engagement_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_engagement_events"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_engagement_events`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`engagement_events`
),

staged as (
    select
        event_id,
        patron_id,
        creator_id,
        post_id,
        event_type,
        event_at,
        
        -- Derived fields
        date_trunc('month', event_at) as event_month,
        date_trunc('day', event_at) as event_date,
        
        -- Engagement weighting (for composite scores)
        case event_type
            when 'view' then 1
            when 'like' then 3
            when 'comment' then 5
            when 'share' then 7
            else 1
        end as engagement_weight,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:20:47.712712 [debug] [Thread-4 (]: Using databricks connection "model.patreon_analytics.stg_creators"
[0m15:20:47.712912 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m15:20:47.713205 [debug] [Thread-4 (]: On model.patreon_analytics.stg_creators: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_creators"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_creators`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`creators`
),

staged as (
    select
        creator_id,
        creator_name,
        email,
        category,
        subcategory,
        country_code,
        coalesce(currency_code, 'USD') as currency_code,
        coalesce(plan_type, 'pro') as plan_type,
        coalesce(is_nsfw, false) as is_nsfw,
        coalesce(is_verified, false) as is_verified,
        created_at,
        first_pledge_received_at,
        last_post_at,
        coalesce(status, 'active') as status,
        
        -- Derived fields
        datediff(day, created_at, coalesce(first_pledge_received_at, current_timestamp())) as days_to_first_pledge,
        datediff(month, created_at, current_timestamp()) as account_age_months,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:20:47.713452 [debug] [Thread-6 (]: Using databricks connection "model.patreon_analytics.stg_patrons"
[0m15:20:47.713765 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:20:47.714062 [debug] [Thread-6 (]: On model.patreon_analytics.stg_patrons: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_patrons"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_patrons`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`patrons`
),

staged as (
    select
        patron_id,
        patron_name,
        email,
        country_code,
        created_at,
        first_pledge_at,
        coalesce(lifetime_spend_usd, 0) as lifetime_spend_usd,
        coalesce(status, 'active') as status,
        
        -- Derived fields
        datediff(month, created_at, current_timestamp()) as account_age_months,
        case 
            when lifetime_spend_usd >= 1000 then 'whale'
            when lifetime_spend_usd >= 500 then 'high_value'
            when lifetime_spend_usd >= 100 then 'regular'
            else 'casual'
        end as patron_value_tier,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:20:47.714505 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m15:20:48.345354 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0db7d-7e33-1ecd-a674-12ba540ba18d) - Created
[0m15:20:48.517351 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db7d-7e47-15fb-9d00-416d53915c59) - Created
[0m15:20:48.519461 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db7d-7e47-1a58-a8b6-e3837b803200) - Created
[0m15:20:48.520214 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db7d-7e47-1068-ba55-ccfa33df48c5) - Created
[0m15:20:48.719421 [debug] [Thread-3 (]: SQL status: OK in 1.040 seconds
[0m15:20:48.721514 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0db7d-7e33-1ecd-a674-12ba540ba18d, command-id=01f0db7d-7e48-1ff6-b7f5-d59035dc32b5) - Closing
[0m15:20:48.757913 [debug] [Thread-3 (]: Writing injected SQL for node "model.patreon_analytics.metricflow_time_spine"
[0m15:20:48.758706 [debug] [Thread-3 (]: Began executing node model.patreon_analytics.metricflow_time_spine
[0m15:20:48.771409 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m15:20:48.897070 [debug] [Thread-3 (]: Writing runtime sql for node "model.patreon_analytics.metricflow_time_spine"
[0m15:20:48.898109 [debug] [Thread-3 (]: Using databricks connection "model.patreon_analytics.metricflow_time_spine"
[0m15:20:48.898795 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.metricflow_time_spine"} */

  
    
        create or replace table `patreon_dev`.`analytics`.`metricflow_time_spine`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      with days as (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
    
    

    )

    select *
    from unioned
    where generated_number <= 4017
    order by generated_number



),

all_periods as (

    select (
        timestampadd(day, (row_number() over (order by 1) - 1), cast('2020-01-01' as timestamp))
    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(timestampadd(year, -1, d.date_day) as date) as prior_year_date_day,
        cast(timestampadd(day, -364, d.date_day) as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(timestampadd(day, -1, d.date_day) as date) as prior_date_day,
    cast(timestampadd(day, 1, d.date_day) as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    date_part('dayofweek', d.date_day) as day_of_week,
    date_part('dayofweek_iso', d.date_day) as day_of_week_iso,
    date_format(d.date_day, 'EEEE') as day_of_week_name,
    date_format(d.date_day, 'E') as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    dayofyear(d.date_day) as day_of_year,

    cast(date_trunc('week', d.date_day) as date) as week_start_date,
    cast(
        timestampadd(day, -1, timestampadd(week, 1, date_trunc('week', d.date_day)))
        as date) as week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_week_start_date,
    cast(
        timestampadd(day, -1, timestampadd(week, 1, date_trunc('week', d.prior_year_over_year_date_day)))
        as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(timestampadd(day, 6, cast(date_trunc('week', d.date_day) as date)) as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(timestampadd(day, 6, cast(date_trunc('week', d.prior_year_over_year_date_day) as date)) as date) as prior_year_iso_week_end_date,
    cast(date_part('week', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    date_format(d.date_day, 'MMMM')  as month_name,
    date_format(d.date_day, 'MMM')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(month, 1, date_trunc('month', d.date_day)))
        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(month, 1, date_trunc('month', d.prior_year_date_day)))
        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(quarter, 1, date_trunc('quarter', d.date_day)))
        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(year, 1, date_trunc('year', d.date_day)))
        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

select
    date_day as date_day
from days
  
[0m15:20:49.600477 [debug] [Thread-6 (]: SQL status: OK in 1.890 seconds
[0m15:20:49.601604 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0db7d-7e47-15fb-9d00-416d53915c59, command-id=01f0db7d-7e64-1804-92f4-840853785f79) - Closing
[0m15:20:49.608315 [debug] [Thread-6 (]: Applying tags to relation None
[0m15:20:49.610926 [debug] [Thread-6 (]: On model.patreon_analytics.stg_patrons: Close
[0m15:20:49.611387 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db7d-7e47-15fb-9d00-416d53915c59) - Closing
[0m15:20:49.611730 [debug] [Thread-5 (]: SQL status: OK in 1.900 seconds
[0m15:20:49.612578 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db7d-7e47-1a58-a8b6-e3837b803200, command-id=01f0db7d-7e64-1693-95fb-f790d1cea0d8) - Closing
[0m15:20:49.612968 [debug] [Thread-5 (]: Applying tags to relation None
[0m15:20:49.628512 [debug] [Thread-4 (]: SQL status: OK in 1.910 seconds
[0m15:20:49.629192 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db7d-7e47-1068-ba55-ccfa33df48c5, command-id=01f0db7d-7e64-10d8-9db5-e9c3ee84179c) - Closing
[0m15:20:49.629591 [debug] [Thread-4 (]: Applying tags to relation None
[0m15:20:49.763696 [debug] [Thread-5 (]: On model.patreon_analytics.stg_engagement_events: Close
[0m15:20:49.764690 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db7d-7e47-1a58-a8b6-e3837b803200) - Closing
[0m15:20:49.923233 [debug] [Thread-4 (]: On model.patreon_analytics.stg_creators: Close
[0m15:20:49.927345 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db7d-7e47-1068-ba55-ccfa33df48c5) - Closing
[0m15:20:50.089024 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16aa5bcb0>]}
[0m15:20:50.089497 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16707fc50>]}
[0m15:20:50.089842 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16aa5bda0>]}
[0m15:20:50.090463 [info ] [Thread-6 (]: 4 of 9 OK created sql view model analytics_staging.stg_patrons ................. [[32mOK[0m in 2.45s]
[0m15:20:50.091044 [info ] [Thread-5 (]: 3 of 9 OK created sql view model analytics_staging.stg_engagement_events ....... [[32mOK[0m in 2.46s]
[0m15:20:50.091411 [info ] [Thread-4 (]: 2 of 9 OK created sql view model analytics_staging.stg_creators ................ [[32mOK[0m in 2.46s]
[0m15:20:50.091744 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_patrons
[0m15:20:50.092011 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.stg_engagement_events
[0m15:20:50.092431 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_creators
[0m15:20:50.092690 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_pledges
[0m15:20:50.092893 [debug] [Thread-5 (]: Began running node model.patreon_analytics.stg_posts
[0m15:20:50.093143 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_tiers
[0m15:20:50.093357 [info ] [Thread-6 (]: 5 of 9 START sql view model analytics_staging.stg_pledges ...................... [RUN]
[0m15:20:50.093573 [info ] [Thread-5 (]: 6 of 9 START sql view model analytics_staging.stg_posts ........................ [RUN]
[0m15:20:50.093779 [info ] [Thread-4 (]: 7 of 9 START sql view model analytics_staging.stg_tiers ........................ [RUN]
[0m15:20:50.094075 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_pledges) - Creating connection
[0m15:20:50.094489 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_posts) - Creating connection
[0m15:20:50.094899 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_tiers) - Creating connection
[0m15:20:50.095146 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_pledges'
[0m15:20:50.095331 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_posts'
[0m15:20:50.095589 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_tiers'
[0m15:20:50.095837 [debug] [Thread-6 (]: Began compiling node model.patreon_analytics.stg_pledges
[0m15:20:50.096049 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.stg_posts
[0m15:20:50.096236 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.stg_tiers
[0m15:20:50.098789 [debug] [Thread-6 (]: Writing injected SQL for node "model.patreon_analytics.stg_pledges"
[0m15:20:50.102393 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.stg_posts"
[0m15:20:50.105217 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.stg_tiers"
[0m15:20:50.105802 [debug] [Thread-6 (]: Began executing node model.patreon_analytics.stg_pledges
[0m15:20:50.106078 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.stg_posts
[0m15:20:50.107880 [debug] [Thread-6 (]: MATERIALIZING VIEW
[0m15:20:50.108228 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.stg_tiers
[0m15:20:50.110151 [debug] [Thread-5 (]: MATERIALIZING VIEW
[0m15:20:50.110851 [debug] [Thread-6 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_pledges`
[0m15:20:50.114266 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m15:20:50.115015 [debug] [Thread-5 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_posts`
[0m15:20:50.115583 [debug] [Thread-6 (]: Writing runtime sql for node "model.patreon_analytics.stg_pledges"
[0m15:20:50.116199 [debug] [Thread-4 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_tiers`
[0m15:20:50.117020 [debug] [Thread-5 (]: Writing runtime sql for node "model.patreon_analytics.stg_posts"
[0m15:20:50.117725 [debug] [Thread-4 (]: Writing runtime sql for node "model.patreon_analytics.stg_tiers"
[0m15:20:50.118304 [debug] [Thread-6 (]: Using databricks connection "model.patreon_analytics.stg_pledges"
[0m15:20:50.118639 [debug] [Thread-6 (]: On model.patreon_analytics.stg_pledges: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_pledges"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_pledges`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`pledges`
),

staged as (
    select
        pledge_id,
        patron_id,
        creator_id,
        tier_id,
        pledge_amount_usd,
        pledge_status,
        coalesce(is_first_pledge, false) as is_first_pledge,
        started_at,
        ended_at,
        pause_started_at,
        churn_reason,
        
        -- Derived fields
        date_trunc('month', started_at) as pledge_month,
        case 
            when pledge_status = 'active' then null
            else datediff(day, started_at, coalesce(ended_at, current_timestamp()))
        end as pledge_duration_days,
        
        -- Is currently active (for point-in-time analysis)
        case 
            when pledge_status = 'active' and pause_started_at is null then true
            else false
        end as is_currently_active,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:20:50.119074 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m15:20:50.119375 [debug] [Thread-4 (]: Using databricks connection "model.patreon_analytics.stg_tiers"
[0m15:20:50.119650 [debug] [Thread-5 (]: Using databricks connection "model.patreon_analytics.stg_posts"
[0m15:20:50.120132 [debug] [Thread-4 (]: On model.patreon_analytics.stg_tiers: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_tiers"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_tiers`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`tiers`
),

staged as (
    select
        tier_id,
        creator_id,
        tier_name,
        tier_rank,
        price_usd,
        description,
        coalesce(is_active, true) as is_active,
        created_at,
        archived_at,
        
        -- Price bucket for analysis
        case 
            when price_usd <= 5 then 'micro'
            when price_usd <= 15 then 'standard'
            when price_usd <= 30 then 'premium'
            else 'whale'
        end as price_bucket,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:20:50.120580 [debug] [Thread-5 (]: On model.patreon_analytics.stg_posts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_posts"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_posts`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`posts`
),

staged as (
    select
        post_id,
        creator_id,
        title,
        post_type,
        access_level,
        minimum_tier_id,
        published_at,
        coalesce(is_pinned, false) as is_pinned,
        
        -- Derived fields
        date_trunc('month', published_at) as published_month,
        date_trunc('day', published_at) as published_date,
        
        -- Content categorization
        case 
            when access_level = 'public' then 'free'
            when access_level = 'patrons_only' then 'paywalled'
            when access_level = 'tier_specific' then 'premium'
            else 'unknown'
        end as content_access_type,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:20:50.120971 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:20:50.121345 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m15:20:50.945823 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db7d-7fb4-19b6-90d9-a547667b3950) - Created
[0m15:20:50.947794 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db7d-7fb5-157e-b56e-55e4e03e63e4) - Created
[0m15:20:50.949525 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db7d-7fb5-12c5-9c55-9ae1cbb2a051) - Created
[0m15:20:51.691661 [debug] [Thread-5 (]: SQL status: OK in 1.570 seconds
[0m15:20:51.692604 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db7d-7fb4-19b6-90d9-a547667b3950, command-id=01f0db7d-7fd7-189a-94bf-d00d95196340) - Closing
[0m15:20:51.693143 [debug] [Thread-5 (]: Applying tags to relation None
[0m15:20:51.693801 [debug] [Thread-5 (]: On model.patreon_analytics.stg_posts: Close
[0m15:20:51.694065 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db7d-7fb4-19b6-90d9-a547667b3950) - Closing
[0m15:20:51.714833 [debug] [Thread-4 (]: SQL status: OK in 1.590 seconds
[0m15:20:51.716104 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db7d-7fb5-157e-b56e-55e4e03e63e4, command-id=01f0db7d-7fd7-19c8-a73d-e9ce99e05e4e) - Closing
[0m15:20:51.716654 [debug] [Thread-4 (]: Applying tags to relation None
[0m15:20:51.744283 [debug] [Thread-6 (]: SQL status: OK in 1.630 seconds
[0m15:20:51.745995 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0db7d-7fb5-12c5-9c55-9ae1cbb2a051, command-id=01f0db7d-7fd7-1b6c-b9d2-4bfc7c166876) - Closing
[0m15:20:51.746725 [debug] [Thread-6 (]: Applying tags to relation None
[0m15:20:51.849514 [debug] [Thread-4 (]: On model.patreon_analytics.stg_tiers: Close
[0m15:20:51.850395 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db7d-7fb5-157e-b56e-55e4e03e63e4) - Closing
[0m15:20:52.006700 [debug] [Thread-6 (]: On model.patreon_analytics.stg_pledges: Close
[0m15:20:52.007336 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0db7d-7fb5-12c5-9c55-9ae1cbb2a051) - Closing
[0m15:20:52.190469 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169c07c50>]}
[0m15:20:52.191079 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1668bb950>]}
[0m15:20:52.191325 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1668ebd10>]}
[0m15:20:52.191709 [info ] [Thread-5 (]: 6 of 9 OK created sql view model analytics_staging.stg_posts ................... [[32mOK[0m in 2.10s]
[0m15:20:52.192147 [info ] [Thread-4 (]: 7 of 9 OK created sql view model analytics_staging.stg_tiers ................... [[32mOK[0m in 2.10s]
[0m15:20:52.192926 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.stg_posts
[0m15:20:52.192498 [info ] [Thread-6 (]: 5 of 9 OK created sql view model analytics_staging.stg_pledges ................. [[32mOK[0m in 2.10s]
[0m15:20:52.193263 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_tiers
[0m15:20:52.193525 [debug] [Thread-5 (]: Began running node model.patreon_analytics.stg_transactions
[0m15:20:52.193961 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_pledges
[0m15:20:52.194369 [info ] [Thread-5 (]: 8 of 9 START sql view model analytics_staging.stg_transactions ................. [RUN]
[0m15:20:52.194771 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_transactions) - Creating connection
[0m15:20:52.194959 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_transactions'
[0m15:20:52.195129 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.stg_transactions
[0m15:20:52.201246 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.stg_transactions"
[0m15:20:52.201841 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.stg_transactions
[0m15:20:52.203606 [debug] [Thread-5 (]: MATERIALIZING VIEW
[0m15:20:52.204172 [debug] [Thread-5 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_transactions`
[0m15:20:52.204510 [debug] [Thread-5 (]: Writing runtime sql for node "model.patreon_analytics.stg_transactions"
[0m15:20:52.204967 [debug] [Thread-5 (]: Using databricks connection "model.patreon_analytics.stg_transactions"
[0m15:20:52.205269 [debug] [Thread-5 (]: On model.patreon_analytics.stg_transactions: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_transactions"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_transactions`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`transactions`
),

staged as (
    select
        transaction_id,
        pledge_id,
        patron_id,
        creator_id,
        transaction_type,
        transaction_status,
        gross_amount_usd,
        platform_fee_usd,
        processing_fee_usd,
        net_amount_usd,
        payment_method,
        failure_reason,
        transaction_at,
        
        -- Derived fields
        date_trunc('month', transaction_at) as transaction_month,
        date_trunc('day', transaction_at) as transaction_date,
        
        -- Fee analysis
        case 
            when gross_amount_usd > 0 and transaction_status = 'succeeded'
            then round((platform_fee_usd / gross_amount_usd) * 100, 2)
            else null
        end as platform_fee_rate_pct,
        
        case 
            when gross_amount_usd > 0 and transaction_status = 'succeeded'
            then round((processing_fee_usd / gross_amount_usd) * 100, 2)
            else null
        end as processing_fee_rate_pct,
        
        -- Success flag
        case when transaction_status = 'succeeded' then 1 else 0 end as is_successful,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:20:52.205526 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m15:20:52.850689 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db7d-80e3-1fe7-8039-324db3ec079d) - Created
[0m15:20:53.638883 [debug] [Thread-5 (]: SQL status: OK in 1.430 seconds
[0m15:20:53.641016 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0db7d-80e3-1fe7-8039-324db3ec079d, command-id=01f0db7d-80f8-1f11-a9cd-2e540d94e8ea) - Closing
[0m15:20:53.641611 [debug] [Thread-5 (]: Applying tags to relation None
[0m15:20:53.642087 [debug] [Thread-5 (]: On model.patreon_analytics.stg_transactions: Close
[0m15:20:53.642333 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0db7d-80e3-1fe7-8039-324db3ec079d) - Closing
[0m15:20:53.798726 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169d81a00>]}
[0m15:20:53.799280 [info ] [Thread-5 (]: 8 of 9 OK created sql view model analytics_staging.stg_transactions ............ [[32mOK[0m in 1.60s]
[0m15:20:53.799617 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.stg_transactions
[0m15:20:53.799972 [debug] [Thread-4 (]: Began running node model.patreon_analytics.fct_creator_monthly_performance
[0m15:20:53.800243 [info ] [Thread-4 (]: 9 of 9 START sql table model analytics_marts.fct_creator_monthly_performance ... [RUN]
[0m15:20:53.801081 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.fct_creator_monthly_performance) - Creating connection
[0m15:20:53.801784 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.fct_creator_monthly_performance'
[0m15:20:53.802343 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.fct_creator_monthly_performance
[0m15:20:53.816509 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.fct_creator_monthly_performance"
[0m15:20:53.817312 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.fct_creator_monthly_performance
[0m15:20:53.822608 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m15:20:53.823557 [debug] [Thread-4 (]: Writing runtime sql for node "model.patreon_analytics.fct_creator_monthly_performance"
[0m15:20:53.824136 [debug] [Thread-4 (]: Using databricks connection "model.patreon_analytics.fct_creator_monthly_performance"
[0m15:20:53.824837 [debug] [Thread-4 (]: On model.patreon_analytics.fct_creator_monthly_performance: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.fct_creator_monthly_performance"} */

  
    
        create or replace table `patreon_dev`.`analytics_marts`.`fct_creator_monthly_performance`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      with creators as (
    select * from `patreon_dev`.`analytics_staging`.`stg_creators`
),

pledges as (
    select * from `patreon_dev`.`analytics_staging`.`stg_pledges`
),

transactions as (
    select * from `patreon_dev`.`analytics_staging`.`stg_transactions`
),

tiers as (
    select * from `patreon_dev`.`analytics_staging`.`stg_tiers`
),

posts as (
    select * from `patreon_dev`.`analytics_staging`.`stg_posts`
),

engagement as (
    select * from `patreon_dev`.`analytics_staging`.`stg_engagement_events`
),

-- Generate month spine from earliest pledge to current month
months as (
    select distinct date_trunc('month', transaction_at)::date as month_start_date
    from transactions
),

-- Create creator-month combinations
creator_months as (
    select 
        c.creator_id,
        m.month_start_date,
        md5(cast(concat(coalesce(cast(c.creator_id as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(m.month_start_date as string), '_dbt_utils_surrogate_key_null_')) as string)) as creator_month_key
    from creators c
    cross join months m
    where m.month_start_date >= date_trunc('month', c.first_pledge_received_at)
),

-- Monthly pledge metrics
pledge_metrics as (
    select
        p.creator_id,
        date_trunc('month', t.transaction_at)::date as month_start_date,
        
        -- Patron counts
        count(distinct p.patron_id) as total_patrons,
        count(distinct case when p.pledge_status = 'active' then p.patron_id end) as active_patrons,
        count(distinct case when p.is_first_pledge = true then p.patron_id end) as new_patrons,
        count(distinct case when p.pledge_status = 'churned' then p.patron_id end) as churned_patrons,
        
        -- MRR
        sum(case when p.pledge_status = 'active' then p.pledge_amount_usd else 0 end) as gross_mrr_usd,
        sum(case when p.is_first_pledge = true then p.pledge_amount_usd else 0 end) as new_mrr_usd,
        sum(case when p.pledge_status = 'churned' then p.pledge_amount_usd else 0 end) as churned_mrr_usd,
        
        -- Tier distribution
        count(distinct case when ti.tier_rank = 1 then p.patron_id end) as tier_1_patrons,
        count(distinct case when ti.tier_rank = 2 then p.patron_id end) as tier_2_patrons,
        count(distinct case when ti.tier_rank >= 3 then p.patron_id end) as tier_3_plus_patrons,
        
        -- ARPP
        avg(p.pledge_amount_usd) as avg_pledge_amount_usd
        
    from pledges p
    inner join transactions t on p.pledge_id = t.pledge_id
    left join tiers ti on p.tier_id = ti.tier_id
    group by 1, 2
),

-- Monthly revenue (actual collections)
revenue_metrics as (
    select
        creator_id,
        date_trunc('month', transaction_at)::date as month_start_date,
        
        sum(case when transaction_status = 'succeeded' then gross_amount_usd else 0 end) as gross_revenue_usd,
        sum(case when transaction_status = 'succeeded' then platform_fee_usd else 0 end) as platform_fees_usd,
        sum(case when transaction_status = 'succeeded' then processing_fee_usd else 0 end) as processing_fees_usd,
        sum(case when transaction_status = 'succeeded' then net_amount_usd else 0 end) as net_creator_earnings_usd,
        
        count(case when transaction_status = 'succeeded' then 1 end) as successful_transactions,
        count(case when transaction_status = 'failed' then 1 end) as failed_transactions,
        sum(case when transaction_status = 'failed' then gross_amount_usd else 0 end) as declined_amount_usd
        
    from transactions
    where transaction_type = 'pledge_payment'
    group by 1, 2
),

-- Monthly content metrics
content_metrics as (
    select
        creator_id,
        published_month as month_start_date,
        
        count(distinct post_id) as posts_published,
        count(distinct case when content_access_type = 'paywalled' then post_id end) as paywalled_posts,
        count(distinct case when content_access_type = 'free' then post_id end) as free_posts
        
    from posts
    group by 1, 2
),

-- Monthly engagement metrics
engagement_metrics as (
    select
        creator_id,
        event_month as month_start_date,
        
        count(case when event_type = 'view' then 1 end) as total_views,
        count(case when event_type = 'like' then 1 end) as total_likes,
        count(case when event_type = 'comment' then 1 end) as total_comments,
        count(distinct patron_id) as engaged_patrons,
        sum(engagement_weight) as total_engagement_score
        
    from engagement
    group by 1, 2
),

-- Previous month for growth calculations
lagged as (
    select
        creator_id,
        month_start_date,
        lag(gross_mrr_usd) over (partition by creator_id order by month_start_date) as prev_mrr,
        lag(active_patrons) over (partition by creator_id order by month_start_date) as prev_patrons
    from pledge_metrics
)

select
    cm.creator_month_key,
    cm.creator_id,
    cm.month_start_date,
    
    -- Creator attributes
    c.creator_name,
    c.category as creator_category,
    c.plan_type,
    c.country_code as creator_country,
    
    -- Patron metrics
    coalesce(pm.total_patrons, 0) as total_patrons,
    coalesce(pm.active_patrons, 0) as active_patrons,
    coalesce(pm.new_patrons, 0) as new_patrons,
    coalesce(pm.churned_patrons, 0) as churned_patrons,
    coalesce(pm.active_patrons, 0) - coalesce(l.prev_patrons, 0) as net_patron_change,
    
    case 
        when coalesce(l.prev_patrons, 0) > 0 
        then round((pm.churned_patrons * 100.0 / l.prev_patrons), 2)
        else 0 
    end as patron_churn_rate_pct,
    
    -- MRR metrics
    coalesce(pm.gross_mrr_usd, 0) as gross_mrr_usd,
    coalesce(pm.new_mrr_usd, 0) as new_mrr_usd,
    coalesce(pm.churned_mrr_usd, 0) as churned_mrr_usd,
    coalesce(pm.gross_mrr_usd, 0) - coalesce(l.prev_mrr, 0) as mrr_change_usd,
    
    case 
        when coalesce(l.prev_mrr, 0) > 0 
        then round(((pm.gross_mrr_usd - l.prev_mrr) * 100.0 / l.prev_mrr), 2)
        else null 
    end as mrr_growth_rate_pct,
    
    -- Revenue metrics
    coalesce(rm.gross_revenue_usd, 0) as gross_revenue_usd,
    coalesce(rm.platform_fees_usd, 0) as platform_fees_usd,
    coalesce(rm.processing_fees_usd, 0) as processing_fees_usd,
    coalesce(rm.net_creator_earnings_usd, 0) as net_creator_earnings_usd,
    
    case 
        when coalesce(pm.gross_mrr_usd, 0) > 0 
        then round((rm.gross_revenue_usd * 100.0 / pm.gross_mrr_usd), 2)
        else null 
    end as collection_rate_pct,
    
    -- Payment health
    coalesce(rm.successful_transactions, 0) as successful_transactions,
    coalesce(rm.failed_transactions, 0) as failed_transactions,
    coalesce(rm.declined_amount_usd, 0) as declined_amount_usd,
    
    case 
        when coalesce(rm.successful_transactions, 0) + coalesce(rm.failed_transactions, 0) > 0
        then round((rm.failed_transactions * 100.0 / (rm.successful_transactions + rm.failed_transactions)), 2)
        else 0 
    end as decline_rate_pct,
    
    -- Tier distribution
    coalesce(pm.tier_1_patrons, 0) as tier_1_patrons,
    coalesce(pm.tier_2_patrons, 0) as tier_2_patrons,
    coalesce(pm.tier_3_plus_patrons, 0) as tier_3_plus_patrons,
    coalesce(pm.avg_pledge_amount_usd, 0) as avg_pledge_amount_usd,
    
    -- Content metrics
    coalesce(cnt.posts_published, 0) as posts_published,
    coalesce(cnt.paywalled_posts, 0) as paywalled_posts,
    coalesce(cnt.free_posts, 0) as free_posts,
    
    -- Engagement metrics
    coalesce(eng.total_views, 0) as total_views,
    coalesce(eng.total_likes, 0) as total_likes,
    coalesce(eng.total_comments, 0) as total_comments,
    coalesce(eng.engaged_patrons, 0) as engaged_patrons,
    coalesce(eng.total_engagement_score, 0) as total_engagement_score,
    
    case 
        when coalesce(pm.active_patrons, 0) > 0 
        then round((eng.engaged_patrons * 100.0 / pm.active_patrons), 2)
        else null 
    end as patron_engagement_rate_pct,
    
    current_timestamp() as updated_at

from creator_months cm
left join creators c on cm.creator_id = c.creator_id
left join pledge_metrics pm on cm.creator_id = pm.creator_id and cm.month_start_date = pm.month_start_date
left join revenue_metrics rm on cm.creator_id = rm.creator_id and cm.month_start_date = rm.month_start_date
left join content_metrics cnt on cm.creator_id = cnt.creator_id and cm.month_start_date = cnt.month_start_date
left join engagement_metrics eng on cm.creator_id = eng.creator_id and cm.month_start_date = eng.month_start_date
left join lagged l on cm.creator_id = l.creator_id and cm.month_start_date = l.month_start_date
  
[0m15:20:53.825347 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:20:54.432206 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db7d-81d4-17a7-9a83-c857aeb4bcbf) - Created
[0m15:20:57.050498 [debug] [Thread-3 (]: SQL status: OK in 8.150 seconds
[0m15:20:57.051226 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0db7d-7e33-1ecd-a674-12ba540ba18d, command-id=01f0db7d-7e9e-1381-a504-3ed79cb356f0) - Closing
[0m15:20:57.242981 [debug] [Thread-3 (]: Applying tags to relation None
[0m15:20:57.252029 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: Close
[0m15:20:57.252287 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0db7d-7e33-1ecd-a674-12ba540ba18d) - Closing
[0m15:20:57.432054 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169d73a40>]}
[0m15:20:57.433251 [info ] [Thread-3 (]: 1 of 9 OK created sql table model analytics.metricflow_time_spine .............. [[32mOK[0m in 9.80s]
[0m15:20:57.433652 [debug] [Thread-3 (]: Finished running node model.patreon_analytics.metricflow_time_spine
[0m15:21:06.344911 [debug] [Thread-4 (]: SQL status: OK in 12.520 seconds
[0m15:21:06.346354 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0db7d-81d4-17a7-9a83-c857aeb4bcbf, command-id=01f0db7d-81ea-1bdf-a0ef-71f524855608) - Closing
[0m15:21:06.558555 [debug] [Thread-4 (]: Applying tags to relation None
[0m15:21:06.560379 [debug] [Thread-4 (]: On model.patreon_analytics.fct_creator_monthly_performance: Close
[0m15:21:06.560859 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0db7d-81d4-17a7-9a83-c857aeb4bcbf) - Closing
[0m15:21:06.725624 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '381af38c-eb06-486a-a388-af42513eafd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16aaa2de0>]}
[0m15:21:06.727091 [info ] [Thread-4 (]: 9 of 9 OK created sql table model analytics_marts.fct_creator_monthly_performance  [[32mOK[0m in 12.92s]
[0m15:21:06.727601 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.fct_creator_monthly_performance
[0m15:21:06.728785 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:21:06.729096 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:21:06.729456 [info ] [MainThread]: 
[0m15:21:06.729683 [info ] [MainThread]: Finished running 2 table models, 7 view models in 0 hours 0 minutes and 32.57 seconds (32.57s).
[0m15:21:06.730584 [debug] [MainThread]: Command end result
[0m15:21:06.767401 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m15:21:06.770284 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m15:21:06.773553 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m15:21:06.773748 [info ] [MainThread]: 
[0m15:21:06.773962 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:21:06.774115 [info ] [MainThread]: 
[0m15:21:06.774281 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=9
[0m15:21:06.774570 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PropertyMovedToConfigDeprecation: 42 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:21:06.777801 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 34.825924, "process_in_blocks": "0", "process_kernel_time": 0.699913, "process_mem_max_rss": "272908288", "process_out_blocks": "0", "process_user_time": 5.798904}
[0m15:21:06.778218 [debug] [MainThread]: Command `dbt run` succeeded at 15:21:06.778152 after 34.83 seconds
[0m15:21:06.778535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1035eef90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169d64b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1225baf90>]}
[0m15:21:06.778835 [debug] [MainThread]: Flushing usage events
[0m15:21:07.593015 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m08:59:45.356352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107681fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112032450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112031d00>]}


============================== 08:59:45.359746 | 172d8217-1611-4cbd-a874-a19fb5cfab6d ==============================
[0m08:59:45.359746 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m08:59:45.360090 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'partial_parse': 'True', 'no_print': 'None', 'quiet': 'False', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'target_path': 'None', 'warn_error': 'None', 'write_json': 'True', 'empty': 'None', 'use_colors': 'True', 'invocation_command': 'dbt docs generate', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'log_format': 'default', 'cache_selected_only': 'False', 'debug': 'False', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'static_parser': 'True'}
[0m08:59:45.866252 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m08:59:45.866582 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m08:59:45.866750 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m08:59:48.342593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '172d8217-1611-4cbd-a874-a19fb5cfab6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13126dac0>]}
[0m08:59:48.376133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '172d8217-1611-4cbd-a874-a19fb5cfab6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11487a450>]}
[0m08:59:48.376615 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m08:59:48.482709 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m08:59:48.483355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '172d8217-1611-4cbd-a874-a19fb5cfab6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11177fce0>]}
[0m08:59:48.505143 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m08:59:48.590215 [warn ] [MainThread]: [[33mWARNING[0m][PropertyMovedToConfigDeprecation]: Deprecated functionality
Found `meta` as a top-level property of `models[0].columns[0]` in file
`models/marts/core/schema.yml`. The `meta` top-level property should be moved
into the `config` of `models[0].columns[0]`.
[0m08:59:48.590550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '172d8217-1611-4cbd-a874-a19fb5cfab6d', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1328a6f00>]}
[0m08:59:48.661134 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 7 files changed.
[0m08:59:48.661730 [debug] [MainThread]: Partial parsing: added file: patreon_analytics://models/staging/sources.yml
[0m08:59:48.662040 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://models/staging/stg_pledges.sql
[0m08:59:48.662206 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://models/staging/stg_patrons.sql
[0m08:59:48.662413 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://models/staging/stg_tiers.sql
[0m08:59:48.662623 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://models/staging/stg_engagement_events.sql
[0m08:59:48.662777 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://models/staging/stg_pledges.sql
[0m08:59:48.663013 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://models/staging/stg_posts.sql
[0m08:59:48.663167 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://models/staging/stg_creators.sql
[0m08:59:48.663318 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://models/staging/stg_posts.sql
[0m08:59:48.663520 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://models/staging/stg_transactions.sql
[0m08:59:48.888384 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_pledges' in
package 'patreon_analytics' (models/staging/schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m08:59:48.888783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '172d8217-1611-4cbd-a874-a19fb5cfab6d', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x133275610>]}
[0m08:59:49.117900 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.intermediate
- models.patreon_analytics.marts.finance
[0m08:59:49.124017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '172d8217-1611-4cbd-a874-a19fb5cfab6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13324bdd0>]}
[0m08:59:49.137592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '172d8217-1611-4cbd-a874-a19fb5cfab6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x132fd9010>]}
[0m08:59:49.137927 [info ] [MainThread]: Found 9 models, 50 data tests, 7 seeds, 7 sources, 4 metrics, 1126 macros, 1 semantic model
[0m08:59:49.138121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '172d8217-1611-4cbd-a874-a19fb5cfab6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x133221e50>]}
[0m08:59:49.140211 [info ] [MainThread]: 
[0m08:59:49.140398 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m08:59:49.140538 [info ] [MainThread]: 
[0m08:59:49.140923 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m08:59:49.141110 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:59:49.144808 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics) - Creating connection
[0m08:59:49.145173 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics'
[0m08:59:49.145532 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_marts) - Creating connection
[0m08:59:49.151839 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics"
[0m08:59:49.152215 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_marts'
[0m08:59:49.152645 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_raw) - Creating connection
[0m08:59:49.152895 [debug] [ThreadPool]: On list_patreon_dev_analytics: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics'

  
[0m08:59:49.153189 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_staging) - Creating connection
[0m08:59:49.154621 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_marts"
[0m08:59:49.154833 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_raw'
[0m08:59:49.155031 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:59:49.155196 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_staging'
[0m08:59:49.155370 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_marts'

  
[0m08:59:49.156736 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_raw"
[0m08:59:49.158649 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_staging"
[0m08:59:49.159060 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:59:49.159343 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_raw'

  
[0m08:59:49.159657 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_staging'

  
[0m08:59:49.160350 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:59:49.160661 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:59:50.567431 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-703b-1052-8c1c-f155a17ac3a2) - Created
[0m08:59:50.570668 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-703e-1563-b837-34c7e97a6456) - Created
[0m08:59:50.583940 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-703d-1b67-b2ab-69a9a724231c) - Created
[0m08:59:50.598920 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-703e-19ff-b883-8ad624720242) - Created
[0m08:59:59.260673 [debug] [ThreadPool]: SQL status: OK in 10.110 seconds
[0m08:59:59.261239 [debug] [ThreadPool]: SQL status: OK in 10.100 seconds
[0m08:59:59.261696 [debug] [ThreadPool]: SQL status: OK in 10.100 seconds
[0m08:59:59.262011 [debug] [ThreadPool]: SQL status: OK in 10.100 seconds
[0m08:59:59.268537 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-703e-19ff-b883-8ad624720242, command-id=01f0dc11-7068-1042-888e-2cab3137e66f) - Closing
[0m08:59:59.269443 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-703d-1b67-b2ab-69a9a724231c, command-id=01f0dc11-7066-14cd-a7dc-7f83d065dc00) - Closing
[0m08:59:59.270566 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-703b-1052-8c1c-f155a17ac3a2, command-id=01f0dc11-7066-1776-936c-23845f4621a3) - Closing
[0m08:59:59.271473 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-703e-1563-b837-34c7e97a6456, command-id=01f0dc11-7067-19f1-8dd0-bce36b22fb89) - Closing
[0m08:59:59.463291 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: Close
[0m08:59:59.463961 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-703d-1b67-b2ab-69a9a724231c) - Closing
[0m08:59:59.615953 [debug] [ThreadPool]: On list_patreon_dev_analytics: Close
[0m08:59:59.616301 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-703e-19ff-b883-8ad624720242) - Closing
[0m08:59:59.775802 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: Close
[0m08:59:59.776987 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-703b-1052-8c1c-f155a17ac3a2) - Closing
[0m08:59:59.934768 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: Close
[0m08:59:59.935513 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-703e-1563-b837-34c7e97a6456) - Closing
[0m09:00:00.098160 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '172d8217-1611-4cbd-a874-a19fb5cfab6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x133220bf0>]}
[0m09:00:00.102382 [debug] [Thread-2 (]: Began running node model.patreon_analytics.metricflow_time_spine
[0m09:00:00.102826 [debug] [Thread-3 (]: Began running node model.patreon_analytics.stg_creators
[0m09:00:00.103102 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_engagement_events
[0m09:00:00.103439 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.metricflow_time_spine) - Creating connection
[0m09:00:00.103623 [debug] [Thread-5 (]: Began running node model.patreon_analytics.stg_patrons
[0m09:00:00.104116 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_creators) - Creating connection
[0m09:00:00.104369 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_engagement_events) - Creating connection
[0m09:00:00.104548 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.patreon_analytics.metricflow_time_spine'
[0m09:00:00.104762 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_patrons) - Creating connection
[0m09:00:00.104926 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_creators'
[0m09:00:00.105133 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_engagement_events'
[0m09:00:00.105363 [debug] [Thread-2 (]: Began compiling node model.patreon_analytics.metricflow_time_spine
[0m09:00:00.105546 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_patrons'
[0m09:00:00.105762 [debug] [Thread-3 (]: Began compiling node model.patreon_analytics.stg_creators
[0m09:00:00.105988 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.stg_engagement_events
[0m09:00:00.125296 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.stg_patrons
[0m09:00:00.130529 [debug] [Thread-2 (]: Using databricks connection "model.patreon_analytics.metricflow_time_spine"
[0m09:00:00.133036 [debug] [Thread-3 (]: Writing injected SQL for node "model.patreon_analytics.stg_creators"
[0m09:00:00.140439 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.stg_engagement_events"
[0m09:00:00.142689 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.stg_patrons"
[0m09:00:00.143045 [debug] [Thread-2 (]: On model.patreon_analytics.metricflow_time_spine: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.metricflow_time_spine"} */


        select timestampdiff(day, date_trunc('day', cast('2020-01-01' as timestamp)), date_trunc('day', cast('2030-12-31' as timestamp)))
[0m09:00:00.143609 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m09:00:00.144562 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.stg_engagement_events
[0m09:00:00.145104 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_engagement_events
[0m09:00:00.145372 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.stg_patrons
[0m09:00:00.145634 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_pledges
[0m09:00:00.146219 [debug] [Thread-3 (]: Began executing node model.patreon_analytics.stg_creators
[0m09:00:00.146750 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.stg_patrons
[0m09:00:00.147196 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_pledges) - Creating connection
[0m09:00:00.147600 [debug] [Thread-3 (]: Finished running node model.patreon_analytics.stg_creators
[0m09:00:00.147835 [debug] [Thread-5 (]: Began running node model.patreon_analytics.stg_posts
[0m09:00:00.148084 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_pledges'
[0m09:00:00.148336 [debug] [Thread-3 (]: Began running node model.patreon_analytics.stg_tiers
[0m09:00:00.148633 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_posts) - Creating connection
[0m09:00:00.148869 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.stg_pledges
[0m09:00:00.149148 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_tiers) - Creating connection
[0m09:00:00.149408 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_posts'
[0m09:00:00.151575 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.stg_pledges"
[0m09:00:00.151903 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_tiers'
[0m09:00:00.152197 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.stg_posts
[0m09:00:00.152476 [debug] [Thread-3 (]: Began compiling node model.patreon_analytics.stg_tiers
[0m09:00:00.154470 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.stg_posts"
[0m09:00:00.156585 [debug] [Thread-3 (]: Writing injected SQL for node "model.patreon_analytics.stg_tiers"
[0m09:00:00.156941 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.stg_pledges
[0m09:00:00.157334 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_pledges
[0m09:00:00.157540 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_transactions
[0m09:00:00.157752 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.stg_posts
[0m09:00:00.157991 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_transactions) - Creating connection
[0m09:00:00.158344 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.stg_posts
[0m09:00:00.158511 [debug] [Thread-3 (]: Began executing node model.patreon_analytics.stg_tiers
[0m09:00:00.158702 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_transactions'
[0m09:00:00.158901 [debug] [Thread-5 (]: Began running node seed.patreon_analytics.creators
[0m09:00:00.159206 [debug] [Thread-3 (]: Finished running node model.patreon_analytics.stg_tiers
[0m09:00:00.159412 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.stg_transactions
[0m09:00:00.159642 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.creators) - Creating connection
[0m09:00:00.159833 [debug] [Thread-3 (]: Began running node seed.patreon_analytics.engagement_events
[0m09:00:00.161822 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.stg_transactions"
[0m09:00:00.162116 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.patreon_analytics.creators'
[0m09:00:00.162436 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.engagement_events) - Creating connection
[0m09:00:00.162740 [debug] [Thread-5 (]: Began compiling node seed.patreon_analytics.creators
[0m09:00:00.162991 [debug] [Thread-3 (]: Acquiring new databricks connection 'seed.patreon_analytics.engagement_events'
[0m09:00:00.165462 [debug] [Thread-5 (]: Began executing node seed.patreon_analytics.creators
[0m09:00:00.165691 [debug] [Thread-3 (]: Began compiling node seed.patreon_analytics.engagement_events
[0m09:00:00.166031 [debug] [Thread-5 (]: Finished running node seed.patreon_analytics.creators
[0m09:00:00.166203 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.stg_transactions
[0m09:00:00.167567 [debug] [Thread-3 (]: Began executing node seed.patreon_analytics.engagement_events
[0m09:00:00.167782 [debug] [Thread-5 (]: Began running node seed.patreon_analytics.patrons
[0m09:00:00.168108 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_transactions
[0m09:00:00.168412 [debug] [Thread-3 (]: Finished running node seed.patreon_analytics.engagement_events
[0m09:00:00.168642 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.patrons) - Creating connection
[0m09:00:00.168825 [debug] [Thread-4 (]: Began running node seed.patreon_analytics.pledges
[0m09:00:00.169057 [debug] [Thread-3 (]: Began running node seed.patreon_analytics.posts
[0m09:00:00.169319 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.patreon_analytics.patrons'
[0m09:00:00.169553 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.pledges) - Creating connection
[0m09:00:00.169775 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.posts) - Creating connection
[0m09:00:00.169936 [debug] [Thread-5 (]: Began compiling node seed.patreon_analytics.patrons
[0m09:00:00.170098 [debug] [Thread-4 (]: Acquiring new databricks connection 'seed.patreon_analytics.pledges'
[0m09:00:00.170261 [debug] [Thread-3 (]: Acquiring new databricks connection 'seed.patreon_analytics.posts'
[0m09:00:00.171606 [debug] [Thread-5 (]: Began executing node seed.patreon_analytics.patrons
[0m09:00:00.171777 [debug] [Thread-4 (]: Began compiling node seed.patreon_analytics.pledges
[0m09:00:00.171944 [debug] [Thread-3 (]: Began compiling node seed.patreon_analytics.posts
[0m09:00:00.172273 [debug] [Thread-5 (]: Finished running node seed.patreon_analytics.patrons
[0m09:00:00.173595 [debug] [Thread-4 (]: Began executing node seed.patreon_analytics.pledges
[0m09:00:00.175149 [debug] [Thread-3 (]: Began executing node seed.patreon_analytics.posts
[0m09:00:00.175408 [debug] [Thread-5 (]: Began running node seed.patreon_analytics.tiers
[0m09:00:00.175839 [debug] [Thread-4 (]: Finished running node seed.patreon_analytics.pledges
[0m09:00:00.176175 [debug] [Thread-3 (]: Finished running node seed.patreon_analytics.posts
[0m09:00:00.176435 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.tiers) - Creating connection
[0m09:00:00.176638 [debug] [Thread-4 (]: Began running node seed.patreon_analytics.transactions
[0m09:00:00.176859 [debug] [Thread-3 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5
[0m09:00:00.177061 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.patreon_analytics.tiers'
[0m09:00:00.177296 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.transactions) - Creating connection
[0m09:00:00.177606 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5) - Creating connection
[0m09:00:00.177877 [debug] [Thread-5 (]: Began compiling node seed.patreon_analytics.tiers
[0m09:00:00.178098 [debug] [Thread-4 (]: Acquiring new databricks connection 'seed.patreon_analytics.transactions'
[0m09:00:00.178646 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5'
[0m09:00:00.180058 [debug] [Thread-5 (]: Began executing node seed.patreon_analytics.tiers
[0m09:00:00.180267 [debug] [Thread-4 (]: Began compiling node seed.patreon_analytics.transactions
[0m09:00:00.180436 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5
[0m09:00:00.180751 [debug] [Thread-5 (]: Finished running node seed.patreon_analytics.tiers
[0m09:00:00.182357 [debug] [Thread-4 (]: Began executing node seed.patreon_analytics.transactions
[0m09:00:00.188482 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5"
[0m09:00:00.188782 [debug] [Thread-5 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711
[0m09:00:00.189129 [debug] [Thread-4 (]: Finished running node seed.patreon_analytics.transactions
[0m09:00:00.189613 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711) - Creating connection
[0m09:00:00.189892 [debug] [Thread-4 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc
[0m09:00:00.190084 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711'
[0m09:00:00.190441 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc) - Creating connection
[0m09:00:00.190686 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711
[0m09:00:00.190917 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5
[0m09:00:00.191149 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc'
[0m09:00:00.193988 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711"
[0m09:00:00.194558 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5
[0m09:00:00.194789 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc
[0m09:00:00.195053 [debug] [Thread-3 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7
[0m09:00:00.197673 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc"
[0m09:00:00.197971 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7) - Creating connection
[0m09:00:00.198292 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7'
[0m09:00:00.198547 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711
[0m09:00:00.198798 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7
[0m09:00:00.199168 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711
[0m09:00:00.201675 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7"
[0m09:00:00.201993 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc
[0m09:00:00.202298 [debug] [Thread-5 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f
[0m09:00:00.202963 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc
[0m09:00:00.203338 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f) - Creating connection
[0m09:00:00.203624 [debug] [Thread-4 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb
[0m09:00:00.203905 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f'
[0m09:00:00.204138 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7
[0m09:00:00.204453 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb) - Creating connection
[0m09:00:00.204663 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f
[0m09:00:00.204995 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7
[0m09:00:00.205200 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb'
[0m09:00:00.207783 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f"
[0m09:00:00.208085 [debug] [Thread-3 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d
[0m09:00:00.208345 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb
[0m09:00:00.208695 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d) - Creating connection
[0m09:00:00.216125 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d'
[0m09:00:00.219295 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d
[0m09:00:00.222507 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb"
[0m09:00:00.228878 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d"
[0m09:00:00.229285 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f
[0m09:00:00.229817 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f
[0m09:00:00.230065 [debug] [Thread-5 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38
[0m09:00:00.230420 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38) - Creating connection
[0m09:00:00.230610 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38'
[0m09:00:00.230799 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb
[0m09:00:00.231000 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38
[0m09:00:00.231165 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d
[0m09:00:00.231534 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb
[0m09:00:00.235354 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38"
[0m09:00:00.235800 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d
[0m09:00:00.236089 [debug] [Thread-4 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7
[0m09:00:00.236387 [debug] [Thread-3 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094
[0m09:00:00.236731 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7) - Creating connection
[0m09:00:00.237060 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094) - Creating connection
[0m09:00:00.237276 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7'
[0m09:00:00.237469 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38
[0m09:00:00.237664 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094'
[0m09:00:00.237859 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7
[0m09:00:00.238169 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38
[0m09:00:00.238331 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094
[0m09:00:00.241030 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7"
[0m09:00:00.241376 [debug] [Thread-5 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4
[0m09:00:00.244224 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094"
[0m09:00:00.244795 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4) - Creating connection
[0m09:00:00.245116 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4'
[0m09:00:00.245306 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4
[0m09:00:00.248088 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4"
[0m09:00:00.248409 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094
[0m09:00:00.248769 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094
[0m09:00:00.248966 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7
[0m09:00:00.249190 [debug] [Thread-3 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33
[0m09:00:00.249596 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7
[0m09:00:00.249860 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33) - Creating connection
[0m09:00:00.250029 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4
[0m09:00:00.250222 [debug] [Thread-4 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958
[0m09:00:00.250407 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33'
[0m09:00:00.250697 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4
[0m09:00:00.250962 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958) - Creating connection
[0m09:00:00.251141 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33
[0m09:00:00.251323 [debug] [Thread-5 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6
[0m09:00:00.251514 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958'
[0m09:00:00.255001 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33"
[0m09:00:00.255336 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6) - Creating connection
[0m09:00:00.255532 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958
[0m09:00:00.255744 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6'
[0m09:00:00.258168 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958"
[0m09:00:00.258412 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6
[0m09:00:00.260886 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6"
[0m09:00:00.261181 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33
[0m09:00:00.261404 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958
[0m09:00:00.261758 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33
[0m09:00:00.262095 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958
[0m09:00:00.262323 [debug] [Thread-3 (]: Began running node test.patreon_analytics.accepted_values_stg_engagement_events_event_type__view__like__unlike__comment__share.6f1a4c0604
[0m09:00:00.262538 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6
[0m09:00:00.262724 [debug] [Thread-4 (]: Began running node test.patreon_analytics.not_null_stg_engagement_events_event_id.a363dfdd90
[0m09:00:00.262998 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.accepted_values_stg_engagement_events_event_type__view__like__unlike__comment__share.6f1a4c0604) - Creating connection
[0m09:00:00.263325 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6
[0m09:00:00.263689 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_engagement_events_event_id.a363dfdd90) - Creating connection
[0m09:00:00.263888 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.accepted_values_stg_engagement_events_event_type__view__like__unlike__comment__share.6f1a4c0604'
[0m09:00:00.264087 [debug] [Thread-5 (]: Began running node test.patreon_analytics.unique_stg_engagement_events_event_id.04a2222875
[0m09:00:00.264270 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_engagement_events_event_id.a363dfdd90'
[0m09:00:00.264435 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.accepted_values_stg_engagement_events_event_type__view__like__unlike__comment__share.6f1a4c0604
[0m09:00:00.264657 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.unique_stg_engagement_events_event_id.04a2222875) - Creating connection
[0m09:00:00.264821 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.not_null_stg_engagement_events_event_id.a363dfdd90
[0m09:00:00.267583 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.accepted_values_stg_engagement_events_event_type__view__like__unlike__comment__share.6f1a4c0604"
[0m09:00:00.267828 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.unique_stg_engagement_events_event_id.04a2222875'
[0m09:00:00.270221 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_engagement_events_event_id.a363dfdd90"
[0m09:00:00.270520 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.unique_stg_engagement_events_event_id.04a2222875
[0m09:00:00.273132 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.unique_stg_engagement_events_event_id.04a2222875"
[0m09:00:00.273594 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.not_null_stg_engagement_events_event_id.a363dfdd90
[0m09:00:00.273789 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.accepted_values_stg_engagement_events_event_type__view__like__unlike__comment__share.6f1a4c0604
[0m09:00:00.273967 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.unique_stg_engagement_events_event_id.04a2222875
[0m09:00:00.274361 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.not_null_stg_engagement_events_event_id.a363dfdd90
[0m09:00:00.274792 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.accepted_values_stg_engagement_events_event_type__view__like__unlike__comment__share.6f1a4c0604
[0m09:00:00.275131 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.unique_stg_engagement_events_event_id.04a2222875
[0m09:00:00.275346 [debug] [Thread-4 (]: Began running node test.patreon_analytics.accepted_values_stg_patrons_patron_value_tier__whale__high_value__regular__casual.3f5dae4aa6
[0m09:00:00.275562 [debug] [Thread-3 (]: Began running node test.patreon_analytics.not_null_stg_patrons_patron_id.9a81327dee
[0m09:00:00.275767 [debug] [Thread-5 (]: Began running node test.patreon_analytics.unique_stg_patrons_patron_id.b6b0671565
[0m09:00:00.276109 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.accepted_values_stg_patrons_patron_value_tier__whale__high_value__regular__casual.3f5dae4aa6) - Creating connection
[0m09:00:00.276387 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_patrons_patron_id.9a81327dee) - Creating connection
[0m09:00:00.276674 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.unique_stg_patrons_patron_id.b6b0671565) - Creating connection
[0m09:00:00.276899 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.accepted_values_stg_patrons_patron_value_tier__whale__high_value__regular__casual.3f5dae4aa6'
[0m09:00:00.277110 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_patrons_patron_id.9a81327dee'
[0m09:00:00.277353 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.unique_stg_patrons_patron_id.b6b0671565'
[0m09:00:00.277585 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.accepted_values_stg_patrons_patron_value_tier__whale__high_value__regular__casual.3f5dae4aa6
[0m09:00:00.277790 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.not_null_stg_patrons_patron_id.9a81327dee
[0m09:00:00.277966 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.unique_stg_patrons_patron_id.b6b0671565
[0m09:00:00.281944 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.accepted_values_stg_patrons_patron_value_tier__whale__high_value__regular__casual.3f5dae4aa6"
[0m09:00:00.284186 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_patrons_patron_id.9a81327dee"
[0m09:00:00.286837 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.unique_stg_patrons_patron_id.b6b0671565"
[0m09:00:00.287519 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.accepted_values_stg_patrons_patron_value_tier__whale__high_value__regular__casual.3f5dae4aa6
[0m09:00:00.287750 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.not_null_stg_patrons_patron_id.9a81327dee
[0m09:00:00.288219 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.accepted_values_stg_patrons_patron_value_tier__whale__high_value__regular__casual.3f5dae4aa6
[0m09:00:00.288417 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.unique_stg_patrons_patron_id.b6b0671565
[0m09:00:00.288744 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.not_null_stg_patrons_patron_id.9a81327dee
[0m09:00:00.288948 [debug] [Thread-4 (]: Began running node test.patreon_analytics.accepted_values_stg_creators_plan_type__lite__pro__premium.3492742776
[0m09:00:00.289268 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.unique_stg_patrons_patron_id.b6b0671565
[0m09:00:00.289460 [debug] [Thread-3 (]: Began running node test.patreon_analytics.accepted_values_stg_creators_status__active__paused__suspended__deleted.7a24db0237
[0m09:00:00.289727 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.accepted_values_stg_creators_plan_type__lite__pro__premium.3492742776) - Creating connection
[0m09:00:00.289917 [debug] [Thread-5 (]: Began running node test.patreon_analytics.not_null_stg_creators_creator_id.a4d196188a
[0m09:00:00.290161 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.accepted_values_stg_creators_status__active__paused__suspended__deleted.7a24db0237) - Creating connection
[0m09:00:00.290346 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.accepted_values_stg_creators_plan_type__lite__pro__premium.3492742776'
[0m09:00:00.290643 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_creators_creator_id.a4d196188a) - Creating connection
[0m09:00:00.290898 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.accepted_values_stg_creators_status__active__paused__suspended__deleted.7a24db0237'
[0m09:00:00.291102 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.accepted_values_stg_creators_plan_type__lite__pro__premium.3492742776
[0m09:00:00.291268 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_creators_creator_id.a4d196188a'
[0m09:00:00.291428 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.accepted_values_stg_creators_status__active__paused__suspended__deleted.7a24db0237
[0m09:00:00.294073 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.accepted_values_stg_creators_plan_type__lite__pro__premium.3492742776"
[0m09:00:00.294340 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.not_null_stg_creators_creator_id.a4d196188a
[0m09:00:00.296836 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.accepted_values_stg_creators_status__active__paused__suspended__deleted.7a24db0237"
[0m09:00:00.300908 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_creators_creator_id.a4d196188a"
[0m09:00:00.301468 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.accepted_values_stg_creators_plan_type__lite__pro__premium.3492742776
[0m09:00:00.302031 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.accepted_values_stg_creators_plan_type__lite__pro__premium.3492742776
[0m09:00:00.302300 [debug] [Thread-4 (]: Began running node test.patreon_analytics.unique_stg_creators_creator_id.78aaa0dba4
[0m09:00:00.302698 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.unique_stg_creators_creator_id.78aaa0dba4) - Creating connection
[0m09:00:00.302978 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.unique_stg_creators_creator_id.78aaa0dba4'
[0m09:00:00.303266 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.unique_stg_creators_creator_id.78aaa0dba4
[0m09:00:00.307254 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.unique_stg_creators_creator_id.78aaa0dba4"
[0m09:00:00.307580 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.accepted_values_stg_creators_status__active__paused__suspended__deleted.7a24db0237
[0m09:00:00.308184 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.accepted_values_stg_creators_status__active__paused__suspended__deleted.7a24db0237
[0m09:00:00.308429 [debug] [Thread-3 (]: Began running node test.patreon_analytics.accepted_values_stg_pledges_pledge_status__active__paused__churned__declined.a38baa5be6
[0m09:00:00.308647 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.not_null_stg_creators_creator_id.a4d196188a
[0m09:00:00.308964 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.accepted_values_stg_pledges_pledge_status__active__paused__churned__declined.a38baa5be6) - Creating connection
[0m09:00:00.309343 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.not_null_stg_creators_creator_id.a4d196188a
[0m09:00:00.309549 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.accepted_values_stg_pledges_pledge_status__active__paused__churned__declined.a38baa5be6'
[0m09:00:00.309721 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.unique_stg_creators_creator_id.78aaa0dba4
[0m09:00:00.309912 [debug] [Thread-5 (]: Began running node test.patreon_analytics.not_null_stg_pledges_creator_id.7ac5bc19ef
[0m09:00:00.310105 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.accepted_values_stg_pledges_pledge_status__active__paused__churned__declined.a38baa5be6
[0m09:00:00.310402 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.unique_stg_creators_creator_id.78aaa0dba4
[0m09:00:00.310682 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_pledges_creator_id.7ac5bc19ef) - Creating connection
[0m09:00:00.313288 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.accepted_values_stg_pledges_pledge_status__active__paused__churned__declined.a38baa5be6"
[0m09:00:00.313550 [debug] [Thread-4 (]: Began running node test.patreon_analytics.not_null_stg_pledges_patron_id.45079734da
[0m09:00:00.313787 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_pledges_creator_id.7ac5bc19ef'
[0m09:00:00.314104 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_pledges_patron_id.45079734da) - Creating connection
[0m09:00:00.314309 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.not_null_stg_pledges_creator_id.7ac5bc19ef
[0m09:00:00.314508 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_pledges_patron_id.45079734da'
[0m09:00:00.317312 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_pledges_creator_id.7ac5bc19ef"
[0m09:00:00.317562 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.not_null_stg_pledges_patron_id.45079734da
[0m09:00:00.317811 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.accepted_values_stg_pledges_pledge_status__active__paused__churned__declined.a38baa5be6
[0m09:00:00.320238 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_pledges_patron_id.45079734da"
[0m09:00:00.320687 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.accepted_values_stg_pledges_pledge_status__active__paused__churned__declined.a38baa5be6
[0m09:00:00.321065 [debug] [Thread-3 (]: Began running node test.patreon_analytics.not_null_stg_pledges_pledge_id.c6c8840915
[0m09:00:00.321347 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.not_null_stg_pledges_creator_id.7ac5bc19ef
[0m09:00:00.321723 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_pledges_pledge_id.c6c8840915) - Creating connection
[0m09:00:00.322079 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.not_null_stg_pledges_creator_id.7ac5bc19ef
[0m09:00:00.322273 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_pledges_pledge_id.c6c8840915'
[0m09:00:00.322480 [debug] [Thread-5 (]: Began running node test.patreon_analytics.relationships_stg_pledges_creator_id__creator_id__ref_stg_creators_.a296b68aa7
[0m09:00:00.322684 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.not_null_stg_pledges_patron_id.45079734da
[0m09:00:00.322882 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.not_null_stg_pledges_pledge_id.c6c8840915
[0m09:00:00.323200 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.relationships_stg_pledges_creator_id__creator_id__ref_stg_creators_.a296b68aa7) - Creating connection
[0m09:00:00.323553 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.not_null_stg_pledges_patron_id.45079734da
[0m09:00:00.325967 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_pledges_pledge_id.c6c8840915"
[0m09:00:00.326174 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.relationships_stg_pledges_creator_id__creator_id__ref_stg_creators_.a296b68aa7'
[0m09:00:00.326439 [debug] [Thread-4 (]: Began running node test.patreon_analytics.relationships_stg_pledges_patron_id__patron_id__ref_stg_patrons_.1aea46a3f2
[0m09:00:00.326747 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.relationships_stg_pledges_creator_id__creator_id__ref_stg_creators_.a296b68aa7
[0m09:00:00.327062 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.relationships_stg_pledges_patron_id__patron_id__ref_stg_patrons_.1aea46a3f2) - Creating connection
[0m09:00:00.329976 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.relationships_stg_pledges_creator_id__creator_id__ref_stg_creators_.a296b68aa7"
[0m09:00:00.330260 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.relationships_stg_pledges_patron_id__patron_id__ref_stg_patrons_.1aea46a3f2'
[0m09:00:00.330550 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.not_null_stg_pledges_pledge_id.c6c8840915
[0m09:00:00.330753 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.relationships_stg_pledges_patron_id__patron_id__ref_stg_patrons_.1aea46a3f2
[0m09:00:00.331139 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.not_null_stg_pledges_pledge_id.c6c8840915
[0m09:00:00.334896 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.relationships_stg_pledges_patron_id__patron_id__ref_stg_patrons_.1aea46a3f2"
[0m09:00:00.335169 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.relationships_stg_pledges_creator_id__creator_id__ref_stg_creators_.a296b68aa7
[0m09:00:00.335412 [debug] [Thread-3 (]: Began running node test.patreon_analytics.unique_stg_pledges_pledge_id.d71e65eb4c
[0m09:00:00.335897 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.relationships_stg_pledges_creator_id__creator_id__ref_stg_creators_.a296b68aa7
[0m09:00:00.336337 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.unique_stg_pledges_pledge_id.d71e65eb4c) - Creating connection
[0m09:00:00.336579 [debug] [Thread-5 (]: Began running node test.patreon_analytics.not_null_stg_posts_creator_id.44240e5ecf
[0m09:00:00.336817 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.unique_stg_pledges_pledge_id.d71e65eb4c'
[0m09:00:00.337072 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_posts_creator_id.44240e5ecf) - Creating connection
[0m09:00:00.337252 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.relationships_stg_pledges_patron_id__patron_id__ref_stg_patrons_.1aea46a3f2
[0m09:00:00.337417 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.unique_stg_pledges_pledge_id.d71e65eb4c
[0m09:00:00.337589 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_posts_creator_id.44240e5ecf'
[0m09:00:00.337867 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.relationships_stg_pledges_patron_id__patron_id__ref_stg_patrons_.1aea46a3f2
[0m09:00:00.340235 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.unique_stg_pledges_pledge_id.d71e65eb4c"
[0m09:00:00.340502 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.not_null_stg_posts_creator_id.44240e5ecf
[0m09:00:00.340748 [debug] [Thread-4 (]: Began running node test.patreon_analytics.not_null_stg_posts_post_id.cd5975e2d9
[0m09:00:00.343478 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_posts_creator_id.44240e5ecf"
[0m09:00:00.343824 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_posts_post_id.cd5975e2d9) - Creating connection
[0m09:00:00.344115 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_posts_post_id.cd5975e2d9'
[0m09:00:00.344324 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.not_null_stg_posts_post_id.cd5975e2d9
[0m09:00:00.344512 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.unique_stg_pledges_pledge_id.d71e65eb4c
[0m09:00:00.347000 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_posts_post_id.cd5975e2d9"
[0m09:00:00.347446 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.unique_stg_pledges_pledge_id.d71e65eb4c
[0m09:00:00.347661 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.not_null_stg_posts_creator_id.44240e5ecf
[0m09:00:00.347896 [debug] [Thread-3 (]: Began running node test.patreon_analytics.relationships_stg_posts_creator_id__creator_id__ref_stg_creators_.ef48439d1e
[0m09:00:00.348252 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.not_null_stg_posts_creator_id.44240e5ecf
[0m09:00:00.348518 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.relationships_stg_posts_creator_id__creator_id__ref_stg_creators_.ef48439d1e) - Creating connection
[0m09:00:00.348708 [debug] [Thread-5 (]: Began running node test.patreon_analytics.unique_stg_posts_post_id.ec6994c39f
[0m09:00:00.348901 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.relationships_stg_posts_creator_id__creator_id__ref_stg_creators_.ef48439d1e'
[0m09:00:00.349140 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.unique_stg_posts_post_id.ec6994c39f) - Creating connection
[0m09:00:00.349315 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.not_null_stg_posts_post_id.cd5975e2d9
[0m09:00:00.349476 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.relationships_stg_posts_creator_id__creator_id__ref_stg_creators_.ef48439d1e
[0m09:00:00.349654 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.unique_stg_posts_post_id.ec6994c39f'
[0m09:00:00.349933 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.not_null_stg_posts_post_id.cd5975e2d9
[0m09:00:00.353188 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.relationships_stg_posts_creator_id__creator_id__ref_stg_creators_.ef48439d1e"
[0m09:00:00.353440 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.unique_stg_posts_post_id.ec6994c39f
[0m09:00:00.353679 [debug] [Thread-4 (]: Began running node test.patreon_analytics.not_null_stg_tiers_creator_id.03c2bb8892
[0m09:00:00.357223 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.unique_stg_posts_post_id.ec6994c39f"
[0m09:00:00.357629 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_tiers_creator_id.03c2bb8892) - Creating connection
[0m09:00:00.357961 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_tiers_creator_id.03c2bb8892'
[0m09:00:00.358174 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.relationships_stg_posts_creator_id__creator_id__ref_stg_creators_.ef48439d1e
[0m09:00:00.358373 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.not_null_stg_tiers_creator_id.03c2bb8892
[0m09:00:00.358739 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.relationships_stg_posts_creator_id__creator_id__ref_stg_creators_.ef48439d1e
[0m09:00:00.361067 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_tiers_creator_id.03c2bb8892"
[0m09:00:00.361347 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.unique_stg_posts_post_id.ec6994c39f
[0m09:00:00.361676 [debug] [Thread-3 (]: Began running node test.patreon_analytics.not_null_stg_tiers_price_usd.41c4bc4c96
[0m09:00:00.362214 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.unique_stg_posts_post_id.ec6994c39f
[0m09:00:00.362533 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_tiers_price_usd.41c4bc4c96) - Creating connection
[0m09:00:00.362835 [debug] [Thread-5 (]: Began running node test.patreon_analytics.not_null_stg_tiers_tier_id.a8da115175
[0m09:00:00.363147 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_tiers_price_usd.41c4bc4c96'
[0m09:00:00.363362 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.not_null_stg_tiers_creator_id.03c2bb8892
[0m09:00:00.363648 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_tiers_tier_id.a8da115175) - Creating connection
[0m09:00:00.363849 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.not_null_stg_tiers_price_usd.41c4bc4c96
[0m09:00:00.364190 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.not_null_stg_tiers_creator_id.03c2bb8892
[0m09:00:00.364398 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_tiers_tier_id.a8da115175'
[0m09:00:00.367045 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_tiers_price_usd.41c4bc4c96"
[0m09:00:00.367291 [debug] [Thread-4 (]: Began running node test.patreon_analytics.relationships_stg_tiers_creator_id__creator_id__ref_stg_creators_.d7715cdf0a
[0m09:00:00.367495 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.not_null_stg_tiers_tier_id.a8da115175
[0m09:00:00.367782 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.relationships_stg_tiers_creator_id__creator_id__ref_stg_creators_.d7715cdf0a) - Creating connection
[0m09:00:00.370423 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_tiers_tier_id.a8da115175"
[0m09:00:00.370792 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.relationships_stg_tiers_creator_id__creator_id__ref_stg_creators_.d7715cdf0a'
[0m09:00:00.371223 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.relationships_stg_tiers_creator_id__creator_id__ref_stg_creators_.d7715cdf0a
[0m09:00:00.374160 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.relationships_stg_tiers_creator_id__creator_id__ref_stg_creators_.d7715cdf0a"
[0m09:00:00.374405 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.not_null_stg_tiers_price_usd.41c4bc4c96
[0m09:00:00.374919 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.not_null_stg_tiers_price_usd.41c4bc4c96
[0m09:00:00.375137 [debug] [Thread-3 (]: Began running node test.patreon_analytics.unique_stg_tiers_tier_id.edaf87f122
[0m09:00:00.375388 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.unique_stg_tiers_tier_id.edaf87f122) - Creating connection
[0m09:00:00.375623 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.unique_stg_tiers_tier_id.edaf87f122'
[0m09:00:00.375833 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.not_null_stg_tiers_tier_id.a8da115175
[0m09:00:00.376064 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.unique_stg_tiers_tier_id.edaf87f122
[0m09:00:00.376454 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.not_null_stg_tiers_tier_id.a8da115175
[0m09:00:00.376667 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.relationships_stg_tiers_creator_id__creator_id__ref_stg_creators_.d7715cdf0a
[0m09:00:00.379181 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.unique_stg_tiers_tier_id.edaf87f122"
[0m09:00:00.379539 [debug] [Thread-5 (]: Began running node model.patreon_analytics.fct_creator_monthly_performance
[0m09:00:00.379998 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.relationships_stg_tiers_creator_id__creator_id__ref_stg_creators_.d7715cdf0a
[0m09:00:00.380337 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.fct_creator_monthly_performance) - Creating connection
[0m09:00:00.380573 [debug] [Thread-4 (]: Began running node test.patreon_analytics.accepted_values_stg_transactions_transaction_status__pending__succeeded__failed__refunded.93b1b6f227
[0m09:00:00.380819 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.fct_creator_monthly_performance'
[0m09:00:00.381143 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.accepted_values_stg_transactions_transaction_status__pending__succeeded__failed__refunded.93b1b6f227) - Creating connection
[0m09:00:00.381376 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.fct_creator_monthly_performance
[0m09:00:00.381574 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.accepted_values_stg_transactions_transaction_status__pending__succeeded__failed__refunded.93b1b6f227'
[0m09:00:00.381743 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.unique_stg_tiers_tier_id.edaf87f122
[0m09:00:00.391427 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.fct_creator_monthly_performance"
[0m09:00:00.391662 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.accepted_values_stg_transactions_transaction_status__pending__succeeded__failed__refunded.93b1b6f227
[0m09:00:00.392116 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.unique_stg_tiers_tier_id.edaf87f122
[0m09:00:00.395064 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.accepted_values_stg_transactions_transaction_status__pending__succeeded__failed__refunded.93b1b6f227"
[0m09:00:00.395344 [debug] [Thread-3 (]: Began running node test.patreon_analytics.accepted_values_stg_transactions_transaction_type__pledge_payment__refund__chargeback.1ac4aaca05
[0m09:00:00.395799 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.accepted_values_stg_transactions_transaction_type__pledge_payment__refund__chargeback.1ac4aaca05) - Creating connection
[0m09:00:00.396057 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.accepted_values_stg_transactions_transaction_type__pledge_payment__refund__chargeback.1ac4aaca05'
[0m09:00:00.396237 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.accepted_values_stg_transactions_transaction_type__pledge_payment__refund__chargeback.1ac4aaca05
[0m09:00:00.398813 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.accepted_values_stg_transactions_transaction_type__pledge_payment__refund__chargeback.1ac4aaca05"
[0m09:00:00.399245 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.accepted_values_stg_transactions_transaction_status__pending__succeeded__failed__refunded.93b1b6f227
[0m09:00:00.399528 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.fct_creator_monthly_performance
[0m09:00:00.399924 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.accepted_values_stg_transactions_transaction_status__pending__succeeded__failed__refunded.93b1b6f227
[0m09:00:00.400285 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.fct_creator_monthly_performance
[0m09:00:00.400496 [debug] [Thread-4 (]: Began running node test.patreon_analytics.not_null_stg_transactions_pledge_id.176713ed98
[0m09:00:00.400710 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.accepted_values_stg_transactions_transaction_type__pledge_payment__refund__chargeback.1ac4aaca05
[0m09:00:00.400938 [debug] [Thread-5 (]: Began running node test.patreon_analytics.not_null_stg_transactions_transaction_id.1d00a8e2e4
[0m09:00:00.401332 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_transactions_pledge_id.176713ed98) - Creating connection
[0m09:00:00.401826 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.accepted_values_stg_transactions_transaction_type__pledge_payment__refund__chargeback.1ac4aaca05
[0m09:00:00.402156 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_stg_transactions_transaction_id.1d00a8e2e4) - Creating connection
[0m09:00:00.402357 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_transactions_pledge_id.176713ed98'
[0m09:00:00.402564 [debug] [Thread-3 (]: Began running node test.patreon_analytics.unique_stg_transactions_transaction_id.ee9a78a396
[0m09:00:00.402791 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_stg_transactions_transaction_id.1d00a8e2e4'
[0m09:00:00.403074 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.not_null_stg_transactions_pledge_id.176713ed98
[0m09:00:00.403502 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.unique_stg_transactions_transaction_id.ee9a78a396) - Creating connection
[0m09:00:00.403712 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.not_null_stg_transactions_transaction_id.1d00a8e2e4
[0m09:00:00.407429 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_transactions_pledge_id.176713ed98"
[0m09:00:00.407754 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.unique_stg_transactions_transaction_id.ee9a78a396'
[0m09:00:00.410161 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.not_null_stg_transactions_transaction_id.1d00a8e2e4"
[0m09:00:00.410535 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.unique_stg_transactions_transaction_id.ee9a78a396
[0m09:00:00.414050 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.unique_stg_transactions_transaction_id.ee9a78a396"
[0m09:00:00.414532 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.not_null_stg_transactions_pledge_id.176713ed98
[0m09:00:00.414873 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.not_null_stg_transactions_pledge_id.176713ed98
[0m09:00:00.415057 [debug] [Thread-4 (]: Began running node test.patreon_analytics.accepted_values_fct_creator_monthly_performance_plan_type__lite__pro__premium.1487829c6d
[0m09:00:00.415292 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.accepted_values_fct_creator_monthly_performance_plan_type__lite__pro__premium.1487829c6d) - Creating connection
[0m09:00:00.415463 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.accepted_values_fct_creator_monthly_performance_plan_type__lite__pro__premium.1487829c6d'
[0m09:00:00.415617 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.accepted_values_fct_creator_monthly_performance_plan_type__lite__pro__premium.1487829c6d
[0m09:00:00.417953 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.accepted_values_fct_creator_monthly_performance_plan_type__lite__pro__premium.1487829c6d"
[0m09:00:00.418277 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.not_null_stg_transactions_transaction_id.1d00a8e2e4
[0m09:00:00.418676 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.unique_stg_transactions_transaction_id.ee9a78a396
[0m09:00:00.419158 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.not_null_stg_transactions_transaction_id.1d00a8e2e4
[0m09:00:00.419566 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.unique_stg_transactions_transaction_id.ee9a78a396
[0m09:00:00.419852 [debug] [Thread-5 (]: Began running node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_id.4643f34720
[0m09:00:00.420115 [debug] [Thread-3 (]: Began running node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_month_key.1c715c9d68
[0m09:00:00.420437 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_id.4643f34720) - Creating connection
[0m09:00:00.420638 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.accepted_values_fct_creator_monthly_performance_plan_type__lite__pro__premium.1487829c6d
[0m09:00:00.420874 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_month_key.1c715c9d68) - Creating connection
[0m09:00:00.421058 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_id.4643f34720'
[0m09:00:00.421376 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.accepted_values_fct_creator_monthly_performance_plan_type__lite__pro__premium.1487829c6d
[0m09:00:00.421548 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_month_key.1c715c9d68'
[0m09:00:00.421721 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_id.4643f34720
[0m09:00:00.421903 [debug] [Thread-4 (]: Began running node test.patreon_analytics.not_null_fct_creator_monthly_performance_month_start_date.ed79aa56db
[0m09:00:00.422092 [debug] [Thread-3 (]: Began compiling node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_month_key.1c715c9d68
[0m09:00:00.424793 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_id.4643f34720"
[0m09:00:00.425112 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.not_null_fct_creator_monthly_performance_month_start_date.ed79aa56db) - Creating connection
[0m09:00:00.427480 [debug] [Thread-3 (]: Writing injected SQL for node "test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_month_key.1c715c9d68"
[0m09:00:00.427797 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.not_null_fct_creator_monthly_performance_month_start_date.ed79aa56db'
[0m09:00:00.428084 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.not_null_fct_creator_monthly_performance_month_start_date.ed79aa56db
[0m09:00:00.430724 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.not_null_fct_creator_monthly_performance_month_start_date.ed79aa56db"
[0m09:00:00.431120 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_id.4643f34720
[0m09:00:00.431311 [debug] [Thread-3 (]: Began executing node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_month_key.1c715c9d68
[0m09:00:00.431652 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_id.4643f34720
[0m09:00:00.431846 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.not_null_fct_creator_monthly_performance_month_start_date.ed79aa56db
[0m09:00:00.432130 [debug] [Thread-3 (]: Finished running node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_month_key.1c715c9d68
[0m09:00:00.432338 [debug] [Thread-5 (]: Began running node test.patreon_analytics.unique_fct_creator_monthly_performance_creator_month_key.e5e4345d05
[0m09:00:00.432638 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.not_null_fct_creator_monthly_performance_month_start_date.ed79aa56db
[0m09:00:00.432949 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.unique_fct_creator_monthly_performance_creator_month_key.e5e4345d05) - Creating connection
[0m09:00:00.433177 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.unique_fct_creator_monthly_performance_creator_month_key.e5e4345d05'
[0m09:00:00.433340 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.unique_fct_creator_monthly_performance_creator_month_key.e5e4345d05
[0m09:00:00.435664 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.unique_fct_creator_monthly_performance_creator_month_key.e5e4345d05"
[0m09:00:00.436077 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.unique_fct_creator_monthly_performance_creator_month_key.e5e4345d05
[0m09:00:00.436368 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.unique_fct_creator_monthly_performance_creator_month_key.e5e4345d05
[0m09:00:00.674287 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0dc11-7652-19e8-a4c3-f903225fb1d9) - Created
[0m09:00:01.181632 [debug] [Thread-2 (]: SQL status: OK in 1.040 seconds
[0m09:00:01.183219 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0dc11-7652-19e8-a4c3-f903225fb1d9, command-id=01f0dc11-7669-18b0-8f2e-710f09d2a216) - Closing
[0m09:00:01.216353 [debug] [Thread-2 (]: Writing injected SQL for node "model.patreon_analytics.metricflow_time_spine"
[0m09:00:01.217341 [debug] [Thread-2 (]: Began executing node model.patreon_analytics.metricflow_time_spine
[0m09:00:01.217684 [debug] [Thread-2 (]: On model.patreon_analytics.metricflow_time_spine: Close
[0m09:00:01.217908 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0dc11-7652-19e8-a4c3-f903225fb1d9) - Closing
[0m09:00:01.376466 [debug] [Thread-2 (]: Finished running node model.patreon_analytics.metricflow_time_spine
[0m09:00:01.380792 [debug] [MainThread]: Command end result
[0m09:00:01.472232 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m09:00:01.474945 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m09:00:01.481051 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m09:00:01.483179 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=generate_catalog) - Creating connection
[0m09:00:01.483443 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m09:00:01.483589 [info ] [MainThread]: Building catalog
[0m09:00:01.485627 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=('patreon_dev', 'analytics_staging')) - Creating connection
[0m09:00:01.485956 [debug] [ThreadPool]: Acquiring new databricks connection '('patreon_dev', 'analytics_staging')'
[0m09:00:01.486203 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=('patreon_dev', 'raw')) - Creating connection
[0m09:00:01.489509 [debug] [ThreadPool]: Using databricks connection "('patreon_dev', 'analytics_staging')"
[0m09:00:01.489918 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=('patreon_dev', 'analytics_raw')) - Creating connection
[0m09:00:01.490313 [debug] [ThreadPool]: Acquiring new databricks connection '('patreon_dev', 'raw')'
[0m09:00:01.490565 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=('patreon_dev', 'analytics')) - Creating connection
[0m09:00:01.490790 [debug] [ThreadPool]: On ('patreon_dev', 'analytics_staging'): /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "('patreon_dev', 'analytics_staging')"} */

    
SELECT current_catalog()

  
[0m09:00:01.490975 [debug] [ThreadPool]: Acquiring new databricks connection '('patreon_dev', 'analytics_raw')'
[0m09:00:01.492623 [debug] [ThreadPool]: Using databricks connection "('patreon_dev', 'raw')"
[0m09:00:01.492832 [debug] [ThreadPool]: Acquiring new databricks connection '('patreon_dev', 'analytics')'
[0m09:00:01.492983 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:00:01.494286 [debug] [ThreadPool]: Using databricks connection "('patreon_dev', 'analytics_raw')"
[0m09:00:01.494542 [debug] [ThreadPool]: On ('patreon_dev', 'raw'): /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "('patreon_dev', 'raw')"} */

    
SELECT current_catalog()

  
[0m09:00:01.495903 [debug] [ThreadPool]: Using databricks connection "('patreon_dev', 'analytics')"
[0m09:00:01.496370 [debug] [ThreadPool]: On ('patreon_dev', 'analytics_raw'): /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "('patreon_dev', 'analytics_raw')"} */

    
SELECT current_catalog()

  
[0m09:00:01.496882 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:00:01.497192 [debug] [ThreadPool]: On ('patreon_dev', 'analytics'): /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "('patreon_dev', 'analytics')"} */

    
SELECT current_catalog()

  
[0m09:00:01.497458 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:00:01.497777 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:00:02.219883 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-7740-1ff4-bb8b-3ac0457a94b3) - Created
[0m09:00:02.233395 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-7742-1dfe-95e6-4e28d177c4fa) - Created
[0m09:00:02.246665 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-7741-1a17-b716-f4abea1b7387) - Created
[0m09:00:02.304082 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-7741-1e9d-97bb-b7e46a5ca86e) - Created
[0m09:00:02.562685 [debug] [ThreadPool]: SQL status: OK in 1.070 seconds
[0m09:00:02.566531 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-7740-1ff4-bb8b-3ac0457a94b3, command-id=01f0dc11-7755-1901-b6d8-9140ca4caeb0) - Closing
[0m09:00:02.581920 [debug] [ThreadPool]: Using databricks connection "('patreon_dev', 'raw')"
[0m09:00:02.582580 [debug] [ThreadPool]: On ('patreon_dev', 'raw'): /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "('patreon_dev', 'raw')"} */

    
SHOW TABLE EXTENDED IN `patreon_dev`.`raw` LIKE 'patrons|transactions|tiers|creators|engagement_events|posts|pledges'

  
[0m09:00:02.594410 [debug] [ThreadPool]: SQL status: OK in 1.100 seconds
[0m09:00:02.595793 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-7741-1e9d-97bb-b7e46a5ca86e, command-id=01f0dc11-7762-1ab8-8288-5ee24acad69e) - Closing
[0m09:00:02.598527 [debug] [ThreadPool]: Using databricks connection "('patreon_dev', 'analytics_staging')"
[0m09:00:02.598780 [debug] [ThreadPool]: On ('patreon_dev', 'analytics_staging'): /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "('patreon_dev', 'analytics_staging')"} */

    
SHOW TABLE EXTENDED IN `patreon_dev`.`analytics_staging` LIKE 'stg_pledges|stg_engagement_events|stg_posts|stg_creators|stg_patrons|stg_tiers|stg_transactions'

  
[0m09:00:02.626690 [debug] [ThreadPool]: SQL status: OK in 1.130 seconds
[0m09:00:02.627905 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-7742-1dfe-95e6-4e28d177c4fa, command-id=01f0dc11-7758-1971-8c21-254117a6e6ae) - Closing
[0m09:00:02.629854 [debug] [ThreadPool]: Using databricks connection "('patreon_dev', 'analytics_raw')"
[0m09:00:02.630234 [debug] [ThreadPool]: On ('patreon_dev', 'analytics_raw'): /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "('patreon_dev', 'analytics_raw')"} */

    
SHOW TABLE EXTENDED IN `patreon_dev`.`analytics_raw` LIKE 'patrons|transactions|tiers|creators|engagement_events|posts|pledges'

  
[0m09:00:02.669739 [debug] [ThreadPool]: SQL status: OK in 1.170 seconds
[0m09:00:02.672551 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-7741-1a17-b716-f4abea1b7387, command-id=01f0dc11-775a-13f1-9409-d0ec13e9f060) - Closing
[0m09:00:02.677694 [debug] [ThreadPool]: Using databricks connection "('patreon_dev', 'analytics')"
[0m09:00:02.677995 [debug] [ThreadPool]: On ('patreon_dev', 'analytics'): /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "('patreon_dev', 'analytics')"} */

    
SHOW TABLE EXTENDED IN `patreon_dev`.`analytics` LIKE 'metricflow_time_spine'

  
[0m09:00:03.346372 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "('patreon_dev', 'raw')"} */

    
SHOW TABLE EXTENDED IN `patreon_dev`.`raw` LIKE 'patrons|transactions|tiers|creators|engagement_events|posts|pledges'

  
: [SCHEMA_NOT_FOUND] The schema `patreon_dev`.`raw` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS. SQLSTATE: 42704
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [SCHEMA_NOT_FOUND] org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException: [SCHEMA_NOT_FOUND] The schema `patreon_dev`.`raw` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException: [SCHEMA_NOT_FOUND] The schema `patreon_dev`.`raw` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS. SQLSTATE: 42704
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.requireScExists(ManagedCatalogSessionCatalog.scala:613)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.doListTables(ManagedCatalogSessionCatalog.scala:2593)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.doListTables(ManagedCatalogSessionCatalog.scala:2707)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:410)
	at org.apache.spark.sql.execution.command.ShowTablesCommand.$anonfun$run$49(tables.scala:1428)
	at scala.Option.map(Option.scala:242)
	at org.apache.spark.sql.execution.command.ShowTablesCommand.run(tables.scala:1428)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:431)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:411)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:701)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:847)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:847)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.requireScExists(ManagedCatalogSessionCatalog.scala:613)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.doListTables(ManagedCatalogSessionCatalog.scala:2593)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.doListTables(ManagedCatalogSessionCatalog.scala:2707)
		at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:410)
		at org.apache.spark.sql.execution.command.ShowTablesCommand.$anonfun$run$49(tables.scala:1428)
		at scala.Option.map(Option.scala:242)
		at org.apache.spark.sql.execution.command.ShowTablesCommand.run(tables.scala:1428)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		... 62 more
, operation-id=01f0dc11-778d-1429-aea9-04b2c7f118de
[0m09:00:03.348949 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro show_table_extended
: Database Error
  [SCHEMA_NOT_FOUND] The schema `patreon_dev`.`raw` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
  To tolerate the error on drop use DROP SCHEMA IF EXISTS. SQLSTATE: 42704
[0m09:00:03.349802 [debug] [ThreadPool]: On ('patreon_dev', 'raw'): Close
[0m09:00:03.350241 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-7740-1ff4-bb8b-3ac0457a94b3) - Closing
[0m09:00:03.507531 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=('patreon_dev', 'analytics_marts')) - Creating connection
[0m09:00:03.507833 [debug] [ThreadPool]: Acquiring new databricks connection '('patreon_dev', 'analytics_marts')'
[0m09:00:03.509649 [debug] [ThreadPool]: Using databricks connection "('patreon_dev', 'analytics_marts')"
[0m09:00:03.509895 [debug] [ThreadPool]: On ('patreon_dev', 'analytics_marts'): /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "('patreon_dev', 'analytics_marts')"} */

    
SELECT current_catalog()

  
[0m09:00:03.510066 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:00:03.581147 [debug] [ThreadPool]: SQL status: OK in 0.980 seconds
[0m09:00:03.582516 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-7741-1e9d-97bb-b7e46a5ca86e, command-id=01f0dc11-7790-10e5-a1d0-4325a899ff2a) - Closing
[0m09:00:03.586245 [debug] [ThreadPool]: On ('patreon_dev', 'analytics_staging'): Close
[0m09:00:03.586470 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-7741-1e9d-97bb-b7e46a5ca86e) - Closing
[0m09:00:04.086447 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-7859-14af-b0f4-7956966b18ff) - Created
[0m09:00:04.378469 [debug] [ThreadPool]: SQL status: OK in 0.870 seconds
[0m09:00:04.381033 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-7859-14af-b0f4-7956966b18ff, command-id=01f0dc11-7873-16ef-89bb-532b51f01eda) - Closing
[0m09:00:04.388456 [debug] [ThreadPool]: Using databricks connection "('patreon_dev', 'analytics_marts')"
[0m09:00:04.389440 [debug] [ThreadPool]: On ('patreon_dev', 'analytics_marts'): /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "('patreon_dev', 'analytics_marts')"} */

    
SHOW TABLE EXTENDED IN `patreon_dev`.`analytics_marts` LIKE 'fct_creator_monthly_performance'

  
[0m09:00:04.944206 [debug] [ThreadPool]: SQL status: OK in 2.270 seconds
[0m09:00:04.945816 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-7741-1a17-b716-f4abea1b7387, command-id=01f0dc11-779b-1b42-b2f7-b3ed2b1e2dcc) - Closing
[0m09:00:04.946390 [debug] [ThreadPool]: On ('patreon_dev', 'analytics'): Close
[0m09:00:04.946566 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-7741-1a17-b716-f4abea1b7387) - Closing
[0m09:00:05.231689 [debug] [ThreadPool]: SQL status: OK in 0.840 seconds
[0m09:00:05.233090 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-7859-14af-b0f4-7956966b18ff, command-id=01f0dc11-78a1-1551-9bf6-80d123a21044) - Closing
[0m09:00:05.234486 [debug] [ThreadPool]: On ('patreon_dev', 'analytics_marts'): Close
[0m09:00:05.234680 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-7859-14af-b0f4-7956966b18ff) - Closing
[0m09:00:06.827648 [debug] [ThreadPool]: SQL status: OK in 4.200 seconds
[0m09:00:06.830271 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dc11-7742-1dfe-95e6-4e28d177c4fa, command-id=01f0dc11-7794-1722-8f16-c3162355d7ab) - Closing
[0m09:00:06.832708 [debug] [ThreadPool]: On ('patreon_dev', 'analytics_raw'): Close
[0m09:00:06.832891 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dc11-7742-1dfe-95e6-4e28d177c4fa) - Closing
[0m09:00:07.006466 [debug] [MainThread]: Wrote artifact CatalogArtifact to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/catalog.json
[0m09:00:07.040676 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m09:00:07.043080 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m09:00:07.043339 [info ] [MainThread]: Catalog written to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/catalog.json
[0m09:00:07.043811 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PropertyMovedToConfigDeprecation: 42 occurrences
- MissingArgumentsPropertyInGenericTestDeprecation: 11 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m09:00:07.058494 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": true, "command_wall_clock_time": 21.74825, "process_in_blocks": "0", "process_kernel_time": 0.632612, "process_mem_max_rss": "291979264", "process_out_blocks": "0", "process_user_time": 5.816773}
[0m09:00:07.058883 [debug] [MainThread]: Command `dbt docs generate` succeeded at 09:00:07.058832 after 21.75 seconds
[0m09:00:07.059119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112032000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x132865af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x132865a30>]}
[0m09:00:07.059327 [debug] [MainThread]: Flushing usage events
[0m09:00:07.393637 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:00:38.592524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114a46e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11202aae0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11202a1b0>]}


============================== 09:00:38.596236 | c58b7aa6-9ace-4f9d-8d7c-297fdc8c75b3 ==============================
[0m09:00:38.596236 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m09:00:38.596672 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'use_experimental_parser': 'False', 'no_print': 'None', 'printer_width': '80', 'indirect_selection': 'eager', 'version_check': 'True', 'cache_selected_only': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'target_path': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'log_format': 'default', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'debug': 'False', 'introspect': 'True', 'empty': 'None', 'invocation_command': 'dbt docs serve', 'quiet': 'False', 'use_colors': 'True', 'static_parser': 'True'}
[0m09:00:39.056544 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:00:39.056858 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:00:39.057034 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:00:39.666754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c58b7aa6-9ace-4f9d-8d7c-297fdc8c75b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1117e7170>]}
[0m09:00:39.700182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c58b7aa6-9ace-4f9d-8d7c-297fdc8c75b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x132772420>]}
13:08:44.432468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107056cc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10982ec90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10982e6f0>]}


============================== 13:08:44.437222 | 39dd43bf-10be-40d9-9356-342153eda947 ==============================
13:08:44.437222 [info ] [MainThread]: Running with dbt=1.11.0-rc3
13:08:44.437595 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'use_colors': 'False', 'static_parser': 'True', 'quiet': 'False', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'no_print': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'target_path': 'None', 'empty': 'None', 'log_cache_events': 'False', 'invocation_command': "dbt --no-use-colors show --inline \nselect \n    date_trunc('month', cancelled_at) as churn_month,\n    count(*) as churned_pledges,\n    sum(pledge_amount_usd) as churned_mrr\nfrom {{ ref('stg_pledges') }}\nwhere pledge_status = 'cancelled'\n    and cancelled_at >= dateadd('year', -1, current_date())\ngroup by 1\norder by 1 desc\n --favor-state --limit 12 --output json", 'introspect': 'True', 'partial_parse': 'True', 'indirect_selection': 'eager', 'debug': 'False', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'cache_selected_only': 'False', 'write_json': 'True', 'warn_error_options': "WarnErrorOptionsV2(error=['NoNodesForSelectionCriteria'], warn=[], silence=[])", 'version_check': 'True', 'warn_error': 'None', 'printer_width': '80', 'log_format': 'default'}
13:08:44.442751 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_HOST'
13:08:44.445132 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.06963446, "process_in_blocks": "0", "process_kernel_time": 0.255402, "process_mem_max_rss": "130613248", "process_out_blocks": "0", "process_user_time": 1.533018}
13:08:44.445453 [debug] [MainThread]: Command `dbt show` failed at 13:08:44.445399 after 0.07 seconds
13:08:44.445654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096fef00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096069c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10982e0f0>]}
13:08:44.445849 [debug] [MainThread]: Flushing usage events
13:08:44.763097 [debug] [MainThread]: An error was encountered while trying to flush usage events
13:10:23.060692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c08290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110435cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1104354f0>]}


============================== 13:10:23.066503 | c35e0403-ea1e-4b0f-bed2-f0f033a911c6 ==============================
13:10:23.066503 [info ] [MainThread]: Running with dbt=1.11.0-rc3
13:10:23.067168 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'write_json': 'True', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'warn_error': 'None', 'quiet': 'False', 'target_path': 'None', 'fail_fast': 'False', 'partial_parse': 'True', 'invocation_command': "dbt --no-use-colors show --inline \nselect \n    date_trunc('month', cancelled_at) as churn_month,\n    count(*) as churned_pledges,\n    sum(pledge_amount_usd) as churned_mrr\nfrom {{ ref('stg_pledges') }}\nwhere pledge_status = 'cancelled'\n    and cancelled_at >= dateadd('year', -1, current_date())\ngroup by 1\norder by 1 desc\n --favor-state --limit 12 --output json", 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'warn_error_options': "WarnErrorOptionsV2(error=['NoNodesForSelectionCriteria'], warn=[], silence=[])", 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'no_print': 'None', 'use_colors': 'False', 'debug': 'False', 'cache_selected_only': 'False', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'printer_width': '80', 'version_check': 'True', 'static_parser': 'True', 'introspect': 'True'}
13:10:23.072188 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_HOST'
13:10:23.078000 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.14539991, "process_in_blocks": "0", "process_kernel_time": 0.31067, "process_mem_max_rss": "128958464", "process_out_blocks": "0", "process_user_time": 1.676765}
13:10:23.078616 [debug] [MainThread]: Command `dbt show` failed at 13:10:23.078544 after 0.15 seconds
13:10:23.078892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ba1730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107381430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10710f050>]}
13:10:23.079173 [debug] [MainThread]: Flushing usage events
13:10:23.567953 [debug] [MainThread]: An error was encountered while trying to flush usage events
13:13:32.269382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c0cb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107135d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107135820>]}


============================== 13:13:32.276499 | 4bbe4f9a-a546-4686-8284-e216e1b422f5 ==============================
13:13:32.276499 [info ] [MainThread]: Running with dbt=1.11.0-rc3
13:13:32.276936 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'printer_width': '80', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'use_experimental_parser': 'False', 'quiet': 'False', 'use_colors': 'False', 'target_path': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error': 'None', 'log_cache_events': 'False', 'warn_error_options': "WarnErrorOptionsV2(error=['NoNodesForSelectionCriteria'], warn=[], silence=[])", 'indirect_selection': 'eager', 'invocation_command': "dbt --no-use-colors show --inline \nselect \n    date_trunc('month', cancelled_at) as churn_month,\n    count(*) as churned_pledges,\n    sum(pledge_amount_usd) as churned_mrr\nfrom {{ ref('stg_pledges') }}\nwhere pledge_status = 'cancelled'\n    and cancelled_at >= dateadd('year', -1, current_date())\ngroup by 1\norder by 1 desc\n --favor-state --limit 12 --output json", 'log_format': 'default', 'write_json': 'True', 'fail_fast': 'False', 'partial_parse': 'True', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'introspect': 'True'}
13:13:32.283188 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_HOST'
13:13:32.286428 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.15430154, "process_in_blocks": "0", "process_kernel_time": 0.331712, "process_mem_max_rss": "131612672", "process_out_blocks": "0", "process_user_time": 1.900592}
13:13:32.287031 [debug] [MainThread]: Command `dbt show` failed at 13:13:32.286809 after 0.15 seconds
13:13:32.287530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e391c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10710c2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071352b0>]}
13:13:32.287789 [debug] [MainThread]: Flushing usage events
13:13:32.935140 [debug] [MainThread]: An error was encountered while trying to flush usage events
13:14:41.377001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a708950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af359a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af35340>]}


============================== 13:14:41.381756 | aee45a98-b39b-4d4b-99dd-3598cb05320d ==============================
13:14:41.381756 [info ] [MainThread]: Running with dbt=1.11.0-rc3
13:14:41.382289 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'cache_selected_only': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': "dbt --no-use-colors show --inline \nselect \n    p.patron_id,\n    pat.email,\n    pat.display_name,\n    p.pledge_id,\n    p.creator_id,\n    p.pledge_amount_usd,\n    p.churn_reason,\n    p.started_at,\n    p.ended_at,\n    p.pledge_duration_days\nfrom {{ ref('stg_pledges') }} p\nleft join {{ ref('stg_patrons') }} pat on p.patron_id = pat.patron_id\nwhere p.pledge_status = 'churned'\norder by p.ended_at desc\n --favor-state --limit 20 --output json", 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'introspect': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'write_json': 'True', 'use_colors': 'False', 'use_experimental_parser': 'False', 'empty': 'None', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'warn_error_options': "WarnErrorOptionsV2(error=['NoNodesForSelectionCriteria'], warn=[], silence=[])", 'printer_width': '80', 'log_cache_events': 'False', 'target_path': 'None', 'debug': 'False', 'no_print': 'None'}
13:14:41.389051 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_HOST'
13:14:41.391300 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.07023188, "process_in_blocks": "0", "process_kernel_time": 0.295838, "process_mem_max_rss": "130973696", "process_out_blocks": "0", "process_user_time": 1.681593}
13:14:41.391655 [debug] [MainThread]: Command `dbt show` failed at 13:14:41.391599 after 0.07 seconds
13:14:41.391891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f11b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ac6dc70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af34b90>]}
13:14:41.392655 [debug] [MainThread]: Flushing usage events
13:14:41.748519 [debug] [MainThread]: An error was encountered while trying to flush usage events
13:19:13.939994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee660f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f032660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f031fd0>]}


============================== 13:19:13.944259 | 4653b2be-f261-4eb6-b16a-8236d0425554 ==============================
13:19:13.944259 [info ] [MainThread]: Running with dbt=1.11.0-rc3
13:19:13.944799 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'no_print': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': "WarnErrorOptionsV2(error=['NoNodesForSelectionCriteria'], warn=[], silence=[])", 'use_colors': 'False', 'quiet': 'False', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'write_json': 'True', 'printer_width': '80', 'version_check': 'True', 'fail_fast': 'False', 'invocation_command': "dbt --no-use-colors show --inline \nselect \n    p.patron_id,\n    pat.email,\n    pat.display_name,\n    p.pledge_id,\n    p.creator_id,\n    p.pledge_amount_usd,\n    p.churn_reason,\n    p.started_at,\n    p.ended_at,\n    p.pledge_duration_days\nfrom {{ ref('stg_pledges') }} p\nleft join {{ ref('stg_patrons') }} pat on p.patron_id = pat.patron_id\nwhere p.pledge_status = 'churned'\norder by p.ended_at desc\n --favor-state --limit 20 --output json", 'debug': 'False', 'cache_selected_only': 'False'}
13:19:13.949381 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_HOST'
13:19:13.951639 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.105694294, "process_in_blocks": "0", "process_kernel_time": 0.293292, "process_mem_max_rss": "129859584", "process_out_blocks": "0", "process_user_time": 1.736327}
13:19:13.952112 [debug] [MainThread]: Command `dbt show` failed at 13:19:13.952032 after 0.11 seconds
13:19:13.952353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7d5970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e516600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f032300>]}
13:19:13.952589 [debug] [MainThread]: Flushing usage events
13:19:14.313992 [debug] [MainThread]: An error was encountered while trying to flush usage events
13:20:06.994769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103ee38c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062365a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106235fa0>]}


============================== 13:20:06.999230 | 2e5678a1-88c6-4b7f-88ea-d3e1b166a5b4 ==============================
13:20:06.999230 [info ] [MainThread]: Running with dbt=1.11.0-rc3
13:20:06.999811 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'printer_width': '80', 'empty': 'None', 'target_path': 'None', 'invocation_command': "dbt --no-use-colors show --inline select p.patron_id, pat.email, pat.display_name, p.pledge_id, p.creator_id, p.pledge_amount_usd, p.churn_reason, p.started_at, p.ended_at, p.pledge_duration_days from {{ ref('stg_pledges') }} p left join {{ ref('stg_patrons') }} pat on p.patron_id = pat.patron_id where p.pledge_status = 'churned' order by p.ended_at desc --favor-state --limit 20 --output json", 'introspect': 'True', 'log_format': 'default', 'warn_error': 'None', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'version_check': 'True', 'debug': 'False', 'fail_fast': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'write_json': 'True', 'warn_error_options': "WarnErrorOptionsV2(error=['NoNodesForSelectionCriteria'], warn=[], silence=[])", 'use_colors': 'False', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'no_print': 'None'}
13:20:07.004120 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_HOST'
13:20:07.006603 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 0.06287338, "process_in_blocks": "0", "process_kernel_time": 0.196889, "process_mem_max_rss": "127991808", "process_out_blocks": "0", "process_user_time": 1.608364}
13:20:07.007037 [debug] [MainThread]: Command `dbt show` failed at 13:20:07.006974 after 0.06 seconds
13:20:07.007265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052077a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106235be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061cea50>]}
13:20:07.007483 [debug] [MainThread]: Flushing usage events
13:20:07.352323 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:47:48.160260 [error] [MainThread]: Encountered an error:

[0m13:47:48.304659 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tarik/miniconda3/envs/sp2/lib/python3.12/site-packages/dbt/cli/requires.py", line 182, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarik/miniconda3/envs/sp2/lib/python3.12/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarik/miniconda3/envs/sp2/lib/python3.12/site-packages/dbt/cli/requires.py", line 276, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarik/miniconda3/envs/sp2/lib/python3.12/site-packages/dbt/cli/requires.py", line 321, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarik/miniconda3/envs/sp2/lib/python3.12/site-packages/dbt/cli/requires.py", line 368, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarik/miniconda3/envs/sp2/lib/python3.12/site-packages/dbt/cli/main.py", line 307, in docs_serve
    results = task.run()
              ^^^^^^^^^^
  File "/Users/tarik/miniconda3/envs/sp2/lib/python3.12/site-packages/dbt/task/docs/serve.py", line 29, in run
    httpd.serve_forever()
  File "/Users/tarik/miniconda3/envs/sp2/lib/python3.12/socketserver.py", line 235, in serve_forever
    ready = selector.select(poll_interval)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/tarik/miniconda3/envs/sp2/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

[0m13:47:48.343891 [debug] [MainThread]: Resource report: {"command_name": "serve", "command_success": false, "command_wall_clock_time": 17229.723, "process_in_blocks": "0", "process_kernel_time": 1.416799, "process_mem_max_rss": "248479744", "process_out_blocks": "0", "process_user_time": 3.129296}
[0m13:47:48.383609 [debug] [MainThread]: Command `dbt docs serve` failed at 13:47:48.383084 after 17229.76 seconds
[0m13:47:48.390848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102c8aff0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1159166f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11426a720>]}
[0m13:47:48.393314 [debug] [MainThread]: Flushing usage events
[0m14:49:27.670143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10769af30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111727500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111726db0>]}


============================== 14:49:27.674353 | 815bd74d-c908-4b3a-b838-1ef7c201fb79 ==============================
[0m14:49:27.674353 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:49:27.674712 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'printer_width': '80', 'quiet': 'False', 'cache_selected_only': 'False', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'introspect': 'True', 'partial_parse': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'no_print': 'None', 'warn_error': 'None', 'empty': 'None', 'target_path': 'None', 'version_check': 'True', 'invocation_command': 'dbt debug', 'debug': 'False', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'write_json': 'True'}
[0m14:49:27.686365 [info ] [MainThread]: dbt version: 1.11.0-rc3
[0m14:49:27.686734 [info ] [MainThread]: python version: 3.12.12
[0m14:49:27.686942 [info ] [MainThread]: python path: /Users/tarik/miniconda3/envs/sp2/bin/python3.12
[0m14:49:27.687086 [info ] [MainThread]: os info: macOS-26.1-arm64-arm-64bit
[0m14:49:27.691317 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'DATABRICKS_HOST'
[0m14:49:27.693491 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.06946779, "process_in_blocks": "0", "process_kernel_time": 0.173379, "process_mem_max_rss": "132186112", "process_out_blocks": "0", "process_user_time": 1.341851}
[0m14:49:27.693771 [debug] [MainThread]: Command `dbt debug` failed at 14:49:27.693723 after 0.07 seconds
[0m14:49:27.693949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111727440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111032120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1108124b0>]}
[0m14:49:27.694131 [debug] [MainThread]: Flushing usage events
[0m14:49:28.101878 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:49:38.569904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136f1ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d2ecf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d2e5a0>]}


============================== 14:49:38.573599 | 64787dd5-fdc4-4388-acd8-3ddd535f07be ==============================
[0m14:49:38.573599 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:49:38.573962 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'debug': 'False', 'no_print': 'None', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'write_json': 'True', 'empty': 'None', 'fail_fast': 'False', 'static_parser': 'True', 'quiet': 'False', 'introspect': 'True', 'printer_width': '80', 'warn_error': 'None', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'log_cache_events': 'False', 'version_check': 'True'}
[0m14:49:38.584543 [info ] [MainThread]: dbt version: 1.11.0-rc3
[0m14:49:38.584916 [info ] [MainThread]: python version: 3.12.12
[0m14:49:38.585136 [info ] [MainThread]: python path: /Users/tarik/miniconda3/envs/sp2/bin/python3.12
[0m14:49:38.585280 [info ] [MainThread]: os info: macOS-26.1-arm64-arm-64bit
[0m14:49:39.125086 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:49:39.125423 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:49:39.125594 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:49:40.933446 [info ] [MainThread]: Using profiles dir at /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project
[0m14:49:40.934523 [info ] [MainThread]: Using profiles.yml file at /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/profiles.yml
[0m14:49:40.935068 [info ] [MainThread]: Using dbt_project.yml file at /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/dbt_project.yml
[0m14:49:40.935285 [info ] [MainThread]: adapter type: databricks
[0m14:49:40.935435 [info ] [MainThread]: adapter version: 1.11.3
[0m14:49:41.235900 [info ] [MainThread]: Configuration:
[0m14:49:41.236539 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:49:41.237126 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:49:41.238992 [info ] [MainThread]: Required dependencies:
[0m14:49:41.239480 [debug] [MainThread]: Executing "git --help"
[0m14:49:41.278525 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:49:41.279724 [debug] [MainThread]: STDERR: "b''"
[0m14:49:41.280384 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:49:41.280663 [info ] [MainThread]: Connection:
[0m14:49:41.280921 [info ] [MainThread]:   host: dbc-789165f8-da9f.cloud.databricks.com
[0m14:49:41.281078 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/256523061edbbcb5
[0m14:49:41.281217 [info ] [MainThread]:   catalog: patreon_dev
[0m14:49:41.281350 [info ] [MainThread]:   schema: analytics
[0m14:49:41.281827 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m14:49:41.408924 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m14:49:41.409692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '64787dd5-fdc4-4388-acd8-3ddd535f07be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12453cc20>]}
[0m14:49:41.410368 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m14:49:41.410631 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m14:49:41.410863 [debug] [MainThread]: Using databricks connection "debug"
[0m14:49:41.411020 [debug] [MainThread]: On debug: select 1 as id
[0m14:49:41.411204 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:49:42.232760 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0dd0b-7ab5-12a6-bca7-03ce9b026aa7) - Created
[0m14:49:51.621224 [debug] [MainThread]: SQL status: OK in 10.210 seconds
[0m14:49:51.628752 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0dd0b-7ab5-12a6-bca7-03ce9b026aa7, command-id=01f0dd0b-7ada-127a-ac83-ccf414e22e33) - Closing
[0m14:49:51.859693 [debug] [MainThread]: On debug: Close
[0m14:49:51.864546 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0dd0b-7ab5-12a6-bca7-03ce9b026aa7) - Closing
[0m14:49:52.039981 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:49:52.048546 [info ] [MainThread]: [32mAll checks passed![0m
[0m14:49:52.065297 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 13.538244, "process_in_blocks": "0", "process_kernel_time": 0.532408, "process_mem_max_rss": "237961216", "process_out_blocks": "0", "process_user_time": 2.487754}
[0m14:49:52.065987 [debug] [MainThread]: Command `dbt debug` succeeded at 14:49:52.065875 after 13.54 seconds
[0m14:49:52.066541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10364af90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1235eb080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112f5db80>]}
[0m14:49:52.066868 [debug] [MainThread]: Flushing usage events
[0m14:49:52.433838 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:50:11.021771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab47530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd2b2c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd2ac00>]}


============================== 14:50:11.025493 | 75c54bee-a8c0-43df-b592-c56b3dc62b8a ==============================
[0m14:50:11.025493 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m14:50:11.025930 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt deps', 'fail_fast': 'False', 'warn_error': 'None', 'quiet': 'False', 'static_parser': 'True', 'target_path': 'None', 'indirect_selection': 'eager', 'printer_width': '80', 'use_colors': 'True', 'debug': 'False', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'version_check': 'True', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'partial_parse': 'True'}
[0m14:50:11.124610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '75c54bee-a8c0-43df-b592-c56b3dc62b8a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bd2b1a0>]}
[0m14:50:11.195234 [debug] [MainThread]: Set downloads directory='/var/folders/17/44fmtn0931x0k_v05ghc5pl40000gn/T/dbt-downloads-dc_s9frb'
[0m14:50:11.195598 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m14:50:11.649993 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m14:50:11.652668 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m14:50:11.887134 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m14:50:11.903619 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json
[0m14:50:12.121992 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json 200
[0m14:50:12.133559 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `calogica/dbt_expectations` package is deprecated in favor of
`metaplane/dbt_expectations`. Please update your `packages.yml` configuration to
use `metaplane/dbt_expectations` instead.
[0m14:50:12.135150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '75c54bee-a8c0-43df-b592-c56b3dc62b8a', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf167e0>]}
[0m14:50:12.151389 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json
[0m14:50:12.388749 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json 200
[0m14:50:12.394199 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m14:50:12.582617 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m14:50:12.585076 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m14:50:13.253282 [info ] [MainThread]: Installed from version 1.3.3
[0m14:50:13.253586 [info ] [MainThread]: Up to date!
[0m14:50:13.253817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '75c54bee-a8c0-43df-b592-c56b3dc62b8a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdd6cc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdd6990>]}
[0m14:50:13.254049 [info ] [MainThread]: Installing calogica/dbt_expectations
[0m14:50:14.223023 [info ] [MainThread]: Installed from version 0.10.4
[0m14:50:14.223306 [info ] [MainThread]: Up to date!
[0m14:50:14.223509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '75c54bee-a8c0-43df-b592-c56b3dc62b8a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0377d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c037200>]}
[0m14:50:14.223720 [info ] [MainThread]: Installing dbt-labs/codegen
[0m14:50:14.702581 [info ] [MainThread]: Installed from version 0.14.0
[0m14:50:14.702991 [info ] [MainThread]: Up to date!
[0m14:50:14.703242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '75c54bee-a8c0-43df-b592-c56b3dc62b8a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0beb70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0be900>]}
[0m14:50:14.703456 [info ] [MainThread]: Installing calogica/dbt_date
[0m14:50:15.099986 [info ] [MainThread]: Installed from version 0.10.1
[0m14:50:15.100265 [info ] [MainThread]: Up to date!
[0m14:50:15.100463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '75c54bee-a8c0-43df-b592-c56b3dc62b8a', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bfa3da0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bfa1b20>]}
[0m14:50:15.101676 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PackageRedirectDeprecation: 2 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:50:15.103635 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 4.128906, "process_in_blocks": "0", "process_kernel_time": 0.422925, "process_mem_max_rss": "144818176", "process_out_blocks": "0", "process_user_time": 1.94507}
[0m14:50:15.104145 [debug] [MainThread]: Command `dbt deps` succeeded at 14:50:15.103890 after 4.13 seconds
[0m14:50:15.104380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aba46e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bf54e60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab1ddc0>]}
[0m14:50:15.104578 [debug] [MainThread]: Flushing usage events
[0m14:50:15.430197 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:24:58.530927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063a2210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077023f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107842480>]}


============================== 15:24:58.535363 | 47ce4dee-1774-4cd3-acb2-14bd1984a078 ==============================
[0m15:24:58.535363 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m15:24:58.535838 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'use_colors': 'True', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'no_print': 'None', 'warn_error': 'None', 'write_json': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'log_format': 'default', 'empty': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'debug': 'False', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'version_check': 'True', 'partial_parse': 'True', 'introspect': 'True', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m15:25:00.025838 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:25:00.026295 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:25:00.026504 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:25:01.976752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187d6660>]}
[0m15:25:02.029492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2a8b00>]}
[0m15:25:02.030042 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m15:25:02.164581 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:25:02.165166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1191c8f50>]}
[0m15:25:02.235425 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m15:25:02.358032 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m15:25:02.358776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1191facf0>]}
[0m15:25:02.379796 [warn ] [MainThread]: [[33mWARNING[0m][PropertyMovedToConfigDeprecation]: Deprecated functionality
Found `meta` as a top-level property of `models[0].columns[0]` in file
`models/marts/core/schema.yml`. The `meta` top-level property should be moved
into the `config` of `models[0].columns[0]`.
[0m15:25:02.380152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105391d60>]}
[0m15:25:04.396367 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on 'stg_creators' in
package 'patreon_analytics' (models/staging/schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m15:25:04.396948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119546900>]}
[0m15:25:04.786023 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.intermediate
- models.patreon_analytics.marts.finance
[0m15:25:04.793319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119abb4a0>]}
[0m15:25:04.896884 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m15:25:04.899782 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m15:25:04.929776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f72150>]}
[0m15:25:04.930206 [info ] [MainThread]: Found 9 models, 7 seeds, 50 data tests, 7 sources, 4 metrics, 1126 macros, 1 semantic model
[0m15:25:04.930410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119b43d10>]}
[0m15:25:04.932063 [info ] [MainThread]: 
[0m15:25:04.932270 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:25:04.932418 [info ] [MainThread]: 
[0m15:25:04.932739 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:25:04.932890 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:25:04.936566 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m15:25:04.936929 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m15:25:04.942855 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m15:25:04.943476 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m15:25:04.943763 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m15:25:04.944174 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m15:25:04.944396 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m15:25:04.944580 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:25:04.944772 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m15:25:04.946349 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m15:25:04.948117 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m15:25:04.948511 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m15:25:04.948761 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m15:25:04.949401 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:25:04.949594 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:25:06.936566 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-6cff-19e3-820f-682b7d8df680) - Created
[0m15:25:06.945392 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-6d02-1b16-970c-2f0120206256) - Created
[0m15:25:06.947589 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-6d03-1d9c-a4ed-ec86690cf308) - Created
[0m15:25:15.810250 [debug] [ThreadPool]: SQL status: OK in 10.860 seconds
[0m15:25:15.811004 [debug] [ThreadPool]: SQL status: OK in 10.860 seconds
[0m15:25:15.811328 [debug] [ThreadPool]: SQL status: OK in 10.870 seconds
[0m15:25:15.824561 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd10-6d02-1b16-970c-2f0120206256, command-id=01f0dd10-6d4b-1ac0-89de-c10ae1043600) - Closing
[0m15:25:15.825562 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd10-6cff-19e3-820f-682b7d8df680, command-id=01f0dd10-6d4b-190a-a2f2-4c499a7b7b6e) - Closing
[0m15:25:15.826359 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd10-6d03-1d9c-a4ed-ec86690cf308, command-id=01f0dd10-6d4c-1fc3-89a0-da4da92a6a07) - Closing
[0m15:25:16.034795 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m15:25:16.035836 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-6cff-19e3-820f-682b7d8df680) - Closing
[0m15:25:16.188954 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m15:25:16.189406 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-6d03-1d9c-a4ed-ec86690cf308) - Closing
[0m15:25:16.346465 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m15:25:16.346858 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-6d02-1b16-970c-2f0120206256) - Closing
[0m15:25:16.528060 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_staging) - Creating connection
[0m15:25:16.528544 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_marts) - Creating connection
[0m15:25:16.528746 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_staging'
[0m15:25:16.529279 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_raw) - Creating connection
[0m15:25:16.529554 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_marts'
[0m15:25:16.530025 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics) - Creating connection
[0m15:25:16.535543 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_staging"
[0m15:25:16.535960 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_raw'
[0m15:25:16.537970 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics'
[0m15:25:16.539405 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_staging'

  
[0m15:25:16.544966 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_marts"
[0m15:25:16.546543 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_raw"
[0m15:25:16.548095 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics"
[0m15:25:16.548341 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:25:16.548535 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_marts'

  
[0m15:25:16.548740 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_raw'

  
[0m15:25:16.548930 [debug] [ThreadPool]: On list_patreon_dev_analytics: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics'

  
[0m15:25:16.549502 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:25:16.550109 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:25:16.550508 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:25:17.384633 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-735f-11e8-8b6e-7c1676bc1430) - Created
[0m15:25:17.385002 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-7360-1eb1-9d0a-57202a743f99) - Created
[0m15:25:17.385837 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-735f-1e7a-b934-9bb64a47940d) - Created
[0m15:25:17.455909 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-7374-13bb-9b1f-17033c8718a8) - Created
[0m15:25:20.517585 [debug] [ThreadPool]: SQL status: OK in 3.950 seconds
[0m15:25:20.666650 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd10-735f-11e8-8b6e-7c1676bc1430, command-id=01f0dd10-7382-145b-9e3a-39cb37e36a1e) - Closing
[0m15:25:20.677239 [debug] [ThreadPool]: SQL status: OK in 4.050 seconds
[0m15:25:20.679332 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: Close
[0m15:25:20.687312 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-735f-11e8-8b6e-7c1676bc1430) - Closing
[0m15:25:20.772779 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd10-735f-1e7a-b934-9bb64a47940d, command-id=01f0dd10-7382-1830-8797-798a8f5d69c0) - Closing
[0m15:25:20.779379 [debug] [ThreadPool]: SQL status: OK in 4.230 seconds
[0m15:25:20.784554 [debug] [ThreadPool]: SQL status: OK in 4.230 seconds
[0m15:25:20.793351 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd10-7360-1eb1-9d0a-57202a743f99, command-id=01f0dd10-7382-1b84-8eb8-1610440ef678) - Closing
[0m15:25:20.796172 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd10-7374-13bb-9b1f-17033c8718a8, command-id=01f0dd10-738d-14b5-9ccf-5dd3eeb879af) - Closing
[0m15:25:20.881947 [debug] [ThreadPool]: On list_patreon_dev_analytics: Close
[0m15:25:20.882727 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-735f-1e7a-b934-9bb64a47940d) - Closing
[0m15:25:21.047371 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: Close
[0m15:25:21.048001 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-7360-1eb1-9d0a-57202a743f99) - Closing
[0m15:25:21.238935 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: Close
[0m15:25:21.239170 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd10-7374-13bb-9b1f-17033c8718a8) - Closing
[0m15:25:21.392879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119af57c0>]}
[0m15:25:21.398655 [debug] [Thread-3 (]: Began running node model.patreon_analytics.metricflow_time_spine
[0m15:25:21.398968 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_creators
[0m15:25:21.399257 [debug] [Thread-5 (]: Began running node model.patreon_analytics.stg_engagement_events
[0m15:25:21.400001 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_patrons
[0m15:25:21.399768 [info ] [Thread-3 (]: 1 of 9 START sql table model analytics.metricflow_time_spine ................... [RUN]
[0m15:25:21.400316 [info ] [Thread-4 (]: 2 of 9 START sql view model analytics_staging.stg_creators ..................... [RUN]
[0m15:25:21.400604 [info ] [Thread-5 (]: 3 of 9 START sql view model analytics_staging.stg_engagement_events ............ [RUN]
[0m15:25:21.401636 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.metricflow_time_spine) - Creating connection
[0m15:25:21.400921 [info ] [Thread-6 (]: 4 of 9 START sql view model analytics_staging.stg_patrons ...................... [RUN]
[0m15:25:21.402479 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_creators) - Creating connection
[0m15:25:21.403521 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_engagement_events) - Creating connection
[0m15:25:21.403972 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.patreon_analytics.metricflow_time_spine'
[0m15:25:21.404681 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_patrons) - Creating connection
[0m15:25:21.404973 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_creators'
[0m15:25:21.405239 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_engagement_events'
[0m15:25:21.405506 [debug] [Thread-3 (]: Began compiling node model.patreon_analytics.metricflow_time_spine
[0m15:25:21.405743 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_patrons'
[0m15:25:21.406097 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.stg_creators
[0m15:25:21.406333 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.stg_engagement_events
[0m15:25:21.418028 [debug] [Thread-3 (]: Using databricks connection "model.patreon_analytics.metricflow_time_spine"
[0m15:25:21.418370 [debug] [Thread-6 (]: Began compiling node model.patreon_analytics.stg_patrons
[0m15:25:21.420804 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.stg_creators"
[0m15:25:21.422317 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.metricflow_time_spine"} */


        select timestampdiff(day, date_trunc('day', cast('2020-01-01' as timestamp)), date_trunc('day', cast('2030-12-31' as timestamp)))
[0m15:25:21.425719 [debug] [Thread-6 (]: Writing injected SQL for node "model.patreon_analytics.stg_patrons"
[0m15:25:21.428082 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:25:21.434702 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.stg_engagement_events"
[0m15:25:21.435317 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.stg_creators
[0m15:25:21.445587 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m15:25:21.448792 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m15:25:21.449381 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1191fac90>]}
[0m15:25:21.454893 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.stg_engagement_events
[0m15:25:21.458087 [debug] [Thread-4 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_creators`
[0m15:25:21.460480 [debug] [Thread-5 (]: MATERIALIZING VIEW
[0m15:25:21.460782 [debug] [Thread-6 (]: Began executing node model.patreon_analytics.stg_patrons
[0m15:25:21.468171 [debug] [Thread-4 (]: Writing runtime sql for node "model.patreon_analytics.stg_creators"
[0m15:25:21.468963 [debug] [Thread-5 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_engagement_events`
[0m15:25:21.471835 [debug] [Thread-6 (]: MATERIALIZING VIEW
[0m15:25:21.472505 [debug] [Thread-5 (]: Writing runtime sql for node "model.patreon_analytics.stg_engagement_events"
[0m15:25:21.473597 [debug] [Thread-6 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_patrons`
[0m15:25:21.473950 [debug] [Thread-4 (]: Using databricks connection "model.patreon_analytics.stg_creators"
[0m15:25:21.474374 [debug] [Thread-6 (]: Writing runtime sql for node "model.patreon_analytics.stg_patrons"
[0m15:25:21.474691 [debug] [Thread-4 (]: On model.patreon_analytics.stg_creators: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_creators"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_creators`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`creators`
),

staged as (
    select
        creator_id,
        creator_name,
        email,
        category,
        subcategory,
        country_code,
        coalesce(currency_code, 'USD') as currency_code,
        coalesce(plan_type, 'pro') as plan_type,
        coalesce(is_nsfw, false) as is_nsfw,
        coalesce(is_verified, false) as is_verified,
        created_at,
        first_pledge_received_at,
        last_post_at,
        coalesce(status, 'active') as status,
        
        -- Derived fields
        datediff(day, created_at, coalesce(first_pledge_received_at, current_timestamp())) as days_to_first_pledge,
        datediff(month, created_at, current_timestamp()) as account_age_months,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:25:21.475101 [debug] [Thread-5 (]: Using databricks connection "model.patreon_analytics.stg_engagement_events"
[0m15:25:21.475369 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:25:21.475691 [debug] [Thread-5 (]: On model.patreon_analytics.stg_engagement_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_engagement_events"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_engagement_events`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`engagement_events`
),

staged as (
    select
        event_id,
        patron_id,
        creator_id,
        post_id,
        event_type,
        event_at,
        
        -- Derived fields
        date_trunc('month', event_at) as event_month,
        date_trunc('day', event_at) as event_date,
        
        -- Engagement weighting (for composite scores)
        case event_type
            when 'view' then 1
            when 'like' then 3
            when 'comment' then 5
            when 'share' then 7
            else 1
        end as engagement_weight,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:25:21.476039 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m15:25:21.476691 [debug] [Thread-6 (]: Using databricks connection "model.patreon_analytics.stg_patrons"
[0m15:25:21.477157 [debug] [Thread-6 (]: On model.patreon_analytics.stg_patrons: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_patrons"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_patrons`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`patrons`
),

staged as (
    select
        patron_id,
        patron_name,
        email,
        country_code,
        created_at,
        first_pledge_at,
        coalesce(lifetime_spend_usd, 0) as lifetime_spend_usd,
        coalesce(status, 'active') as status,
        
        -- Derived fields
        datediff(month, created_at, current_timestamp()) as account_age_months,
        case 
            when lifetime_spend_usd >= 1000 then 'whale'
            when lifetime_spend_usd >= 500 then 'high_value'
            when lifetime_spend_usd >= 100 then 'regular'
            else 'casual'
        end as patron_value_tier,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:25:21.477465 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m15:25:22.440872 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd10-7671-15e6-8802-b93d27db8b25) - Created
[0m15:25:22.444834 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd10-7671-1eba-b346-62be186a221c) - Created
[0m15:25:22.450724 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd10-766f-1102-9c45-546d92bcfedf) - Created
[0m15:25:22.621587 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd10-7680-1006-8875-4318a7ca50ce) - Created
[0m15:25:23.163911 [debug] [Thread-5 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_engagement_events"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_engagement_events`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`engagement_events`
),

staged as (
    select
        event_id,
        patron_id,
        creator_id,
        post_id,
        event_type,
        event_at,
        
        -- Derived fields
        date_trunc('month', event_at) as event_month,
        date_trunc('day', event_at) as event_date,
        
        -- Engagement weighting (for composite scores)
        case event_type
            when 'view' then 1
            when 'like' then 3
            when 'comment' then 5
            when 'share' then 7
            else 1
        end as engagement_weight,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd10-7685-19a4-9906-251b46241e21
[0m15:25:23.164623 [debug] [Thread-5 (]: On model.patreon_analytics.stg_engagement_events: Close
[0m15:25:23.164890 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd10-7671-15e6-8802-b93d27db8b25) - Closing
[0m15:25:23.165935 [debug] [Thread-6 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_patrons"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_patrons`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`patrons`
),

staged as (
    select
        patron_id,
        patron_name,
        email,
        country_code,
        created_at,
        first_pledge_at,
        coalesce(lifetime_spend_usd, 0) as lifetime_spend_usd,
        coalesce(status, 'active') as status,
        
        -- Derived fields
        datediff(month, created_at, current_timestamp()) as account_age_months,
        case 
            when lifetime_spend_usd >= 1000 then 'whale'
            when lifetime_spend_usd >= 500 then 'high_value'
            when lifetime_spend_usd >= 100 then 'regular'
            else 'casual'
        end as patron_value_tier,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd10-7686-1279-a91f-06e91d47d421
[0m15:25:23.178261 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_creators"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_creators`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`creators`
),

staged as (
    select
        creator_id,
        creator_name,
        email,
        category,
        subcategory,
        country_code,
        coalesce(currency_code, 'USD') as currency_code,
        coalesce(plan_type, 'pro') as plan_type,
        coalesce(is_nsfw, false) as is_nsfw,
        coalesce(is_verified, false) as is_verified,
        created_at,
        first_pledge_received_at,
        last_post_at,
        coalesce(status, 'active') as status,
        
        -- Derived fields
        datediff(day, created_at, coalesce(first_pledge_received_at, current_timestamp())) as days_to_first_pledge,
        datediff(month, created_at, current_timestamp()) as account_age_months,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd10-76a1-11e9-843a-f675fe8e00cc
[0m15:25:23.189824 [debug] [Thread-3 (]: SQL status: OK in 1.760 seconds
[0m15:25:23.196560 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd10-766f-1102-9c45-546d92bcfedf, command-id=01f0dd10-768a-1873-b3d6-3be335ee8749) - Closing
[0m15:25:23.339724 [debug] [Thread-6 (]: On model.patreon_analytics.stg_patrons: Close
[0m15:25:23.340055 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd10-7671-1eba-b346-62be186a221c) - Closing
[0m15:25:23.349408 [debug] [Thread-5 (]: Database Error in model stg_engagement_events (models/staging/stg_engagement_events.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_engagement_events.sql
[0m15:25:23.493457 [debug] [Thread-4 (]: On model.patreon_analytics.stg_creators: Close
[0m15:25:23.493834 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd10-7680-1006-8875-4318a7ca50ce) - Closing
[0m15:25:23.495484 [debug] [Thread-6 (]: Database Error in model stg_patrons (models/staging/stg_patrons.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_patrons.sql
[0m15:25:23.674196 [debug] [Thread-4 (]: Database Error in model stg_creators (models/staging/stg_creators.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_creators.sql
[0m15:25:23.681011 [debug] [Thread-3 (]: Writing injected SQL for node "model.patreon_analytics.metricflow_time_spine"
[0m15:25:23.682421 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10678d010>]}
[0m15:25:23.683031 [error] [Thread-5 (]: 3 of 9 ERROR creating sql view model analytics_staging.stg_engagement_events ... [[31mERROR[0m in 2.27s]
[0m15:25:23.683637 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.stg_engagement_events
[0m15:25:23.684000 [debug] [Thread-5 (]: Began running node model.patreon_analytics.stg_pledges
[0m15:25:23.684266 [info ] [Thread-5 (]: 5 of 9 START sql view model analytics_staging.stg_pledges ...................... [RUN]
[0m15:25:23.684627 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_pledges) - Creating connection
[0m15:25:23.684819 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_pledges'
[0m15:25:23.684983 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.stg_pledges
[0m15:25:23.688850 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.stg_pledges"
[0m15:25:23.689564 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_engagement_events' to be skipped because of status 'error'.  Reason: Database Error in model stg_engagement_events (models/staging/stg_engagement_events.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_engagement_events.sql.
[0m15:25:23.689880 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119aae180>]}
[0m15:25:23.690343 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119fb2420>]}
[0m15:25:23.696562 [debug] [Thread-3 (]: Began executing node model.patreon_analytics.metricflow_time_spine
[0m15:25:23.697358 [error] [Thread-6 (]: 4 of 9 ERROR creating sql view model analytics_staging.stg_patrons ............. [[31mERROR[0m in 2.27s]
[0m15:25:23.700151 [error] [Thread-4 (]: 2 of 9 ERROR creating sql view model analytics_staging.stg_creators ............ [[31mERROR[0m in 2.27s]
[0m15:25:23.707645 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_patrons
[0m15:25:23.716525 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m15:25:23.716955 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.stg_pledges
[0m15:25:23.717950 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_creators
[0m15:25:23.718276 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_posts
[0m15:25:23.733812 [debug] [Thread-5 (]: MATERIALIZING VIEW
[0m15:25:23.734141 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_patrons' to be skipped because of status 'error'.  Reason: Database Error in model stg_patrons (models/staging/stg_patrons.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_patrons.sql.
[0m15:25:23.743106 [debug] [Thread-3 (]: Writing runtime sql for node "model.patreon_analytics.metricflow_time_spine"
[0m15:25:23.743519 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_tiers
[0m15:25:23.743882 [info ] [Thread-6 (]: 6 of 9 START sql view model analytics_staging.stg_posts ........................ [RUN]
[0m15:25:23.744516 [debug] [Thread-5 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_pledges`
[0m15:25:23.744856 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_creators' to be skipped because of status 'error'.  Reason: Database Error in model stg_creators (models/staging/stg_creators.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_creators.sql.
[0m15:25:23.745255 [info ] [Thread-4 (]: 7 of 9 START sql view model analytics_staging.stg_tiers ........................ [RUN]
[0m15:25:23.745792 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_posts) - Creating connection
[0m15:25:23.746246 [debug] [Thread-5 (]: Writing runtime sql for node "model.patreon_analytics.stg_pledges"
[0m15:25:23.746672 [debug] [Thread-3 (]: Using databricks connection "model.patreon_analytics.metricflow_time_spine"
[0m15:25:23.747076 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_tiers) - Creating connection
[0m15:25:23.747355 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_posts'
[0m15:25:23.747876 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.metricflow_time_spine"} */

  
    
        create or replace table `patreon_dev`.`analytics`.`metricflow_time_spine`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      with days as (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
    
    

    )

    select *
    from unioned
    where generated_number <= 4017
    order by generated_number



),

all_periods as (

    select (
        timestampadd(day, (row_number() over (order by 1) - 1), cast('2020-01-01' as timestamp))
    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(timestampadd(year, -1, d.date_day) as date) as prior_year_date_day,
        cast(timestampadd(day, -364, d.date_day) as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(timestampadd(day, -1, d.date_day) as date) as prior_date_day,
    cast(timestampadd(day, 1, d.date_day) as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    date_part('dayofweek', d.date_day) as day_of_week,
    date_part('dayofweek_iso', d.date_day) as day_of_week_iso,
    date_format(d.date_day, 'EEEE') as day_of_week_name,
    date_format(d.date_day, 'E') as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    dayofyear(d.date_day) as day_of_year,

    cast(date_trunc('week', d.date_day) as date) as week_start_date,
    cast(
        timestampadd(day, -1, timestampadd(week, 1, date_trunc('week', d.date_day)))
        as date) as week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_week_start_date,
    cast(
        timestampadd(day, -1, timestampadd(week, 1, date_trunc('week', d.prior_year_over_year_date_day)))
        as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(timestampadd(day, 6, cast(date_trunc('week', d.date_day) as date)) as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(timestampadd(day, 6, cast(date_trunc('week', d.prior_year_over_year_date_day) as date)) as date) as prior_year_iso_week_end_date,
    cast(date_part('week', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    date_format(d.date_day, 'MMMM')  as month_name,
    date_format(d.date_day, 'MMM')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(month, 1, date_trunc('month', d.date_day)))
        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(month, 1, date_trunc('month', d.prior_year_date_day)))
        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(quarter, 1, date_trunc('quarter', d.date_day)))
        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(year, 1, date_trunc('year', d.date_day)))
        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

select
    date_day as date_day
from days
  
[0m15:25:23.748308 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_tiers'
[0m15:25:23.748561 [debug] [Thread-6 (]: Began compiling node model.patreon_analytics.stg_posts
[0m15:25:23.749114 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.stg_tiers
[0m15:25:23.749449 [debug] [Thread-5 (]: Using databricks connection "model.patreon_analytics.stg_pledges"
[0m15:25:23.752493 [debug] [Thread-6 (]: Writing injected SQL for node "model.patreon_analytics.stg_posts"
[0m15:25:23.755218 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.stg_tiers"
[0m15:25:23.755707 [debug] [Thread-5 (]: On model.patreon_analytics.stg_pledges: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_pledges"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_pledges`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`pledges`
),

staged as (
    select
        pledge_id,
        patron_id,
        creator_id,
        tier_id,
        pledge_amount_usd,
        pledge_status,
        coalesce(is_first_pledge, false) as is_first_pledge,
        started_at,
        ended_at,
        pause_started_at,
        churn_reason,
        
        -- Derived fields
        date_trunc('month', started_at) as pledge_month,
        case 
            when pledge_status = 'active' then null
            else datediff(day, started_at, coalesce(ended_at, current_timestamp()))
        end as pledge_duration_days,
        
        -- Is currently active (for point-in-time analysis)
        case 
            when pledge_status = 'active' and pause_started_at is null then true
            else false
        end as is_currently_active,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:25:23.756286 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m15:25:23.757308 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.stg_tiers
[0m15:25:23.757690 [debug] [Thread-6 (]: Began executing node model.patreon_analytics.stg_posts
[0m15:25:23.759482 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m15:25:23.761275 [debug] [Thread-6 (]: MATERIALIZING VIEW
[0m15:25:23.761923 [debug] [Thread-4 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_tiers`
[0m15:25:23.762615 [debug] [Thread-6 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_posts`
[0m15:25:23.763198 [debug] [Thread-4 (]: Writing runtime sql for node "model.patreon_analytics.stg_tiers"
[0m15:25:23.763636 [debug] [Thread-6 (]: Writing runtime sql for node "model.patreon_analytics.stg_posts"
[0m15:25:23.764643 [debug] [Thread-6 (]: Using databricks connection "model.patreon_analytics.stg_posts"
[0m15:25:23.765005 [debug] [Thread-6 (]: On model.patreon_analytics.stg_posts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_posts"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_posts`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`posts`
),

staged as (
    select
        post_id,
        creator_id,
        title,
        post_type,
        access_level,
        minimum_tier_id,
        published_at,
        coalesce(is_pinned, false) as is_pinned,
        
        -- Derived fields
        date_trunc('month', published_at) as published_month,
        date_trunc('day', published_at) as published_date,
        
        -- Content categorization
        case 
            when access_level = 'public' then 'free'
            when access_level = 'patrons_only' then 'paywalled'
            when access_level = 'tier_specific' then 'premium'
            else 'unknown'
        end as content_access_type,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:25:23.765254 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m15:25:23.765541 [debug] [Thread-4 (]: Using databricks connection "model.patreon_analytics.stg_tiers"
[0m15:25:23.765830 [debug] [Thread-4 (]: On model.patreon_analytics.stg_tiers: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_tiers"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_tiers`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`tiers`
),

staged as (
    select
        tier_id,
        creator_id,
        tier_name,
        tier_rank,
        price_usd,
        description,
        coalesce(is_active, true) as is_active,
        created_at,
        archived_at,
        
        -- Price bucket for analysis
        case 
            when price_usd <= 5 then 'micro'
            when price_usd <= 15 then 'standard'
            when price_usd <= 30 then 'premium'
            else 'whale'
        end as price_bucket,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:25:23.766098 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:25:24.535479 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd10-77ae-1de4-aea5-40682ff75373) - Created
[0m15:25:24.718479 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd10-77be-1f86-9179-95db901fff5e) - Created
[0m15:25:24.720701 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd10-77bf-1edf-abe7-f5aa10913547) - Created
[0m15:25:25.027249 [debug] [Thread-5 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_pledges"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_pledges`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`pledges`
),

staged as (
    select
        pledge_id,
        patron_id,
        creator_id,
        tier_id,
        pledge_amount_usd,
        pledge_status,
        coalesce(is_first_pledge, false) as is_first_pledge,
        started_at,
        ended_at,
        pause_started_at,
        churn_reason,
        
        -- Derived fields
        date_trunc('month', started_at) as pledge_month,
        case 
            when pledge_status = 'active' then null
            else datediff(day, started_at, coalesce(ended_at, current_timestamp()))
        end as pledge_duration_days,
        
        -- Is currently active (for point-in-time analysis)
        case 
            when pledge_status = 'active' and pause_started_at is null then true
            else false
        end as is_currently_active,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd10-77c7-1f92-ad24-ef89ae7b672b
[0m15:25:25.027900 [debug] [Thread-5 (]: On model.patreon_analytics.stg_pledges: Close
[0m15:25:25.028218 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd10-77ae-1de4-aea5-40682ff75373) - Closing
[0m15:25:25.243710 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_tiers"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_tiers`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`tiers`
),

staged as (
    select
        tier_id,
        creator_id,
        tier_name,
        tier_rank,
        price_usd,
        description,
        coalesce(is_active, true) as is_active,
        created_at,
        archived_at,
        
        -- Price bucket for analysis
        case 
            when price_usd <= 5 then 'micro'
            when price_usd <= 15 then 'standard'
            when price_usd <= 30 then 'premium'
            else 'whale'
        end as price_bucket,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd10-77e0-1f24-bef0-0dac59b91a24
[0m15:25:25.245010 [debug] [Thread-6 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_posts"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_posts`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`posts`
),

staged as (
    select
        post_id,
        creator_id,
        title,
        post_type,
        access_level,
        minimum_tier_id,
        published_at,
        coalesce(is_pinned, false) as is_pinned,
        
        -- Derived fields
        date_trunc('month', published_at) as published_month,
        date_trunc('day', published_at) as published_date,
        
        -- Content categorization
        case 
            when access_level = 'public' then 'free'
            when access_level = 'patrons_only' then 'paywalled'
            when access_level = 'tier_specific' then 'premium'
            else 'unknown'
        end as content_access_type,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd10-77e2-1131-be56-7a107464f5be
[0m15:25:25.245580 [debug] [Thread-4 (]: On model.patreon_analytics.stg_tiers: Close
[0m15:25:25.246987 [debug] [Thread-5 (]: Database Error in model stg_pledges (models/staging/stg_pledges.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_pledges.sql
[0m15:25:25.247627 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd10-77be-1f86-9179-95db901fff5e) - Closing
[0m15:25:25.436982 [debug] [Thread-6 (]: On model.patreon_analytics.stg_posts: Close
[0m15:25:25.438351 [debug] [Thread-4 (]: Database Error in model stg_tiers (models/staging/stg_tiers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_tiers.sql
[0m15:25:25.438680 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd10-77bf-1edf-abe7-f5aa10913547) - Closing
[0m15:25:25.605945 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a4baa80>]}
[0m15:25:25.606272 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a4ba720>]}
[0m15:25:25.606827 [error] [Thread-4 (]: 7 of 9 ERROR creating sql view model analytics_staging.stg_tiers ............... [[31mERROR[0m in 1.86s]
[0m15:25:25.608215 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_tiers
[0m15:25:25.608698 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_transactions
[0m15:25:25.609267 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_tiers' to be skipped because of status 'error'.  Reason: Database Error in model stg_tiers (models/staging/stg_tiers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_tiers.sql.
[0m15:25:25.607095 [error] [Thread-5 (]: 5 of 9 ERROR creating sql view model analytics_staging.stg_pledges ............. [[31mERROR[0m in 1.92s]
[0m15:25:25.609800 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.stg_pledges
[0m15:25:25.609007 [info ] [Thread-4 (]: 8 of 9 START sql view model analytics_staging.stg_transactions ................. [RUN]
[0m15:25:25.610187 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_pledges' to be skipped because of status 'error'.  Reason: Database Error in model stg_pledges (models/staging/stg_pledges.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_pledges.sql.
[0m15:25:25.610580 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_transactions) - Creating connection
[0m15:25:25.612160 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_transactions'
[0m15:25:25.613645 [debug] [Thread-6 (]: Database Error in model stg_posts (models/staging/stg_posts.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_posts.sql
[0m15:25:25.613997 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.stg_transactions
[0m15:25:25.614715 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e48440>]}
[0m15:25:25.618290 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.stg_transactions"
[0m15:25:25.618811 [error] [Thread-6 (]: 6 of 9 ERROR creating sql view model analytics_staging.stg_posts ............... [[31mERROR[0m in 1.87s]
[0m15:25:25.619485 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.stg_transactions
[0m15:25:25.619834 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_posts
[0m15:25:25.622203 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m15:25:25.622597 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_posts' to be skipped because of status 'error'.  Reason: Database Error in model stg_posts (models/staging/stg_posts.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_posts.sql.
[0m15:25:25.628624 [debug] [Thread-4 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_transactions`
[0m15:25:25.629161 [debug] [Thread-4 (]: Writing runtime sql for node "model.patreon_analytics.stg_transactions"
[0m15:25:25.629988 [debug] [Thread-4 (]: Using databricks connection "model.patreon_analytics.stg_transactions"
[0m15:25:25.630304 [debug] [Thread-4 (]: On model.patreon_analytics.stg_transactions: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_transactions"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_transactions`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`transactions`
),

staged as (
    select
        transaction_id,
        pledge_id,
        patron_id,
        creator_id,
        transaction_type,
        transaction_status,
        gross_amount_usd,
        platform_fee_usd,
        processing_fee_usd,
        net_amount_usd,
        payment_method,
        failure_reason,
        transaction_at,
        
        -- Derived fields
        date_trunc('month', transaction_at) as transaction_month,
        date_trunc('day', transaction_at) as transaction_date,
        
        -- Fee analysis
        case 
            when gross_amount_usd > 0 and transaction_status = 'succeeded'
            then round((platform_fee_usd / gross_amount_usd) * 100, 2)
            else null
        end as platform_fee_rate_pct,
        
        case 
            when gross_amount_usd > 0 and transaction_status = 'succeeded'
            then round((processing_fee_usd / gross_amount_usd) * 100, 2)
            else null
        end as processing_fee_rate_pct,
        
        -- Success flag
        case when transaction_status = 'succeeded' then 1 else 0 end as is_successful,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m15:25:25.630517 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:25:26.293486 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd10-78ae-1cd7-b438-f3ea560ccb32) - Created
[0m15:25:26.829500 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_transactions"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_transactions`
  
  as (
    with source as (
    select * from `patreon_dev`.`raw`.`transactions`
),

staged as (
    select
        transaction_id,
        pledge_id,
        patron_id,
        creator_id,
        transaction_type,
        transaction_status,
        gross_amount_usd,
        platform_fee_usd,
        processing_fee_usd,
        net_amount_usd,
        payment_method,
        failure_reason,
        transaction_at,
        
        -- Derived fields
        date_trunc('month', transaction_at) as transaction_month,
        date_trunc('day', transaction_at) as transaction_date,
        
        -- Fee analysis
        case 
            when gross_amount_usd > 0 and transaction_status = 'succeeded'
            then round((platform_fee_usd / gross_amount_usd) * 100, 2)
            else null
        end as platform_fee_rate_pct,
        
        case 
            when gross_amount_usd > 0 and transaction_status = 'succeeded'
            then round((processing_fee_usd / gross_amount_usd) * 100, 2)
            else null
        end as processing_fee_rate_pct,
        
        -- Success flag
        case when transaction_status = 'succeeded' then 1 else 0 end as is_successful,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd10-78d1-1a63-9cbc-ddb6ac083ac7
[0m15:25:26.831348 [debug] [Thread-4 (]: On model.patreon_analytics.stg_transactions: Close
[0m15:25:26.831567 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd10-78ae-1cd7-b438-f3ea560ccb32) - Closing
[0m15:25:27.011177 [debug] [Thread-4 (]: Database Error in model stg_transactions (models/staging/stg_transactions.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_transactions.sql
[0m15:25:27.011743 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a4eade0>]}
[0m15:25:27.012939 [error] [Thread-4 (]: 8 of 9 ERROR creating sql view model analytics_staging.stg_transactions ........ [[31mERROR[0m in 1.40s]
[0m15:25:27.014528 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_transactions
[0m15:25:27.015276 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_transactions' to be skipped because of status 'error'.  Reason: Database Error in model stg_transactions (models/staging/stg_transactions.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_transactions.sql.
[0m15:25:27.015849 [debug] [Thread-5 (]: Began running node model.patreon_analytics.fct_creator_monthly_performance
[0m15:25:27.016129 [info ] [Thread-5 (]: 9 of 9 SKIP relation analytics_marts.fct_creator_monthly_performance ........... [[33mSKIP[0m]
[0m15:25:27.016440 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.fct_creator_monthly_performance
[0m15:25:33.961427 [debug] [Thread-3 (]: SQL status: OK in 10.210 seconds
[0m15:25:33.964187 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd10-766f-1102-9c45-546d92bcfedf, command-id=01f0dd10-774e-1e63-aec3-c657b5e93ccf) - Closing
[0m15:25:34.169005 [debug] [Thread-3 (]: Applying tags to relation None
[0m15:25:34.182802 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: Close
[0m15:25:34.183141 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd10-766f-1102-9c45-546d92bcfedf) - Closing
[0m15:25:34.357525 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47ce4dee-1774-4cd3-acb2-14bd1984a078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10464e930>]}
[0m15:25:34.358409 [info ] [Thread-3 (]: 1 of 9 OK created sql table model analytics.metricflow_time_spine .............. [[32mOK[0m in 12.96s]
[0m15:25:34.358860 [debug] [Thread-3 (]: Finished running node model.patreon_analytics.metricflow_time_spine
[0m15:25:34.360540 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m15:25:34.360915 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:25:34.361549 [info ] [MainThread]: 
[0m15:25:34.361957 [info ] [MainThread]: Finished running 2 table models, 7 view models in 0 hours 0 minutes and 29.43 seconds (29.43s).
[0m15:25:34.363182 [debug] [MainThread]: Command end result
[0m15:25:34.411520 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m15:25:34.420021 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m15:25:34.424609 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m15:25:34.424919 [info ] [MainThread]: 
[0m15:25:34.425198 [info ] [MainThread]: [31mCompleted with 7 errors, 0 partial successes, and 0 warnings:[0m
[0m15:25:34.425520 [info ] [MainThread]: 
[0m15:25:34.425787 [error] [MainThread]: [31mFailure in model stg_engagement_events (models/staging/stg_engagement_events.sql)[0m
[0m15:25:34.425997 [error] [MainThread]:   Database Error in model stg_engagement_events (models/staging/stg_engagement_events.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_engagement_events.sql
[0m15:25:34.426155 [info ] [MainThread]: 
[0m15:25:34.426327 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/stg_engagement_events.sql
[0m15:25:34.426469 [info ] [MainThread]: 
[0m15:25:34.426636 [error] [MainThread]: [31mFailure in model stg_patrons (models/staging/stg_patrons.sql)[0m
[0m15:25:34.426814 [error] [MainThread]:   Database Error in model stg_patrons (models/staging/stg_patrons.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_patrons.sql
[0m15:25:34.426959 [info ] [MainThread]: 
[0m15:25:34.427114 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/stg_patrons.sql
[0m15:25:34.427249 [info ] [MainThread]: 
[0m15:25:34.427408 [error] [MainThread]: [31mFailure in model stg_creators (models/staging/stg_creators.sql)[0m
[0m15:25:34.427577 [error] [MainThread]:   Database Error in model stg_creators (models/staging/stg_creators.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_creators.sql
[0m15:25:34.427720 [info ] [MainThread]: 
[0m15:25:34.427872 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/stg_creators.sql
[0m15:25:34.428001 [info ] [MainThread]: 
[0m15:25:34.428157 [error] [MainThread]: [31mFailure in model stg_tiers (models/staging/stg_tiers.sql)[0m
[0m15:25:34.428337 [error] [MainThread]:   Database Error in model stg_tiers (models/staging/stg_tiers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_tiers.sql
[0m15:25:34.428479 [info ] [MainThread]: 
[0m15:25:34.428633 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/stg_tiers.sql
[0m15:25:34.428847 [info ] [MainThread]: 
[0m15:25:34.429016 [error] [MainThread]: [31mFailure in model stg_pledges (models/staging/stg_pledges.sql)[0m
[0m15:25:34.429420 [error] [MainThread]:   Database Error in model stg_pledges (models/staging/stg_pledges.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_pledges.sql
[0m15:25:34.429676 [info ] [MainThread]: 
[0m15:25:34.429914 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/stg_pledges.sql
[0m15:25:34.430061 [info ] [MainThread]: 
[0m15:25:34.430227 [error] [MainThread]: [31mFailure in model stg_posts (models/staging/stg_posts.sql)[0m
[0m15:25:34.430405 [error] [MainThread]:   Database Error in model stg_posts (models/staging/stg_posts.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_posts.sql
[0m15:25:34.430549 [info ] [MainThread]: 
[0m15:25:34.430704 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/stg_posts.sql
[0m15:25:34.430835 [info ] [MainThread]: 
[0m15:25:34.430999 [error] [MainThread]: [31mFailure in model stg_transactions (models/staging/stg_transactions.sql)[0m
[0m15:25:34.431171 [error] [MainThread]:   Database Error in model stg_transactions (models/staging/stg_transactions.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 9 pos 18
  compiled code at target/run/patreon_analytics/models/staging/stg_transactions.sql
[0m15:25:34.431313 [info ] [MainThread]: 
[0m15:25:34.431468 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/stg_transactions.sql
[0m15:25:34.431619 [info ] [MainThread]: 
[0m15:25:34.431828 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=7 SKIP=1 NO-OP=0 TOTAL=9
[0m15:25:34.432458 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PropertyMovedToConfigDeprecation: 42 occurrences
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:25:34.436075 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 35.956577, "process_in_blocks": "0", "process_kernel_time": 0.846646, "process_mem_max_rss": "273498112", "process_out_blocks": "0", "process_user_time": 7.233647}
[0m15:25:34.436475 [debug] [MainThread]: Command `dbt run` failed at 15:25:34.436423 after 35.96 seconds
[0m15:25:34.436937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x101402f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107842840>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063a2210>]}
[0m15:25:34.437142 [debug] [MainThread]: Flushing usage events
[0m15:25:35.077117 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:02:24.462172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ad03920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11be2bcb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11be2b620>]}


============================== 16:02:24.465803 | c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989 ==============================
[0m16:02:24.465803 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:02:24.466195 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'quiet': 'False', 'no_print': 'None', 'partial_parse': 'True', 'empty': 'None', 'write_json': 'True', 'debug': 'False', 'invocation_command': 'dbt seed', 'use_experimental_parser': 'False', 'printer_width': '80', 'static_parser': 'True', 'warn_error': 'None', 'target_path': 'None', 'version_check': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'log_cache_events': 'False', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs'}
[0m16:02:25.012693 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:02:25.013046 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:02:25.013305 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:02:26.159753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185d0890>]}
[0m16:02:26.191973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e5f9d30>]}
[0m16:02:26.192433 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:02:26.283704 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:02:26.284246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1483a4890>]}
[0m16:02:26.304755 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m16:02:26.466120 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m16:02:26.466703 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://seeds/creators.csv
[0m16:02:26.466923 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://seeds/posts.csv
[0m16:02:26.467117 [debug] [MainThread]: Partial parsing: updated file: patreon_analytics://seeds/tiers.csv
[0m16:02:26.616606 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.marts.finance
- models.patreon_analytics.intermediate
[0m16:02:26.624818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14a1c3e90>]}
[0m16:02:26.705663 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m16:02:26.708742 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m16:02:26.720793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14a0d6060>]}
[0m16:02:26.721150 [info ] [MainThread]: Found 9 models, 7 seeds, 50 data tests, 7 sources, 4 metrics, 1126 macros, 1 semantic model
[0m16:02:26.721343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ef0bf0>]}
[0m16:02:26.723009 [info ] [MainThread]: 
[0m16:02:26.723246 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:02:26.723403 [info ] [MainThread]: 
[0m16:02:26.723713 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:02:26.723874 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:02:26.727491 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m16:02:26.727760 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m16:02:26.734528 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m16:02:26.734743 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m16:02:26.734899 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:27.904309 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd15-a4d8-161d-a4e4-0ef7cf5354ef) - Created
[0m16:02:36.116420 [debug] [ThreadPool]: SQL status: OK in 9.380 seconds
[0m16:02:36.129100 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd15-a4d8-161d-a4e4-0ef7cf5354ef, command-id=01f0dd15-a501-1e87-a8c7-617f713c73c7) - Closing
[0m16:02:36.397149 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m16:02:36.397845 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd15-a4d8-161d-a4e4-0ef7cf5354ef) - Closing
[0m16:02:36.628172 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_staging) - Creating connection
[0m16:02:36.628669 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_marts) - Creating connection
[0m16:02:36.628889 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_staging'
[0m16:02:36.629136 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_raw) - Creating connection
[0m16:02:36.629482 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics) - Creating connection
[0m16:02:36.629711 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_marts'
[0m16:02:36.634890 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_staging"
[0m16:02:36.635270 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_raw'
[0m16:02:36.635460 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics'
[0m16:02:36.637019 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_marts"
[0m16:02:36.637295 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_staging'

  
[0m16:02:36.640324 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics"
[0m16:02:36.758770 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_marts'

  
[0m16:02:36.759931 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_raw"
[0m16:02:36.760233 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:36.760444 [debug] [ThreadPool]: On list_patreon_dev_analytics: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics'

  
[0m16:02:36.760619 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:36.760805 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_raw'

  
[0m16:02:36.761169 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:36.761672 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:37.663111 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd15-aab2-15bc-806b-f61b5967a02d) - Created
[0m16:02:37.663832 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd15-aab5-1107-ae9e-d4facb032112) - Created
[0m16:02:37.669391 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd15-aab7-19c6-a270-57f7d4e97e38) - Created
[0m16:02:37.673397 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd15-aab7-1afc-b11a-badae301b404) - Created
[0m16:02:40.436549 [debug] [ThreadPool]: SQL status: OK in 3.680 seconds
[0m16:02:40.437124 [debug] [ThreadPool]: SQL status: OK in 3.680 seconds
[0m16:02:40.438451 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd15-aab7-1afc-b11a-badae301b404, command-id=01f0dd15-aad4-1a2c-8d34-4aa50b56df4c) - Closing
[0m16:02:40.439293 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd15-aab5-1107-ae9e-d4facb032112, command-id=01f0dd15-aad2-14cf-8a37-271777064cf1) - Closing
[0m16:02:40.439614 [debug] [ThreadPool]: On list_patreon_dev_analytics: Close
[0m16:02:40.439885 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd15-aab7-1afc-b11a-badae301b404) - Closing
[0m16:02:40.534462 [debug] [ThreadPool]: SQL status: OK in 3.770 seconds
[0m16:02:40.538097 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd15-aab7-19c6-a270-57f7d4e97e38, command-id=01f0dd15-aad3-17d7-8354-f8e814d0eb60) - Closing
[0m16:02:40.548548 [debug] [ThreadPool]: SQL status: OK in 3.790 seconds
[0m16:02:40.549832 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd15-aab2-15bc-806b-f61b5967a02d, command-id=01f0dd15-aace-1fba-8520-07978f5f7537) - Closing
[0m16:02:40.595148 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: Close
[0m16:02:40.596004 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd15-aab5-1107-ae9e-d4facb032112) - Closing
[0m16:02:40.813088 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: Close
[0m16:02:40.814300 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd15-aab7-19c6-a270-57f7d4e97e38) - Closing
[0m16:02:40.971470 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: Close
[0m16:02:40.972384 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd15-aab2-15bc-806b-f61b5967a02d) - Closing
[0m16:02:41.147348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14a0fd940>]}
[0m16:02:41.149765 [debug] [Thread-3 (]: Began running node seed.patreon_analytics.creators
[0m16:02:41.150129 [debug] [Thread-4 (]: Began running node seed.patreon_analytics.engagement_events
[0m16:02:41.150411 [debug] [Thread-5 (]: Began running node seed.patreon_analytics.patrons
[0m16:02:41.151013 [debug] [Thread-6 (]: Began running node seed.patreon_analytics.pledges
[0m16:02:41.150805 [info ] [Thread-3 (]: 1 of 7 START seed file analytics_raw.creators .................................. [RUN]
[0m16:02:41.151364 [info ] [Thread-4 (]: 2 of 7 START seed file analytics_raw.engagement_events ......................... [RUN]
[0m16:02:41.151708 [info ] [Thread-5 (]: 3 of 7 START seed file analytics_raw.patrons ................................... [RUN]
[0m16:02:41.152066 [info ] [Thread-6 (]: 4 of 7 START seed file analytics_raw.pledges ................................... [RUN]
[0m16:02:41.152520 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.creators) - Creating connection
[0m16:02:41.152875 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.engagement_events) - Creating connection
[0m16:02:41.153174 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.patrons) - Creating connection
[0m16:02:41.153427 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.pledges) - Creating connection
[0m16:02:41.153640 [debug] [Thread-3 (]: Acquiring new databricks connection 'seed.patreon_analytics.creators'
[0m16:02:41.153985 [debug] [Thread-4 (]: Acquiring new databricks connection 'seed.patreon_analytics.engagement_events'
[0m16:02:41.154295 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.patreon_analytics.patrons'
[0m16:02:41.154520 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.patreon_analytics.pledges'
[0m16:02:41.154764 [debug] [Thread-3 (]: Began compiling node seed.patreon_analytics.creators
[0m16:02:41.154968 [debug] [Thread-4 (]: Began compiling node seed.patreon_analytics.engagement_events
[0m16:02:41.155308 [debug] [Thread-5 (]: Began compiling node seed.patreon_analytics.patrons
[0m16:02:41.155577 [debug] [Thread-6 (]: Began compiling node seed.patreon_analytics.pledges
[0m16:02:41.156377 [debug] [Thread-3 (]: Began executing node seed.patreon_analytics.creators
[0m16:02:41.156654 [debug] [Thread-4 (]: Began executing node seed.patreon_analytics.engagement_events
[0m16:02:41.156946 [debug] [Thread-5 (]: Began executing node seed.patreon_analytics.patrons
[0m16:02:41.157455 [debug] [Thread-6 (]: Began executing node seed.patreon_analytics.pledges
[0m16:02:41.162221 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:02:41.164790 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:02:41.166672 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:02:41.169495 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x149fac860>]}
[0m16:02:41.169849 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x148309040>]}
[0m16:02:41.168436 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:02:41.170156 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1482ca5d0>]}
[0m16:02:41.179933 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x149932600>]}
[0m16:02:41.342250 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.creators"
[0m16:02:41.400749 [debug] [Thread-3 (]: On seed.patreon_analytics.creators: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.creators"} */

    create or replace table `patreon_dev`.`analytics_raw`.`creators` (creator_id string ,creator_name string ,email string ,category string ,subcategory string ,country_code string ,currency_code string ,plan_type string ,is_nsfw boolean ,is_verified boolean ,created_at timestamp ,first_pledge_received_at timestamp ,last_post_at timestamp ,status string )
    
    using delta
  
    
    
    
    
    
  
[0m16:02:41.420063 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:02:41.725708 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:02:41.726498 [debug] [Thread-6 (]: On seed.patreon_analytics.pledges: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.pledges"} */

    create or replace table `patreon_dev`.`analytics_raw`.`pledges` (pledge_id string ,patron_id string ,creator_id string ,tier_id string ,pledge_amount_usd decimal(10,2) ,pledge_status string ,is_first_pledge boolean ,started_at timestamp ,ended_at timestamp ,pause_started_at bigint ,churn_reason string )
    
    using delta
  
    
    
    
    
    
  
[0m16:02:41.726952 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:02:42.032167 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m16:02:42.045157 [debug] [Thread-5 (]: On seed.patreon_analytics.patrons: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.patrons"} */

    create or replace table `patreon_dev`.`analytics_raw`.`patrons` (patron_id string ,patron_name string ,email string ,country_code string ,created_at timestamp ,first_pledge_at timestamp ,lifetime_spend_usd double ,status string )
    
    using delta
  
    
    
    
    
    
  
[0m16:02:42.051855 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m16:02:42.234507 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:02:42.234856 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.engagement_events"} */

    create or replace table `patreon_dev`.`analytics_raw`.`engagement_events` (event_id string ,patron_id string ,creator_id string ,post_id string ,event_type string ,event_at timestamp )
    
    using delta
  
    
    
    
    
    
  
[0m16:02:42.235047 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:02:42.384827 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd15-ad8a-1256-b83a-541f58d44912) - Created
[0m16:02:42.422661 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd15-ad8c-170c-a43c-f6d5f37cd570) - Created
[0m16:02:42.769255 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd15-adc5-1898-97fb-902ece208353) - Created
[0m16:02:42.793343 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd15-adc6-1b60-9cb7-02d9c7cb7205) - Created
[0m16:02:46.766490 [debug] [Thread-4 (]: SQL status: OK in 4.530 seconds
[0m16:02:46.767088 [debug] [Thread-5 (]: SQL status: OK in 4.720 seconds
[0m16:02:46.767907 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd15-adc6-1b60-9cb7-02d9c7cb7205, command-id=01f0dd15-addf-104d-ae46-c6e3e77332f9) - Closing
[0m16:02:46.768366 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd15-adc5-1898-97fb-902ece208353, command-id=01f0dd15-ade2-1fe8-a806-4718da580de0) - Closing
[0m16:02:46.918349 [debug] [Thread-3 (]: SQL status: OK in 5.500 seconds
[0m16:02:46.931633 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-ad8c-170c-a43c-f6d5f37cd570, command-id=01f0dd15-ada5-1f3c-81c9-a87a5f941f25) - Closing
[0m16:02:47.759099 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.creators"
[0m16:02:47.766649 [debug] [Thread-3 (]: On seed.patreon_analytics.creators: 
          insert overwrite `patreon_dev`.`analytics_raw`.`creators` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s...
[0m16:02:49.210397 [debug] [Thread-6 (]: SQL status: OK in 7.480 seconds
[0m16:02:49.216229 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd15-ad8a-1256-b83a-541f58d44912, command-id=01f0dd15-ada0-1293-8a49-7c508c20bb50) - Closing
[0m16:02:51.553357 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:02:51.569010 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert overwrite `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:02:52.680093 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m16:02:52.690428 [debug] [Thread-5 (]: On seed.patreon_analytics.patrons: 
          insert overwrite `patreon_dev`.`analytics_raw`.`patrons` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%...
[0m16:02:53.318416 [debug] [Thread-3 (]: SQL status: OK in 5.540 seconds
[0m16:02:53.325249 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-ad8c-170c-a43c-f6d5f37cd570, command-id=01f0dd15-b103-116b-a241-3d232352b261) - Closing
[0m16:02:53.334840 [debug] [Thread-3 (]: Writing runtime SQL for node "seed.patreon_analytics.creators"
[0m16:02:53.385217 [debug] [Thread-3 (]: On seed.patreon_analytics.creators: Close
[0m16:02:53.391927 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd15-ad8c-170c-a43c-f6d5f37cd570) - Closing
[0m16:02:53.639586 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185a7ad0>]}
[0m16:02:53.652732 [info ] [Thread-3 (]: 1 of 7 OK loaded seed file analytics_raw.creators .............................. [[32mINSERT 500[0m in 12.44s]
[0m16:02:53.666021 [debug] [Thread-3 (]: Finished running node seed.patreon_analytics.creators
[0m16:02:53.672739 [debug] [Thread-3 (]: Began running node seed.patreon_analytics.posts
[0m16:02:53.679410 [info ] [Thread-3 (]: 5 of 7 START seed file analytics_raw.posts ..................................... [RUN]
[0m16:02:53.686228 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.posts) - Creating connection
[0m16:02:53.692920 [debug] [Thread-3 (]: Acquiring new databricks connection 'seed.patreon_analytics.posts'
[0m16:02:53.699491 [debug] [Thread-3 (]: Began compiling node seed.patreon_analytics.posts
[0m16:02:53.706205 [debug] [Thread-3 (]: Began executing node seed.patreon_analytics.posts
[0m16:02:53.912744 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.posts"
[0m16:02:53.919377 [debug] [Thread-3 (]: On seed.patreon_analytics.posts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.posts"} */

    create or replace table `patreon_dev`.`analytics_raw`.`posts` (post_id string ,creator_id string ,title string ,post_type string ,access_level string ,minimum_tier_id string ,published_at timestamp ,is_pinned boolean )
    
    using delta
  
    
    
    
    
    
  
[0m16:02:53.926032 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:02:54.595879 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd15-b4d1-1052-9bf6-db8494880b32) - Created
[0m16:02:54.637400 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:02:54.642765 [debug] [Thread-6 (]: On seed.patreon_analytics.pledges: 
          insert overwrite `patreon_dev`.`analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:02:57.334051 [debug] [Thread-3 (]: SQL status: OK in 3.410 seconds
[0m16:02:57.334771 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-b4d1-1052-9bf6-db8494880b32, command-id=01f0dd15-b4e9-1946-9ec4-b2147efd2192) - Closing
[0m16:02:58.136622 [debug] [Thread-5 (]: SQL status: OK in 5.440 seconds
[0m16:02:58.143494 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd15-adc5-1898-97fb-902ece208353, command-id=01f0dd15-b451-18e0-9e9d-0d23fa1572e0) - Closing
[0m16:02:58.155716 [debug] [Thread-4 (]: SQL status: OK in 6.590 seconds
[0m16:02:58.162447 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd15-adc6-1b60-9cb7-02d9c7cb7205, command-id=01f0dd15-b3b6-1127-91e5-f5fa9e3f6a56) - Closing
[0m16:02:59.208643 [debug] [Thread-6 (]: SQL status: OK in 4.570 seconds
[0m16:02:59.220496 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd15-ad8a-1256-b83a-541f58d44912, command-id=01f0dd15-b577-1f7c-a7c8-131bfd0f92b1) - Closing
[0m16:03:01.233589 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.posts"
[0m16:03:01.242647 [debug] [Thread-3 (]: On seed.patreon_analytics.posts: 
          insert overwrite `patreon_dev`.`analytics_raw`.`posts` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,...
[0m16:03:01.840178 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m16:03:01.848797 [debug] [Thread-5 (]: On seed.patreon_analytics.patrons: 
          insert into `patreon_dev`.`analytics_raw`.`patrons` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,...
[0m16:03:03.124994 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:03:03.134595 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m16:03:04.819598 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:03:04.825516 [debug] [Thread-6 (]: On seed.patreon_analytics.pledges: 
          insert into `patreon_dev`.`analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m16:03:05.770915 [debug] [Thread-5 (]: SQL status: OK in 3.920 seconds
[0m16:03:05.771306 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd15-adc5-1898-97fb-902ece208353, command-id=01f0dd15-b94b-1d5a-a9ce-5ee0b21ebcc0) - Closing
[0m16:03:05.772794 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.patreon_analytics.patrons"
[0m16:03:05.776080 [debug] [Thread-5 (]: On seed.patreon_analytics.patrons: Close
[0m16:03:05.776334 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd15-adc5-1898-97fb-902ece208353) - Closing
[0m16:03:05.982137 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169202510>]}
[0m16:03:05.983644 [info ] [Thread-5 (]: 3 of 7 OK loaded seed file analytics_raw.patrons ............................... [[32mINSERT 15000[0m in 24.83s]
[0m16:03:05.984204 [debug] [Thread-5 (]: Finished running node seed.patreon_analytics.patrons
[0m16:03:05.985362 [debug] [Thread-5 (]: Began running node seed.patreon_analytics.tiers
[0m16:03:05.987464 [info ] [Thread-5 (]: 6 of 7 START seed file analytics_raw.tiers ..................................... [RUN]
[0m16:03:05.987888 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.tiers) - Creating connection
[0m16:03:05.988081 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.patreon_analytics.tiers'
[0m16:03:05.988385 [debug] [Thread-5 (]: Began compiling node seed.patreon_analytics.tiers
[0m16:03:05.988557 [debug] [Thread-5 (]: Began executing node seed.patreon_analytics.tiers
[0m16:03:06.020993 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.tiers"
[0m16:03:06.021340 [debug] [Thread-5 (]: On seed.patreon_analytics.tiers: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.tiers"} */

    create or replace table `patreon_dev`.`analytics_raw`.`tiers` (tier_id string ,creator_id string ,tier_name string ,tier_rank bigint ,price_usd decimal(10,2) ,description string ,is_active boolean ,created_at timestamp ,archived_at bigint )
    
    using delta
  
    
    
    
    
    
  
[0m16:03:06.021526 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m16:03:06.348032 [debug] [Thread-3 (]: SQL status: OK in 5.100 seconds
[0m16:03:06.348365 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-b4d1-1052-9bf6-db8494880b32, command-id=01f0dd15-b97d-17f1-af05-2a9da6e4f7d9) - Closing
[0m16:03:06.349605 [debug] [Thread-3 (]: Writing runtime SQL for node "seed.patreon_analytics.posts"
[0m16:03:06.352872 [debug] [Thread-3 (]: On seed.patreon_analytics.posts: Close
[0m16:03:06.353134 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd15-b4d1-1052-9bf6-db8494880b32) - Closing
[0m16:03:06.543382 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1690ae660>]}
[0m16:03:06.544711 [info ] [Thread-3 (]: 5 of 7 OK loaded seed file analytics_raw.posts ................................. [[32mINSERT 8000[0m in 12.86s]
[0m16:03:06.547414 [debug] [Thread-3 (]: Finished running node seed.patreon_analytics.posts
[0m16:03:06.547662 [debug] [Thread-3 (]: Began running node seed.patreon_analytics.transactions
[0m16:03:06.552352 [info ] [Thread-3 (]: 7 of 7 START seed file analytics_raw.transactions .............................. [RUN]
[0m16:03:06.553210 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.transactions) - Creating connection
[0m16:03:06.553570 [debug] [Thread-3 (]: Acquiring new databricks connection 'seed.patreon_analytics.transactions'
[0m16:03:06.553850 [debug] [Thread-3 (]: Began compiling node seed.patreon_analytics.transactions
[0m16:03:06.554145 [debug] [Thread-3 (]: Began executing node seed.patreon_analytics.transactions
[0m16:03:06.577797 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd15-bbf0-1660-98b3-9f22db59aab9) - Created
[0m16:03:06.566924 [debug] [Thread-4 (]: SQL status: OK in 3.420 seconds
[0m16:03:06.596428 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd15-adc6-1b60-9cb7-02d9c7cb7205, command-id=01f0dd15-ba23-1d7e-bd21-add49d694f94) - Closing
[0m16:03:08.104800 [debug] [Thread-6 (]: SQL status: OK in 3.280 seconds
[0m16:03:08.117784 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd15-ad8a-1256-b83a-541f58d44912, command-id=01f0dd15-bb1f-182a-bbff-34d6a6182c30) - Closing
[0m16:03:08.272306 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:03:08.278984 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.transactions"} */

    create or replace table `patreon_dev`.`analytics_raw`.`transactions` (transaction_id string ,pledge_id string ,patron_id string ,creator_id string ,transaction_type string ,transaction_status string ,gross_amount_usd decimal(10,2) ,platform_fee_usd decimal(10,2) ,processing_fee_usd decimal(10,2) ,net_amount_usd decimal(10,2) ,payment_method string ,failure_reason string ,transaction_at timestamp )
    
    using delta
  
    
    
    
    
    
  
[0m16:03:08.291964 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:03:08.293219 [debug] [Thread-5 (]: SQL status: OK in 2.270 seconds
[0m16:03:08.343416 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd15-bbf0-1660-98b3-9f22db59aab9, command-id=01f0dd15-bc0f-13ad-8959-bb128d02571f) - Closing
[0m16:03:08.364331 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:03:08.371232 [debug] [Thread-6 (]: On seed.patreon_analytics.pledges: 
          insert into `patreon_dev`.`analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m16:03:09.075671 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.tiers"
[0m16:03:09.082963 [debug] [Thread-5 (]: On seed.patreon_analytics.tiers: 
          insert overwrite `patreon_dev`.`analytics_raw`.`tiers` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%...
[0m16:03:09.155108 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359) - Created
[0m16:03:09.805500 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:03:09.809297 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m16:03:10.065691 [debug] [Thread-6 (]: SQL status: OK in 1.690 seconds
[0m16:03:10.066081 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd15-ad8a-1256-b83a-541f58d44912, command-id=01f0dd15-bd22-16e2-af41-5f2987406a8d) - Closing
[0m16:03:10.066840 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.patreon_analytics.pledges"
[0m16:03:10.068038 [debug] [Thread-6 (]: On seed.patreon_analytics.pledges: Close
[0m16:03:10.068232 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd15-ad8a-1256-b83a-541f58d44912) - Closing
[0m16:03:10.230810 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x149931a00>]}
[0m16:03:10.231425 [info ] [Thread-6 (]: 4 of 7 OK loaded seed file analytics_raw.pledges ............................... [[32mINSERT 20302[0m in 29.08s]
[0m16:03:10.232347 [debug] [Thread-6 (]: Finished running node seed.patreon_analytics.pledges
[0m16:03:11.472160 [debug] [Thread-5 (]: SQL status: OK in 2.380 seconds
[0m16:03:11.473019 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd15-bbf0-1660-98b3-9f22db59aab9, command-id=01f0dd15-bdbd-115a-be3e-868840fcc26b) - Closing
[0m16:03:11.474435 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.patreon_analytics.tiers"
[0m16:03:11.478289 [debug] [Thread-5 (]: On seed.patreon_analytics.tiers: Close
[0m16:03:11.478756 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd15-bbf0-1660-98b3-9f22db59aab9) - Closing
[0m16:03:11.634962 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1690fd850>]}
[0m16:03:11.636060 [info ] [Thread-5 (]: 6 of 7 OK loaded seed file analytics_raw.tiers ................................. [[32mINSERT 1403[0m in 5.65s]
[0m16:03:11.637235 [debug] [Thread-5 (]: Finished running node seed.patreon_analytics.tiers
[0m16:03:11.741774 [debug] [Thread-3 (]: SQL status: OK in 3.450 seconds
[0m16:03:11.742589 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-bd96-1e12-b326-cdb4a507eb62) - Closing
[0m16:03:12.186187 [debug] [Thread-4 (]: SQL status: OK in 2.380 seconds
[0m16:03:12.192799 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd15-adc6-1b60-9cb7-02d9c7cb7205, command-id=01f0dd15-be0e-1e48-ac50-26771e37e769) - Closing
[0m16:03:15.320625 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:03:15.336546 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m16:03:16.754388 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:03:16.761684 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert overwrite `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m16:03:17.639355 [debug] [Thread-4 (]: SQL status: OK in 2.300 seconds
[0m16:03:17.639726 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd15-adc6-1b60-9cb7-02d9c7cb7205, command-id=01f0dd15-c15a-1f81-92a6-85d3fc7fbf8c) - Closing
[0m16:03:18.542073 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:03:18.544008 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m16:03:20.657248 [debug] [Thread-3 (]: SQL status: OK in 3.900 seconds
[0m16:03:20.657606 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-c28e-1c08-a316-f4ec4c6648a1) - Closing
[0m16:03:21.026703 [debug] [Thread-4 (]: SQL status: OK in 2.480 seconds
[0m16:03:21.027299 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd15-adc6-1b60-9cb7-02d9c7cb7205, command-id=01f0dd15-c332-1d43-bc7c-f80f07f8c124) - Closing
[0m16:03:21.036553 [debug] [Thread-4 (]: Writing runtime SQL for node "seed.patreon_analytics.engagement_events"
[0m16:03:21.081310 [debug] [Thread-4 (]: On seed.patreon_analytics.engagement_events: Close
[0m16:03:21.087910 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd15-adc6-1b60-9cb7-02d9c7cb7205) - Closing
[0m16:03:21.340381 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168d7c200>]}
[0m16:03:21.347343 [info ] [Thread-4 (]: 2 of 7 OK loaded seed file analytics_raw.engagement_events ..................... [[32mINSERT 45541[0m in 40.19s]
[0m16:03:21.354216 [debug] [Thread-4 (]: Finished running node seed.patreon_analytics.engagement_events
[0m16:03:24.325967 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:03:24.332593 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:03:27.182242 [debug] [Thread-3 (]: SQL status: OK in 2.850 seconds
[0m16:03:27.182676 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-c6b9-1d87-8199-40b827a30d30) - Closing
[0m16:03:31.063219 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:03:31.073188 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:03:34.017703 [debug] [Thread-3 (]: SQL status: OK in 2.940 seconds
[0m16:03:34.018229 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-cab3-1f61-a05a-ac923048d6a9) - Closing
[0m16:03:37.792077 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:03:37.799064 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:03:40.709101 [debug] [Thread-3 (]: SQL status: OK in 2.910 seconds
[0m16:03:40.709485 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-cec5-1248-b0c6-09f75b7620a3) - Closing
[0m16:03:44.589728 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:03:44.597225 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:03:47.397144 [debug] [Thread-3 (]: SQL status: OK in 2.800 seconds
[0m16:03:47.397750 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-d2cf-1ebf-8cdd-4f737e5f135a) - Closing
[0m16:03:51.304590 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:03:51.311674 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:03:54.214677 [debug] [Thread-3 (]: SQL status: OK in 2.900 seconds
[0m16:03:54.215249 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-d6c4-10ac-8ed8-66eb9f354840) - Closing
[0m16:03:57.690466 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:03:57.696798 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:04:00.337573 [debug] [Thread-3 (]: SQL status: OK in 2.640 seconds
[0m16:04:00.338497 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-da9e-1c73-be46-6da9fe432135) - Closing
[0m16:04:03.966588 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:04:03.973559 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:04:08.249967 [debug] [Thread-3 (]: SQL status: OK in 4.280 seconds
[0m16:04:08.250353 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-de4e-18c0-accd-677d9d98b4ba) - Closing
[0m16:04:11.762154 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:04:11.768899 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:04:15.076600 [debug] [Thread-3 (]: SQL status: OK in 3.310 seconds
[0m16:04:15.076959 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-e2f7-137e-960f-71e8861b56e4) - Closing
[0m16:04:18.505876 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:04:18.512230 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:04:21.612349 [debug] [Thread-3 (]: SQL status: OK in 3.100 seconds
[0m16:04:21.613153 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-e6f5-117f-ae1b-1db0529e45e3) - Closing
[0m16:04:25.184245 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:04:25.190783 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:04:29.148297 [debug] [Thread-3 (]: SQL status: OK in 3.960 seconds
[0m16:04:29.152087 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-eb03-1a1a-a8d2-4774d65e9ad9) - Closing
[0m16:04:32.607577 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:04:32.613913 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:04:36.503119 [debug] [Thread-3 (]: SQL status: OK in 3.890 seconds
[0m16:04:36.505091 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-ef62-1229-8411-77c3912feb04) - Closing
[0m16:04:40.463782 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:04:40.470923 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:04:44.041788 [debug] [Thread-3 (]: SQL status: OK in 3.570 seconds
[0m16:04:44.043272 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-f40d-1a94-a5f2-c15ce9d0849b) - Closing
[0m16:04:47.735064 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:04:47.741465 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:04:51.383574 [debug] [Thread-3 (]: SQL status: OK in 3.640 seconds
[0m16:04:51.389657 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-f860-11ba-98cc-78d531001d72) - Closing
[0m16:04:55.975825 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:04:55.982309 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:04:59.038546 [debug] [Thread-3 (]: SQL status: OK in 3.060 seconds
[0m16:04:59.039896 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359, command-id=01f0dd15-fd52-151d-a662-34a01488c147) - Closing
[0m16:04:59.045236 [debug] [Thread-3 (]: Writing runtime SQL for node "seed.patreon_analytics.transactions"
[0m16:04:59.057829 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: Close
[0m16:04:59.058667 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd15-bd7a-1e9b-9f4f-3607837e5359) - Closing
[0m16:04:59.250040 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8a0f0dd-12cd-4d54-8b8c-d73ca21c4989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1696010d0>]}
[0m16:04:59.251113 [info ] [Thread-3 (]: 7 of 7 OK loaded seed file analytics_raw.transactions .......................... [[32mINSERT 146464[0m in 112.70s]
[0m16:04:59.251801 [debug] [Thread-3 (]: Finished running node seed.patreon_analytics.transactions
[0m16:04:59.254537 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:04:59.254814 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:04:59.255347 [info ] [MainThread]: 
[0m16:04:59.255640 [info ] [MainThread]: Finished running 7 seeds in 0 hours 2 minutes and 32.53 seconds (152.53s).
[0m16:04:59.257952 [debug] [MainThread]: Command end result
[0m16:04:59.315742 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m16:04:59.320241 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m16:04:59.324975 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m16:04:59.325244 [info ] [MainThread]: 
[0m16:04:59.325507 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:04:59.325676 [info ] [MainThread]: 
[0m16:04:59.325857 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=7
[0m16:04:59.329298 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 154.88867, "process_in_blocks": "0", "process_kernel_time": 1.836114, "process_mem_max_rss": "586579968", "process_out_blocks": "0", "process_user_time": 77.1432}
[0m16:04:59.329778 [debug] [MainThread]: Command `dbt seed` succeeded at 16:04:59.329725 after 154.89 seconds
[0m16:04:59.330118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11be2bbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11be2bc80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11be2bcb0>]}
[0m16:04:59.330339 [debug] [MainThread]: Flushing usage events
[0m16:04:59.896277 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:33:27.738229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106667b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c608f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c60260>]}


============================== 16:33:27.742396 | bed0c9e1-2619-4a72-a6a1-858d85c3523d ==============================
[0m16:33:27.742396 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:33:27.742815 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'quiet': 'False', 'write_json': 'True', 'version_check': 'True', 'introspect': 'True', 'fail_fast': 'False', 'target_path': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'use_colors': 'True', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'static_parser': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'empty': 'False', 'log_format': 'default', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs'}
[0m16:33:28.314488 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:33:28.314798 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:33:28.314968 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:33:29.575394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044f17f0>]}
[0m16:33:29.615481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c637a0>]}
[0m16:33:29.616229 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:33:29.720878 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:33:29.721480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118d343e0>]}
[0m16:33:29.749411 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m16:33:29.939864 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:33:29.940359 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:33:29.940520 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:33:29.946247 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.intermediate
- models.patreon_analytics.marts.finance
[0m16:33:29.984333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1192cd5b0>]}
[0m16:33:30.084347 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m16:33:30.087851 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m16:33:30.113317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1191d6d20>]}
[0m16:33:30.113704 [info ] [MainThread]: Found 9 models, 7 seeds, 50 data tests, 7 sources, 4 metrics, 1126 macros, 1 semantic model
[0m16:33:30.116921 [info ] [MainThread]: 
[0m16:33:30.117190 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:33:30.117338 [info ] [MainThread]: 
[0m16:33:30.117650 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:33:30.117804 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:33:30.121533 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m16:33:30.122053 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m16:33:30.122299 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m16:33:30.122650 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m16:33:30.122876 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m16:33:30.123130 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m16:33:30.131835 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m16:33:30.132165 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m16:33:30.133870 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m16:33:30.134140 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m16:33:30.134408 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m16:33:30.136504 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m16:33:30.136759 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m16:33:30.138234 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m16:33:30.138474 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:30.138711 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m16:33:30.138965 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:30.139186 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m16:33:30.139505 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:30.139841 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:31.395355 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fb99-11a6-bb44-f1245ccd3f28) - Created
[0m16:33:31.401780 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fb9a-1dff-86ee-9d70d97b9a77) - Created
[0m16:33:31.405628 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fb98-1696-a576-62e2c28cc32c) - Created
[0m16:33:31.407499 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fb9e-1405-91e9-e652520e10ab) - Created
[0m16:33:32.187407 [debug] [ThreadPool]: SQL status: OK in 2.050 seconds
[0m16:33:32.188000 [debug] [ThreadPool]: SQL status: OK in 2.050 seconds
[0m16:33:32.188290 [debug] [ThreadPool]: SQL status: OK in 2.050 seconds
[0m16:33:32.189235 [debug] [ThreadPool]: SQL status: OK in 2.050 seconds
[0m16:33:32.212733 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd19-fb9e-1405-91e9-e652520e10ab, command-id=01f0dd19-fbc8-19ba-9be8-4f4c656bbcd2) - Closing
[0m16:33:32.213591 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd19-fb99-11a6-bb44-f1245ccd3f28, command-id=01f0dd19-fbc8-1d33-a954-06e1c7140f07) - Closing
[0m16:33:32.214321 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd19-fb9a-1dff-86ee-9d70d97b9a77, command-id=01f0dd19-fbc8-1bfd-98fd-885962442080) - Closing
[0m16:33:32.215201 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m16:33:32.215812 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd19-fb98-1696-a576-62e2c28cc32c, command-id=01f0dd19-fbc8-1eeb-a2dd-557b727be730) - Closing
[0m16:33:32.216188 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fb9e-1405-91e9-e652520e10ab) - Closing
[0m16:33:32.389582 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m16:33:32.390662 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fb99-11a6-bb44-f1245ccd3f28) - Closing
[0m16:33:32.578290 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m16:33:32.579103 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fb9a-1dff-86ee-9d70d97b9a77) - Closing
[0m16:33:32.761472 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m16:33:32.762286 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fb98-1696-a576-62e2c28cc32c) - Closing
[0m16:33:32.933857 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_raw) - Creating connection
[0m16:33:32.934682 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_marts) - Creating connection
[0m16:33:32.935129 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_raw'
[0m16:33:32.935449 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics) - Creating connection
[0m16:33:32.935661 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_marts'
[0m16:33:32.940557 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_raw"
[0m16:33:32.940959 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_staging) - Creating connection
[0m16:33:32.941539 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics'
[0m16:33:32.943286 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_marts"
[0m16:33:32.943546 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_raw'

  
[0m16:33:32.943729 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_staging'
[0m16:33:32.945029 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics"
[0m16:33:32.945317 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_marts'

  
[0m16:33:32.945546 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:32.947115 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_staging"
[0m16:33:32.947392 [debug] [ThreadPool]: On list_patreon_dev_analytics: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics'

  
[0m16:33:32.947578 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:32.948040 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_staging'

  
[0m16:33:32.948808 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:32.949160 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:33.763670 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fd09-1498-88d9-de26447534de) - Created
[0m16:33:33.767318 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fd09-1393-97c7-273285b8126c) - Created
[0m16:33:33.769992 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fd09-176d-a5b5-c47dc734b905) - Created
[0m16:33:33.773422 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fd09-189c-89db-ed0f7793d62e) - Created
[0m16:33:34.889599 [debug] [ThreadPool]: SQL status: OK in 1.940 seconds
[0m16:33:34.890930 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd19-fd09-176d-a5b5-c47dc734b905, command-id=01f0dd19-fd1f-1fff-bbdf-0691baf83fb1) - Closing
[0m16:33:34.891313 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: Close
[0m16:33:34.891486 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fd09-176d-a5b5-c47dc734b905) - Closing
[0m16:33:34.925393 [debug] [ThreadPool]: SQL status: OK in 1.980 seconds
[0m16:33:34.925926 [debug] [ThreadPool]: SQL status: OK in 1.980 seconds
[0m16:33:34.928336 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd19-fd09-1498-88d9-de26447534de, command-id=01f0dd19-fd1f-13c1-8120-0ee0fc651eee) - Closing
[0m16:33:34.929410 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd19-fd09-1393-97c7-273285b8126c, command-id=01f0dd19-fd1f-1227-ad77-2edd9fdbf32f) - Closing
[0m16:33:34.951723 [debug] [ThreadPool]: SQL status: OK in 2.010 seconds
[0m16:33:34.952996 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd19-fd09-189c-89db-ed0f7793d62e, command-id=01f0dd19-fd1f-1f2a-a627-4abeef5b099f) - Closing
[0m16:33:35.054721 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: Close
[0m16:33:35.055218 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fd09-1498-88d9-de26447534de) - Closing
[0m16:33:35.230607 [debug] [ThreadPool]: On list_patreon_dev_analytics: Close
[0m16:33:35.231308 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fd09-1393-97c7-273285b8126c) - Closing
[0m16:33:35.394928 [debug] [ThreadPool]: On list_patreon_dev_analytics_raw: Close
[0m16:33:35.396103 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd19-fd09-189c-89db-ed0f7793d62e) - Closing
[0m16:33:35.570510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5bab70>]}
[0m16:33:35.574243 [debug] [Thread-3 (]: Began running node model.patreon_analytics.metricflow_time_spine
[0m16:33:35.574501 [debug] [Thread-4 (]: Began running node seed.patreon_analytics.creators
[0m16:33:35.574843 [info ] [Thread-3 (]: 1 of 66 START sql table model analytics.metricflow_time_spine .................. [RUN]
[0m16:33:35.575105 [debug] [Thread-5 (]: Began running node seed.patreon_analytics.engagement_events
[0m16:33:35.575383 [info ] [Thread-4 (]: 2 of 66 START seed file analytics_raw.creators ................................. [RUN]
[0m16:33:35.575637 [debug] [Thread-6 (]: Began running node seed.patreon_analytics.patrons
[0m16:33:35.576034 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.metricflow_time_spine) - Creating connection
[0m16:33:35.576358 [info ] [Thread-5 (]: 3 of 66 START seed file analytics_raw.engagement_events ........................ [RUN]
[0m16:33:35.576665 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.creators) - Creating connection
[0m16:33:35.576890 [info ] [Thread-6 (]: 4 of 66 START seed file analytics_raw.patrons .................................. [RUN]
[0m16:33:35.577279 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.patreon_analytics.metricflow_time_spine'
[0m16:33:35.577507 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.engagement_events) - Creating connection
[0m16:33:35.577675 [debug] [Thread-4 (]: Acquiring new databricks connection 'seed.patreon_analytics.creators'
[0m16:33:35.577883 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.patrons) - Creating connection
[0m16:33:35.578065 [debug] [Thread-3 (]: Began compiling node model.patreon_analytics.metricflow_time_spine
[0m16:33:35.578225 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.patreon_analytics.engagement_events'
[0m16:33:35.578379 [debug] [Thread-4 (]: Began compiling node seed.patreon_analytics.creators
[0m16:33:35.578525 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.patreon_analytics.patrons'
[0m16:33:35.582810 [debug] [Thread-5 (]: Began compiling node seed.patreon_analytics.engagement_events
[0m16:33:35.584238 [debug] [Thread-4 (]: Began executing node seed.patreon_analytics.creators
[0m16:33:35.731670 [debug] [Thread-6 (]: Began compiling node seed.patreon_analytics.patrons
[0m16:33:35.732436 [debug] [Thread-5 (]: Began executing node seed.patreon_analytics.engagement_events
[0m16:33:35.742057 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:33:35.754968 [debug] [Thread-6 (]: Began executing node seed.patreon_analytics.patrons
[0m16:33:35.760474 [debug] [Thread-3 (]: Using databricks connection "model.patreon_analytics.metricflow_time_spine"
[0m16:33:35.762611 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:33:35.762972 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e2c920>]}
[0m16:33:35.764310 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:33:35.764616 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.metricflow_time_spine"} */


        select timestampdiff(day, date_trunc('day', cast('2020-01-01' as timestamp)), date_trunc('day', cast('2030-12-31' as timestamp)))
[0m16:33:35.764847 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11994b5f0>]}
[0m16:33:35.772380 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119821b50>]}
[0m16:33:35.772691 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:33:35.802473 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.creators"
[0m16:33:35.808527 [debug] [Thread-4 (]: On seed.patreon_analytics.creators: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.creators"} */

    create or replace table `patreon_dev`.`analytics_raw`.`creators` (creator_id string ,creator_name string ,email string ,category string ,subcategory string ,country_code string ,currency_code string ,plan_type string ,is_nsfw boolean ,is_verified boolean ,created_at timestamp ,first_pledge_received_at timestamp ,last_post_at timestamp ,status string )
    
    using delta
  
    
    
    
    
    
  
[0m16:33:35.814767 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:33:36.285254 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m16:33:36.291431 [debug] [Thread-6 (]: On seed.patreon_analytics.patrons: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.patrons"} */

    create or replace table `patreon_dev`.`analytics_raw`.`patrons` (patron_id string ,patron_name string ,email string ,country_code string ,created_at timestamp ,first_pledge_at timestamp ,lifetime_spend_usd double ,status string )
    
    using delta
  
    
    
    
    
    
  
[0m16:33:36.298223 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:33:36.529036 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:33:36.529783 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.engagement_events"} */

    create or replace table `patreon_dev`.`analytics_raw`.`engagement_events` (event_id string ,patron_id string ,creator_id string ,post_id string ,event_type string ,event_at timestamp )
    
    using delta
  
    
    
    
    
    
  
[0m16:33:36.530028 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m16:33:36.569302 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd19-feb5-1174-bde1-0847f75b04ac) - Created
[0m16:33:36.668161 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd19-fec4-1e6c-883c-3fc80af8adbd) - Created
[0m16:33:36.965379 [debug] [Thread-3 (]: SQL status: OK in 1.190 seconds
[0m16:33:36.966843 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd19-feb5-1174-bde1-0847f75b04ac, command-id=01f0dd19-feca-1c19-8906-610d74afb694) - Closing
[0m16:33:36.998060 [debug] [Thread-3 (]: Writing injected SQL for node "model.patreon_analytics.metricflow_time_spine"
[0m16:33:37.002144 [debug] [Thread-3 (]: Began executing node model.patreon_analytics.metricflow_time_spine
[0m16:33:37.024317 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m16:33:37.046862 [debug] [Thread-3 (]: Writing runtime sql for node "model.patreon_analytics.metricflow_time_spine"
[0m16:33:37.047680 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd19-fefa-1b3f-bab5-40b7e0f9a1cb) - Created
[0m16:33:37.048708 [debug] [Thread-3 (]: Using databricks connection "model.patreon_analytics.metricflow_time_spine"
[0m16:33:37.049211 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.metricflow_time_spine"} */

  
    
        create or replace table `patreon_dev`.`analytics`.`metricflow_time_spine`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      with days as (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
    
    

    )

    select *
    from unioned
    where generated_number <= 4017
    order by generated_number



),

all_periods as (

    select (
        timestampadd(day, (row_number() over (order by 1) - 1), cast('2020-01-01' as timestamp))
    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(timestampadd(year, -1, d.date_day) as date) as prior_year_date_day,
        cast(timestampadd(day, -364, d.date_day) as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(timestampadd(day, -1, d.date_day) as date) as prior_date_day,
    cast(timestampadd(day, 1, d.date_day) as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    date_part('dayofweek', d.date_day) as day_of_week,
    date_part('dayofweek_iso', d.date_day) as day_of_week_iso,
    date_format(d.date_day, 'EEEE') as day_of_week_name,
    date_format(d.date_day, 'E') as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    dayofyear(d.date_day) as day_of_year,

    cast(date_trunc('week', d.date_day) as date) as week_start_date,
    cast(
        timestampadd(day, -1, timestampadd(week, 1, date_trunc('week', d.date_day)))
        as date) as week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_week_start_date,
    cast(
        timestampadd(day, -1, timestampadd(week, 1, date_trunc('week', d.prior_year_over_year_date_day)))
        as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(timestampadd(day, 6, cast(date_trunc('week', d.date_day) as date)) as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(timestampadd(day, 6, cast(date_trunc('week', d.prior_year_over_year_date_day) as date)) as date) as prior_year_iso_week_end_date,
    cast(date_part('week', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    date_format(d.date_day, 'MMMM')  as month_name,
    date_format(d.date_day, 'MMM')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(month, 1, date_trunc('month', d.date_day)))
        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(month, 1, date_trunc('month', d.prior_year_date_day)))
        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(quarter, 1, date_trunc('quarter', d.date_day)))
        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(year, 1, date_trunc('year', d.date_day)))
        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

select
    date_day as date_day
from days
  
[0m16:33:37.137318 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd19-ff0c-14ba-95c9-079226ba414b) - Created
[0m16:33:39.629738 [debug] [Thread-4 (]: SQL status: OK in 3.810 seconds
[0m16:33:39.630707 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd19-fec4-1e6c-883c-3fc80af8adbd, command-id=01f0dd19-fed9-1c5f-bafa-41bec99081da) - Closing
[0m16:33:39.663257 [debug] [Thread-6 (]: SQL status: OK in 3.370 seconds
[0m16:33:39.670680 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd19-fefa-1b3f-bab5-40b7e0f9a1cb, command-id=01f0dd19-ff13-1d7f-94e9-4b8a5a5d776d) - Closing
[0m16:33:39.974221 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.creators"
[0m16:33:39.980209 [debug] [Thread-4 (]: On seed.patreon_analytics.creators: 
          insert overwrite `patreon_dev`.`analytics_raw`.`creators` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s...
[0m16:33:41.839543 [debug] [Thread-5 (]: SQL status: OK in 5.310 seconds
[0m16:33:41.846772 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd19-ff0c-14ba-95c9-079226ba414b, command-id=01f0dd19-ff21-1511-ade7-06ea450636a9) - Closing
[0m16:33:42.127754 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m16:33:42.138154 [debug] [Thread-6 (]: On seed.patreon_analytics.patrons: 
          insert overwrite `patreon_dev`.`analytics_raw`.`patrons` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%...
[0m16:33:43.118329 [debug] [Thread-4 (]: SQL status: OK in 3.130 seconds
[0m16:33:43.125054 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd19-fec4-1e6c-883c-3fc80af8adbd, command-id=01f0dd1a-00e9-1097-91dd-22bba7c6d5d1) - Closing
[0m16:33:43.134976 [debug] [Thread-4 (]: Writing runtime SQL for node "seed.patreon_analytics.creators"
[0m16:33:43.193240 [debug] [Thread-4 (]: On seed.patreon_analytics.creators: Close
[0m16:33:43.199938 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd19-fec4-1e6c-883c-3fc80af8adbd) - Closing
[0m16:33:43.444387 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044f07d0>]}
[0m16:33:43.451097 [info ] [Thread-4 (]: 2 of 66 OK loaded seed file analytics_raw.creators ............................. [[32mINSERT 500[0m in 7.81s]
[0m16:33:43.451906 [debug] [Thread-4 (]: Finished running node seed.patreon_analytics.creators
[0m16:33:43.458556 [debug] [Thread-4 (]: Began running node seed.patreon_analytics.pledges
[0m16:33:43.459019 [info ] [Thread-4 (]: 5 of 66 START seed file analytics_raw.pledges .................................. [RUN]
[0m16:33:43.459409 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.pledges) - Creating connection
[0m16:33:43.472281 [debug] [Thread-4 (]: Acquiring new databricks connection 'seed.patreon_analytics.pledges'
[0m16:33:43.479052 [debug] [Thread-4 (]: Began compiling node seed.patreon_analytics.pledges
[0m16:33:43.485896 [debug] [Thread-4 (]: Began executing node seed.patreon_analytics.pledges
[0m16:33:43.727461 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:33:43.733747 [debug] [Thread-4 (]: On seed.patreon_analytics.pledges: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.pledges"} */

    create or replace table `patreon_dev`.`analytics_raw`.`pledges` (pledge_id string ,patron_id string ,creator_id string ,tier_id string ,pledge_amount_usd decimal(10,2) ,pledge_status string ,is_first_pledge boolean ,started_at timestamp ,ended_at timestamp ,pause_started_at bigint ,churn_reason string )
    
    using delta
  
    
    
    
    
    
  
[0m16:33:43.740429 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:33:43.838600 [debug] [Thread-3 (]: SQL status: OK in 6.790 seconds
[0m16:33:43.845688 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd19-feb5-1174-bde1-0847f75b04ac, command-id=01f0dd19-ff14-13db-aec5-4d6c55887c76) - Closing
[0m16:33:43.987473 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:33:43.990876 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: 
          insert overwrite `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:33:44.112901 [debug] [Thread-3 (]: Applying tags to relation None
[0m16:33:44.122737 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: Close
[0m16:33:44.123002 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd19-feb5-1174-bde1-0847f75b04ac) - Closing
[0m16:33:44.276125 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1194c04d0>]}
[0m16:33:44.276623 [info ] [Thread-3 (]: 1 of 66 OK created sql table model analytics.metricflow_time_spine ............. [[32mOK[0m in 8.70s]
[0m16:33:44.276933 [debug] [Thread-3 (]: Finished running node model.patreon_analytics.metricflow_time_spine
[0m16:33:44.277158 [debug] [Thread-3 (]: Began running node seed.patreon_analytics.posts
[0m16:33:44.277387 [info ] [Thread-3 (]: 6 of 66 START seed file analytics_raw.posts .................................... [RUN]
[0m16:33:44.277816 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.posts) - Creating connection
[0m16:33:44.278050 [debug] [Thread-3 (]: Acquiring new databricks connection 'seed.patreon_analytics.posts'
[0m16:33:44.278241 [debug] [Thread-3 (]: Began compiling node seed.patreon_analytics.posts
[0m16:33:44.278420 [debug] [Thread-3 (]: Began executing node seed.patreon_analytics.posts
[0m16:33:44.482478 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.posts"
[0m16:33:44.483092 [debug] [Thread-3 (]: On seed.patreon_analytics.posts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.posts"} */

    create or replace table `patreon_dev`.`analytics_raw`.`posts` (post_id string ,creator_id string ,title string ,post_type string ,access_level string ,minimum_tier_id string ,published_at timestamp ,is_pinned boolean )
    
    using delta
  
    
    
    
    
    
  
[0m16:33:44.483448 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:33:44.634296 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-0382-1286-aac7-2f077bd7cc48) - Created
[0m16:33:45.000268 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd1a-03bc-1806-bea7-ce54fe97455b) - Created
[0m16:33:46.744818 [debug] [Thread-6 (]: SQL status: OK in 4.600 seconds
[0m16:33:46.745721 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd19-fefa-1b3f-bab5-40b7e0f9a1cb, command-id=01f0dd1a-0280-1f29-b974-27763afc4fa8) - Closing
[0m16:33:46.877171 [debug] [Thread-4 (]: SQL status: OK in 3.140 seconds
[0m16:33:46.886007 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0382-1286-aac7-2f077bd7cc48, command-id=01f0dd1a-0399-1a5c-b153-47dfd94cd3e4) - Closing
[0m16:33:47.033265 [debug] [Thread-3 (]: SQL status: OK in 2.550 seconds
[0m16:33:47.046213 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-03bc-1806-bea7-ce54fe97455b, command-id=01f0dd1a-03d1-1224-a69a-f2c5557ac569) - Closing
[0m16:33:47.588247 [debug] [Thread-5 (]: SQL status: OK in 3.600 seconds
[0m16:33:47.593715 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd19-ff0c-14ba-95c9-079226ba414b, command-id=01f0dd1a-0398-1f53-8bfd-606a6cf0b5a4) - Closing
[0m16:33:51.414290 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m16:33:51.430067 [debug] [Thread-6 (]: On seed.patreon_analytics.patrons: 
          insert into `patreon_dev`.`analytics_raw`.`patrons` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,...
[0m16:33:54.043436 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.posts"
[0m16:33:54.059416 [debug] [Thread-3 (]: On seed.patreon_analytics.posts: 
          insert overwrite `patreon_dev`.`analytics_raw`.`posts` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,...
[0m16:33:54.176508 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:33:54.177721 [debug] [Thread-6 (]: SQL status: OK in 2.740 seconds
[0m16:33:54.193428 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m16:33:54.220809 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd19-fefa-1b3f-bab5-40b7e0f9a1cb, command-id=01f0dd1a-07cd-186a-80bf-a7351ac3802e) - Closing
[0m16:33:54.293810 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.patreon_analytics.patrons"
[0m16:33:54.312193 [debug] [Thread-6 (]: On seed.patreon_analytics.patrons: Close
[0m16:33:54.355043 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd19-fefa-1b3f-bab5-40b7e0f9a1cb) - Closing
[0m16:33:54.633982 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fb813a0>]}
[0m16:33:54.676696 [info ] [Thread-6 (]: 4 of 66 OK loaded seed file analytics_raw.patrons .............................. [[32mINSERT 15000[0m in 19.06s]
[0m16:33:54.689327 [debug] [Thread-6 (]: Finished running node seed.patreon_analytics.patrons
[0m16:33:54.704183 [debug] [Thread-6 (]: Began running node seed.patreon_analytics.tiers
[0m16:33:54.711766 [info ] [Thread-6 (]: 7 of 66 START seed file analytics_raw.tiers .................................... [RUN]
[0m16:33:54.737590 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.tiers) - Creating connection
[0m16:33:54.738323 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.patreon_analytics.tiers'
[0m16:33:54.744935 [debug] [Thread-6 (]: Began compiling node seed.patreon_analytics.tiers
[0m16:33:54.751726 [debug] [Thread-6 (]: Began executing node seed.patreon_analytics.tiers
[0m16:33:54.866635 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.tiers"
[0m16:33:54.873365 [debug] [Thread-6 (]: On seed.patreon_analytics.tiers: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.tiers"} */

    create or replace table `patreon_dev`.`analytics_raw`.`tiers` (tier_id string ,creator_id string ,tier_name string ,tier_rank bigint ,price_usd decimal(10,2) ,description string ,is_active boolean ,created_at timestamp ,archived_at bigint )
    
    using delta
  
    
    
    
    
    
  
[0m16:33:54.879106 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:33:55.678386 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-0a14-1ceb-b131-49a1e4e3e02c) - Created
[0m16:33:55.826941 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:33:55.832895 [debug] [Thread-4 (]: On seed.patreon_analytics.pledges: 
          insert overwrite `patreon_dev`.`analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:33:57.220735 [debug] [Thread-5 (]: SQL status: OK in 2.980 seconds
[0m16:33:57.221452 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd19-ff0c-14ba-95c9-079226ba414b, command-id=01f0dd1a-0981-1d7a-9ce6-8034b944e2ee) - Closing
[0m16:33:57.422701 [debug] [Thread-6 (]: SQL status: OK in 2.540 seconds
[0m16:33:57.429948 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0a14-1ceb-b131-49a1e4e3e02c, command-id=01f0dd1a-0a30-1cb2-9ba1-3ac4e4bc7a95) - Closing
[0m16:33:57.794442 [debug] [Thread-3 (]: SQL status: OK in 3.730 seconds
[0m16:33:57.801065 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-03bc-1806-bea7-ce54fe97455b, command-id=01f0dd1a-09a6-18aa-8e12-8fd16dadea78) - Closing
[0m16:33:57.802913 [debug] [Thread-3 (]: Writing runtime SQL for node "seed.patreon_analytics.posts"
[0m16:33:57.860521 [debug] [Thread-3 (]: On seed.patreon_analytics.posts: Close
[0m16:33:57.873446 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd1a-03bc-1806-bea7-ce54fe97455b) - Closing
[0m16:33:58.082653 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.tiers"
[0m16:33:58.083119 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b7f6ea0>]}
[0m16:33:58.095868 [debug] [Thread-6 (]: On seed.patreon_analytics.tiers: 
          insert overwrite `patreon_dev`.`analytics_raw`.`tiers` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%...
[0m16:33:58.114215 [info ] [Thread-3 (]: 6 of 66 OK loaded seed file analytics_raw.posts ................................ [[32mINSERT 8000[0m in 13.81s]
[0m16:33:58.135629 [debug] [Thread-3 (]: Finished running node seed.patreon_analytics.posts
[0m16:33:58.136026 [debug] [Thread-3 (]: Began running node seed.patreon_analytics.transactions
[0m16:33:58.136560 [info ] [Thread-3 (]: 8 of 66 START seed file analytics_raw.transactions ............................. [RUN]
[0m16:33:58.143368 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.transactions) - Creating connection
[0m16:33:58.156333 [debug] [Thread-3 (]: Acquiring new databricks connection 'seed.patreon_analytics.transactions'
[0m16:33:58.156575 [debug] [Thread-3 (]: Began compiling node seed.patreon_analytics.transactions
[0m16:33:58.156867 [debug] [Thread-3 (]: Began executing node seed.patreon_analytics.transactions
[0m16:33:59.609032 [debug] [Thread-4 (]: SQL status: OK in 3.780 seconds
[0m16:33:59.615075 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0382-1286-aac7-2f077bd7cc48, command-id=01f0dd1a-0aa4-1c44-bb70-e22e88f6a1bd) - Closing
[0m16:34:00.100711 [debug] [Thread-6 (]: SQL status: OK in 1.990 seconds
[0m16:34:00.106778 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0a14-1ceb-b131-49a1e4e3e02c, command-id=01f0dd1a-0bc6-18a8-aa1d-f78100d7ccaa) - Closing
[0m16:34:00.137841 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.patreon_analytics.tiers"
[0m16:34:00.295973 [debug] [Thread-6 (]: On seed.patreon_analytics.tiers: Close
[0m16:34:00.307688 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-0a14-1ceb-b131-49a1e4e3e02c) - Closing
[0m16:34:00.785797 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b72bf20>]}
[0m16:34:00.786275 [info ] [Thread-6 (]: 7 of 66 OK loaded seed file analytics_raw.tiers ................................ [[32mINSERT 1403[0m in 6.05s]
[0m16:34:00.786614 [debug] [Thread-6 (]: Finished running node seed.patreon_analytics.tiers
[0m16:34:00.798454 [debug] [Thread-6 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5
[0m16:34:00.805361 [info ] [Thread-6 (]: 9 of 66 START test source_not_null_raw_patreon_creators_creator_id ............. [RUN]
[0m16:34:00.817659 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5) - Creating connection
[0m16:34:00.829772 [debug] [Thread-6 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5'
[0m16:34:00.841136 [debug] [Thread-6 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5
[0m16:34:00.911278 [debug] [Thread-6 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5"
[0m16:34:00.925933 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:34:00.945311 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.transactions"} */

    create or replace table `patreon_dev`.`analytics_raw`.`transactions` (transaction_id string ,pledge_id string ,patron_id string ,creator_id string ,transaction_type string ,transaction_status string ,gross_amount_usd decimal(10,2) ,platform_fee_usd decimal(10,2) ,processing_fee_usd decimal(10,2) ,net_amount_usd decimal(10,2) ,payment_method string ,failure_reason string ,transaction_at timestamp )
    
    using delta
  
    
    
    
    
    
  
[0m16:34:00.952058 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:34:01.039125 [debug] [Thread-6 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5
[0m16:34:01.092605 [debug] [Thread-6 (]: Writing runtime sql for node "test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5"
[0m16:34:01.177260 [debug] [Thread-6 (]: Using databricks connection "test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5"
[0m16:34:01.190317 [debug] [Thread-6 (]: On test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select creator_id
from `patreon_dev`.`raw`.`creators`
where creator_id is null



  
  
      
    ) dbt_internal_test
[0m16:34:01.202627 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:34:01.691403 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae) - Created
[0m16:34:01.876979 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:34:01.886537 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m16:34:02.123169 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-0dec-1ec9-9088-02274a675587) - Created
[0m16:34:02.751985 [debug] [Thread-6 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select creator_id
from `patreon_dev`.`raw`.`creators`
where creator_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-0e08-1e39-9970-29d83cddba9d
[0m16:34:02.759373 [debug] [Thread-6 (]: On test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5: Close
[0m16:34:02.765986 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-0dec-1ec9-9088-02274a675587) - Closing
[0m16:34:03.470368 [debug] [Thread-6 (]: Database Error in test source_not_null_raw_patreon_creators_creator_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_creators_creator_id.sql
[0m16:34:03.477205 [error] [Thread-6 (]: 9 of 66 ERROR source_not_null_raw_patreon_creators_creator_id .................. [[31mERROR[0m in 2.66s]
[0m16:34:03.483975 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5
[0m16:34:03.490216 [debug] [Thread-6 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711
[0m16:34:03.490516 [info ] [Thread-6 (]: 10 of 66 START test source_not_null_raw_patreon_engagement_events_event_id ..... [RUN]
[0m16:34:03.490863 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_not_null_raw_patreon_creators_creator_id.973eda54d5' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_patreon_creators_creator_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_creators_creator_id.sql.
[0m16:34:03.491260 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711) - Creating connection
[0m16:34:03.523986 [debug] [Thread-6 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711'
[0m16:34:03.524428 [debug] [Thread-6 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711
[0m16:34:03.527893 [debug] [Thread-6 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711"
[0m16:34:03.540814 [debug] [Thread-3 (]: SQL status: OK in 2.590 seconds
[0m16:34:03.547832 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-0dc9-16b5-b31b-9387ed7a9067) - Closing
[0m16:34:03.567034 [debug] [Thread-6 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711
[0m16:34:03.574770 [debug] [Thread-6 (]: Writing runtime sql for node "test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711"
[0m16:34:03.662866 [debug] [Thread-6 (]: Using databricks connection "test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711"
[0m16:34:03.668347 [debug] [Thread-6 (]: On test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_id
from `patreon_dev`.`raw`.`engagement_events`
where event_id is null



  
  
      
    ) dbt_internal_test
[0m16:34:03.681409 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:34:04.377091 [debug] [Thread-5 (]: SQL status: OK in 2.480 seconds
[0m16:34:04.389341 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd19-ff0c-14ba-95c9-079226ba414b, command-id=01f0dd1a-0dfc-19cf-874e-effa599ad34d) - Closing
[0m16:34:04.709894 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-0f77-100d-b75e-8c7144d9880c) - Created
[0m16:34:04.891214 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:34:04.902888 [debug] [Thread-4 (]: On seed.patreon_analytics.pledges: 
          insert into `patreon_dev`.`analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m16:34:05.221226 [debug] [Thread-6 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_id
from `patreon_dev`.`raw`.`engagement_events`
where event_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-0f96-1caf-8bc4-e0592a0c6410
[0m16:34:05.235961 [debug] [Thread-6 (]: On test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711: Close
[0m16:34:05.248062 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-0f77-100d-b75e-8c7144d9880c) - Closing
[0m16:34:05.678409 [debug] [Thread-6 (]: Database Error in test source_not_null_raw_patreon_engagement_events_event_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_engagement_events_event_id.sql
[0m16:34:05.684670 [error] [Thread-6 (]: 10 of 66 ERROR source_not_null_raw_patreon_engagement_events_event_id .......... [[31mERROR[0m in 2.19s]
[0m16:34:05.703320 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711
[0m16:34:05.709442 [debug] [Thread-6 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc
[0m16:34:05.716464 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_not_null_raw_patreon_engagement_events_event_id.2044a41711' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_patreon_engagement_events_event_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_engagement_events_event_id.sql.
[0m16:34:05.716077 [info ] [Thread-6 (]: 11 of 66 START test source_not_null_raw_patreon_patrons_patron_id .............. [RUN]
[0m16:34:05.759017 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc) - Creating connection
[0m16:34:05.771895 [debug] [Thread-6 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc'
[0m16:34:05.783114 [debug] [Thread-6 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc
[0m16:34:05.797560 [debug] [Thread-6 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc"
[0m16:34:05.853714 [debug] [Thread-6 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc
[0m16:34:05.868047 [debug] [Thread-6 (]: Writing runtime sql for node "test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc"
[0m16:34:05.924928 [debug] [Thread-6 (]: Using databricks connection "test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc"
[0m16:34:05.937871 [debug] [Thread-6 (]: On test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select patron_id
from `patreon_dev`.`raw`.`patrons`
where patron_id is null



  
  
      
    ) dbt_internal_test
[0m16:34:05.944500 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:34:06.671108 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-10a3-1eeb-b165-e66f4ecac6b9) - Created
[0m16:34:07.208538 [debug] [Thread-6 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select patron_id
from `patreon_dev`.`raw`.`patrons`
where patron_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-10bf-1c62-841f-4b8f5f60726e
[0m16:34:07.215441 [debug] [Thread-6 (]: On test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc: Close
[0m16:34:07.228341 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-10a3-1eeb-b165-e66f4ecac6b9) - Closing
[0m16:34:07.542269 [debug] [Thread-6 (]: Database Error in test source_not_null_raw_patreon_patrons_patron_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_patrons_patron_id.sql
[0m16:34:07.555320 [error] [Thread-6 (]: 11 of 66 ERROR source_not_null_raw_patreon_patrons_patron_id ................... [[31mERROR[0m in 1.80s]
[0m16:34:07.562112 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc
[0m16:34:07.568245 [debug] [Thread-6 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7
[0m16:34:07.568491 [info ] [Thread-6 (]: 12 of 66 START test source_not_null_raw_patreon_pledges_pledge_id .............. [RUN]
[0m16:34:07.568807 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7) - Creating connection
[0m16:34:07.569002 [debug] [Thread-6 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7'
[0m16:34:07.569173 [debug] [Thread-6 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7
[0m16:34:07.571979 [debug] [Thread-6 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7"
[0m16:34:07.573222 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_not_null_raw_patreon_patrons_patron_id.2e93a6c0cc' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_patreon_patrons_patron_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_patrons_patron_id.sql.
[0m16:34:07.585627 [debug] [Thread-4 (]: SQL status: OK in 2.680 seconds
[0m16:34:07.599243 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0382-1286-aac7-2f077bd7cc48, command-id=01f0dd1a-0fd9-1466-b0df-652ad11e7ab6) - Closing
[0m16:34:07.660475 [debug] [Thread-6 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7
[0m16:34:07.681083 [debug] [Thread-6 (]: Writing runtime sql for node "test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7"
[0m16:34:07.814000 [debug] [Thread-6 (]: Using databricks connection "test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7"
[0m16:34:07.820701 [debug] [Thread-6 (]: On test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pledge_id
from `patreon_dev`.`raw`.`pledges`
where pledge_id is null



  
  
      
    ) dbt_internal_test
[0m16:34:07.846168 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:34:07.888106 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:34:07.894966 [debug] [Thread-4 (]: On seed.patreon_analytics.pledges: 
          insert into `patreon_dev`.`analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m16:34:08.320450 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:34:08.348844 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m16:34:08.762294 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-11df-12d3-bc76-66abb6dba874) - Created
[0m16:34:09.302560 [debug] [Thread-6 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select pledge_id
from `patreon_dev`.`raw`.`pledges`
where pledge_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-11fc-17b8-911f-f22c50a59d27
[0m16:34:09.309573 [debug] [Thread-6 (]: On test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7: Close
[0m16:34:09.316219 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-11df-12d3-bc76-66abb6dba874) - Closing
[0m16:34:09.451222 [debug] [Thread-4 (]: SQL status: OK in 1.540 seconds
[0m16:34:09.457971 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0382-1286-aac7-2f077bd7cc48, command-id=01f0dd1a-1180-1f87-bc32-e53389a7e0bd) - Closing
[0m16:34:09.465976 [debug] [Thread-4 (]: Writing runtime SQL for node "seed.patreon_analytics.pledges"
[0m16:34:09.514663 [debug] [Thread-4 (]: On seed.patreon_analytics.pledges: Close
[0m16:34:09.521291 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-0382-1286-aac7-2f077bd7cc48) - Closing
[0m16:34:09.605677 [debug] [Thread-6 (]: Database Error in test source_not_null_raw_patreon_pledges_pledge_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_pledges_pledge_id.sql
[0m16:34:09.723633 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd35730>]}
[0m16:34:09.715394 [error] [Thread-6 (]: 12 of 66 ERROR source_not_null_raw_patreon_pledges_pledge_id ................... [[31mERROR[0m in 2.15s]
[0m16:34:09.747477 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7
[0m16:34:09.730008 [info ] [Thread-4 (]: 5 of 66 OK loaded seed file analytics_raw.pledges .............................. [[32mINSERT 20302[0m in 26.26s]
[0m16:34:09.754623 [debug] [Thread-6 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f
[0m16:34:09.766122 [debug] [Thread-4 (]: Finished running node seed.patreon_analytics.pledges
[0m16:34:09.766693 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_not_null_raw_patreon_pledges_pledge_id.4811b752a7' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_patreon_pledges_pledge_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_pledges_pledge_id.sql.
[0m16:34:09.790411 [info ] [Thread-6 (]: 13 of 66 START test source_not_null_raw_patreon_posts_post_id .................. [RUN]
[0m16:34:09.790856 [debug] [Thread-4 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb
[0m16:34:09.825631 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f) - Creating connection
[0m16:34:09.849090 [info ] [Thread-4 (]: 14 of 66 START test source_not_null_raw_patreon_tiers_tier_id .................. [RUN]
[0m16:34:09.861807 [debug] [Thread-6 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f'
[0m16:34:09.879836 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb) - Creating connection
[0m16:34:09.897235 [debug] [Thread-6 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f
[0m16:34:09.913860 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb'
[0m16:34:09.936691 [debug] [Thread-6 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f"
[0m16:34:09.951861 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb
[0m16:34:09.952120 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:34:09.955001 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb"
[0m16:34:09.961161 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert overwrite `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m16:34:09.974357 [debug] [Thread-6 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f
[0m16:34:09.983451 [debug] [Thread-6 (]: Writing runtime sql for node "test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f"
[0m16:34:09.990210 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb
[0m16:34:10.004195 [debug] [Thread-4 (]: Writing runtime sql for node "test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb"
[0m16:34:10.036565 [debug] [Thread-6 (]: Using databricks connection "test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f"
[0m16:34:10.036928 [debug] [Thread-6 (]: On test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select post_id
from `patreon_dev`.`raw`.`posts`
where post_id is null



  
  
      
    ) dbt_internal_test
[0m16:34:10.037149 [debug] [Thread-4 (]: Using databricks connection "test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb"
[0m16:34:10.037566 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:34:10.037790 [debug] [Thread-4 (]: On test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select tier_id
from `patreon_dev`.`raw`.`tiers`
where tier_id is null



  
  
      
    ) dbt_internal_test
[0m16:34:10.038307 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:34:10.675597 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-1309-1caa-8a8e-128b58fedfc1) - Created
[0m16:34:10.715433 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-130b-167e-a9fb-4a01f7269174) - Created
[0m16:34:10.853353 [debug] [Thread-5 (]: SQL status: OK in 2.500 seconds
[0m16:34:10.853739 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd19-ff0c-14ba-95c9-079226ba414b, command-id=01f0dd1a-11df-1f55-b2b6-beadfd6c6e98) - Closing
[0m16:34:11.232062 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select tier_id
from `patreon_dev`.`raw`.`tiers`
where tier_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-1325-1448-973b-83437d0b3d45
[0m16:34:11.239732 [debug] [Thread-6 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select post_id
from `patreon_dev`.`raw`.`posts`
where post_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-131e-146f-8682-0a4a5305b158
[0m16:34:11.251433 [debug] [Thread-4 (]: On test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb: Close
[0m16:34:11.282824 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-130b-167e-a9fb-4a01f7269174) - Closing
[0m16:34:11.561995 [debug] [Thread-6 (]: On test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f: Close
[0m16:34:11.562357 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-1309-1caa-8a8e-128b58fedfc1) - Closing
[0m16:34:11.607492 [debug] [Thread-4 (]: Database Error in test source_not_null_raw_patreon_tiers_tier_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_tiers_tier_id.sql
[0m16:34:11.740808 [error] [Thread-4 (]: 14 of 66 ERROR source_not_null_raw_patreon_tiers_tier_id ....................... [[31mERROR[0m in 1.86s]
[0m16:34:11.741110 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:34:11.741460 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb
[0m16:34:11.743415 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%...
[0m16:34:11.743741 [debug] [Thread-4 (]: Began running node test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d
[0m16:34:11.744037 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_not_null_raw_patreon_tiers_tier_id.be9bad69eb' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_patreon_tiers_tier_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_tiers_tier_id.sql.
[0m16:34:11.757842 [debug] [Thread-6 (]: Database Error in test source_not_null_raw_patreon_posts_post_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_posts_post_id.sql
[0m16:34:11.758151 [info ] [Thread-4 (]: 15 of 66 START test source_not_null_raw_patreon_transactions_transaction_id .... [RUN]
[0m16:34:11.771044 [error] [Thread-6 (]: 13 of 66 ERROR source_not_null_raw_patreon_posts_post_id ....................... [[31mERROR[0m in 1.95s]
[0m16:34:11.771474 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d) - Creating connection
[0m16:34:11.772110 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f
[0m16:34:11.772322 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d'
[0m16:34:11.772653 [debug] [Thread-6 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38
[0m16:34:11.772920 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_not_null_raw_patreon_posts_post_id.61a25c7b8f' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_patreon_posts_post_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_posts_post_id.sql.
[0m16:34:11.773110 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d
[0m16:34:11.773312 [info ] [Thread-6 (]: 16 of 66 START test source_unique_raw_patreon_creators_creator_id .............. [RUN]
[0m16:34:11.776556 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d"
[0m16:34:11.776977 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38) - Creating connection
[0m16:34:11.777294 [debug] [Thread-6 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38'
[0m16:34:11.777476 [debug] [Thread-6 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38
[0m16:34:11.781291 [debug] [Thread-6 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38"
[0m16:34:11.781705 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d
[0m16:34:11.783335 [debug] [Thread-4 (]: Writing runtime sql for node "test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d"
[0m16:34:11.783614 [debug] [Thread-6 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38
[0m16:34:11.785182 [debug] [Thread-6 (]: Writing runtime sql for node "test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38"
[0m16:34:11.785532 [debug] [Thread-4 (]: Using databricks connection "test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d"
[0m16:34:11.786715 [debug] [Thread-4 (]: On test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select transaction_id
from `patreon_dev`.`raw`.`transactions`
where transaction_id is null



  
  
      
    ) dbt_internal_test
[0m16:34:11.786913 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:34:11.787241 [debug] [Thread-6 (]: Using databricks connection "test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38"
[0m16:34:11.787458 [debug] [Thread-6 (]: On test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    creator_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`creators`
where creator_id is not null
group by creator_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:34:11.787703 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:34:12.424581 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-140d-1d4d-b7b0-b073f562003f) - Created
[0m16:34:12.428761 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-1410-126e-9853-05ebb26e35f5) - Created
[0m16:34:12.952540 [debug] [Thread-6 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    creator_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`creators`
where creator_id is not null
group by creator_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-1429-1bdf-af4e-9b19ed9136da
[0m16:34:12.953614 [debug] [Thread-6 (]: On test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38: Close
[0m16:34:12.954052 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-140d-1d4d-b7b0-b073f562003f) - Closing
[0m16:34:12.960972 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select transaction_id
from `patreon_dev`.`raw`.`transactions`
where transaction_id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-1429-1f93-86c8-301ab8b09c03
[0m16:34:13.113917 [debug] [Thread-4 (]: On test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d: Close
[0m16:34:13.114781 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-1410-126e-9853-05ebb26e35f5) - Closing
[0m16:34:13.117892 [debug] [Thread-6 (]: Database Error in test source_unique_raw_patreon_creators_creator_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_creators_creator_id.sql
[0m16:34:13.276709 [debug] [Thread-4 (]: Database Error in test source_not_null_raw_patreon_transactions_transaction_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_transactions_transaction_id.sql
[0m16:34:13.275794 [error] [Thread-6 (]: 16 of 66 ERROR source_unique_raw_patreon_creators_creator_id ................... [[31mERROR[0m in 1.50s]
[0m16:34:13.277099 [error] [Thread-4 (]: 15 of 66 ERROR source_not_null_raw_patreon_transactions_transaction_id ......... [[31mERROR[0m in 1.51s]
[0m16:34:13.277426 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38
[0m16:34:13.277679 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d
[0m16:34:13.277880 [debug] [Thread-6 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7
[0m16:34:13.278129 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_unique_raw_patreon_creators_creator_id.021e36ab38' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_raw_patreon_creators_creator_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_creators_creator_id.sql.
[0m16:34:13.278352 [debug] [Thread-4 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094
[0m16:34:13.278900 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_not_null_raw_patreon_transactions_transaction_id.9159be8f9d' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_patreon_transactions_transaction_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_transactions_transaction_id.sql.
[0m16:34:13.278550 [info ] [Thread-6 (]: 17 of 66 START test source_unique_raw_patreon_engagement_events_event_id ....... [RUN]
[0m16:34:13.279316 [info ] [Thread-4 (]: 18 of 66 START test source_unique_raw_patreon_patrons_patron_id ................ [RUN]
[0m16:34:13.279697 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7) - Creating connection
[0m16:34:13.279960 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094) - Creating connection
[0m16:34:13.280149 [debug] [Thread-6 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7'
[0m16:34:13.280319 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094'
[0m16:34:13.280486 [debug] [Thread-6 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7
[0m16:34:13.280648 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094
[0m16:34:13.283502 [debug] [Thread-6 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7"
[0m16:34:13.285934 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094"
[0m16:34:13.286463 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094
[0m16:34:13.286677 [debug] [Thread-6 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7
[0m16:34:13.288308 [debug] [Thread-4 (]: Writing runtime sql for node "test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094"
[0m16:34:13.289863 [debug] [Thread-6 (]: Writing runtime sql for node "test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7"
[0m16:34:13.290266 [debug] [Thread-4 (]: Using databricks connection "test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094"
[0m16:34:13.290495 [debug] [Thread-4 (]: On test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    patron_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`patrons`
where patron_id is not null
group by patron_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:34:13.290671 [debug] [Thread-6 (]: Using databricks connection "test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7"
[0m16:34:13.290844 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:34:13.291055 [debug] [Thread-6 (]: On test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    event_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`engagement_events`
where event_id is not null
group by event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:34:13.291297 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:34:13.877713 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-14f0-1e28-b074-b5288f844c75) - Created
[0m16:34:13.885212 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-14f0-1934-a26a-e86baf067562) - Created
[0m16:34:14.003045 [debug] [Thread-5 (]: SQL status: OK in 2.260 seconds
[0m16:34:14.004145 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd19-ff0c-14ba-95c9-079226ba414b, command-id=01f0dd1a-13c7-1be3-a832-63d190c23959) - Closing
[0m16:34:14.007690 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.patreon_analytics.engagement_events"
[0m16:34:14.012103 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: Close
[0m16:34:14.012615 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd19-ff0c-14ba-95c9-079226ba414b) - Closing
[0m16:34:14.167088 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119908e30>]}
[0m16:34:14.168004 [info ] [Thread-5 (]: 3 of 66 OK loaded seed file analytics_raw.engagement_events .................... [[32mINSERT 45541[0m in 38.59s]
[0m16:34:14.168843 [debug] [Thread-5 (]: Finished running node seed.patreon_analytics.engagement_events
[0m16:34:14.169398 [debug] [Thread-5 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4
[0m16:34:14.169914 [info ] [Thread-5 (]: 19 of 66 START test source_unique_raw_patreon_pledges_pledge_id ................ [RUN]
[0m16:34:14.170438 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4) - Creating connection
[0m16:34:14.170721 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4'
[0m16:34:14.171141 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4
[0m16:34:14.175569 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4"
[0m16:34:14.178130 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4
[0m16:34:14.184549 [debug] [Thread-5 (]: Writing runtime sql for node "test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4"
[0m16:34:14.185168 [debug] [Thread-5 (]: Using databricks connection "test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4"
[0m16:34:14.185450 [debug] [Thread-5 (]: On test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pledge_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`pledges`
where pledge_id is not null
group by pledge_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:34:14.185680 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m16:34:14.371066 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    patron_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`patrons`
where patron_id is not null
group by patron_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-1507-1850-9eee-fce7aeb793ad
[0m16:34:14.371659 [debug] [Thread-4 (]: On test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094: Close
[0m16:34:14.371851 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-14f0-1e28-b074-b5288f844c75) - Closing
[0m16:34:14.391609 [debug] [Thread-6 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    event_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`engagement_events`
where event_id is not null
group by event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-1508-1d52-ac9c-51d5ce0a059f
[0m16:34:14.550666 [debug] [Thread-6 (]: On test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7: Close
[0m16:34:14.550985 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-14f0-1934-a26a-e86baf067562) - Closing
[0m16:34:14.552377 [debug] [Thread-4 (]: Database Error in test source_unique_raw_patreon_patrons_patron_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_patrons_patron_id.sql
[0m16:34:14.715854 [error] [Thread-4 (]: 18 of 66 ERROR source_unique_raw_patreon_patrons_patron_id ..................... [[31mERROR[0m in 1.44s]
[0m16:34:14.716342 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094
[0m16:34:14.716581 [debug] [Thread-4 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33
[0m16:34:14.716771 [info ] [Thread-4 (]: 20 of 66 START test source_unique_raw_patreon_posts_post_id .................... [RUN]
[0m16:34:14.717071 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33) - Creating connection
[0m16:34:14.717351 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_unique_raw_patreon_patrons_patron_id.fce6643094' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_raw_patreon_patrons_patron_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_patrons_patron_id.sql.
[0m16:34:14.717575 [debug] [Thread-4 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33'
[0m16:34:14.717968 [debug] [Thread-4 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33
[0m16:34:14.720922 [debug] [Thread-4 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33"
[0m16:34:14.722196 [debug] [Thread-6 (]: Database Error in test source_unique_raw_patreon_engagement_events_event_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_engagement_events_event_id.sql
[0m16:34:14.722558 [error] [Thread-6 (]: 17 of 66 ERROR source_unique_raw_patreon_engagement_events_event_id ............ [[31mERROR[0m in 1.44s]
[0m16:34:14.723019 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7
[0m16:34:14.723257 [debug] [Thread-6 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958
[0m16:34:14.723448 [info ] [Thread-6 (]: 21 of 66 START test source_unique_raw_patreon_tiers_tier_id .................... [RUN]
[0m16:34:14.723707 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_unique_raw_patreon_engagement_events_event_id.0f270374d7' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_raw_patreon_engagement_events_event_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_engagement_events_event_id.sql.
[0m16:34:14.724017 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958) - Creating connection
[0m16:34:14.724208 [debug] [Thread-4 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33
[0m16:34:14.724516 [debug] [Thread-6 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958'
[0m16:34:14.726442 [debug] [Thread-4 (]: Writing runtime sql for node "test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33"
[0m16:34:14.726712 [debug] [Thread-6 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958
[0m16:34:14.729308 [debug] [Thread-6 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958"
[0m16:34:14.729804 [debug] [Thread-6 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958
[0m16:34:14.731418 [debug] [Thread-6 (]: Writing runtime sql for node "test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958"
[0m16:34:14.731706 [debug] [Thread-4 (]: Using databricks connection "test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33"
[0m16:34:14.732038 [debug] [Thread-4 (]: On test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    post_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`posts`
where post_id is not null
group by post_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:34:14.732251 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:34:14.732882 [debug] [Thread-6 (]: Using databricks connection "test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958"
[0m16:34:14.733151 [debug] [Thread-6 (]: On test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    tier_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`tiers`
where tier_id is not null
group by tier_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:34:14.733334 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:34:14.765944 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1a-157a-1bda-92b3-417abb20996e) - Created
[0m16:34:15.263460 [debug] [Thread-5 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    pledge_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`pledges`
where pledge_id is not null
group by pledge_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-158f-1984-b8fa-2b0531f7a6b5
[0m16:34:15.264133 [debug] [Thread-5 (]: On test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4: Close
[0m16:34:15.264333 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1a-157a-1bda-92b3-417abb20996e) - Closing
[0m16:34:15.265517 [debug] [Thread-3 (]: SQL status: OK in 5.300 seconds
[0m16:34:15.265774 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-130f-1bb4-9e16-2ad926e2100f) - Closing
[0m16:34:15.309448 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-15cb-12bb-803f-7f0faee9107c) - Created
[0m16:34:15.322804 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-15cb-1531-91e7-249a923338b9) - Created
[0m16:34:15.490380 [debug] [Thread-5 (]: Database Error in test source_unique_raw_patreon_pledges_pledge_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_pledges_pledge_id.sql
[0m16:34:15.497250 [error] [Thread-5 (]: 19 of 66 ERROR source_unique_raw_patreon_pledges_pledge_id ..................... [[31mERROR[0m in 1.33s]
[0m16:34:15.510484 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4
[0m16:34:15.516568 [debug] [Thread-5 (]: Began running node test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6
[0m16:34:15.522453 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_unique_raw_patreon_pledges_pledge_id.22ed9168b4' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_raw_patreon_pledges_pledge_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_pledges_pledge_id.sql.
[0m16:34:15.522069 [info ] [Thread-5 (]: 22 of 66 START test source_unique_raw_patreon_transactions_transaction_id ...... [RUN]
[0m16:34:15.559840 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6) - Creating connection
[0m16:34:15.565983 [debug] [Thread-5 (]: Acquiring new databricks connection 'test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6'
[0m16:34:15.566179 [debug] [Thread-5 (]: Began compiling node test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6
[0m16:34:15.568866 [debug] [Thread-5 (]: Writing injected SQL for node "test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6"
[0m16:34:15.607109 [debug] [Thread-5 (]: Began executing node test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6
[0m16:34:15.620929 [debug] [Thread-5 (]: Writing runtime sql for node "test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6"
[0m16:34:15.669634 [debug] [Thread-5 (]: Using databricks connection "test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6"
[0m16:34:15.676692 [debug] [Thread-5 (]: On test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    transaction_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`transactions`
where transaction_id is not null
group by transaction_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:34:15.677556 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m16:34:15.812474 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    post_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`posts`
where post_id is not null
group by post_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-15e5-1e31-8308-21fe55a99bcb
[0m16:34:15.813624 [debug] [Thread-6 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    tier_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`tiers`
where tier_id is not null
group by tier_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-15e7-13a9-b015-b5256edd260a
[0m16:34:15.814098 [debug] [Thread-4 (]: On test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33: Close
[0m16:34:15.857882 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-15cb-12bb-803f-7f0faee9107c) - Closing
[0m16:34:16.116337 [debug] [Thread-6 (]: On test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958: Close
[0m16:34:16.129109 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-15cb-1531-91e7-249a923338b9) - Closing
[0m16:34:16.180055 [debug] [Thread-4 (]: Database Error in test source_unique_raw_patreon_posts_post_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_posts_post_id.sql
[0m16:34:16.313076 [error] [Thread-4 (]: 20 of 66 ERROR source_unique_raw_patreon_posts_post_id ......................... [[31mERROR[0m in 1.60s]
[0m16:34:16.332541 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33
[0m16:34:16.339181 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_creators
[0m16:34:16.345576 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_unique_raw_patreon_posts_post_id.bee1de4d33' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_raw_patreon_posts_post_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_posts_post_id.sql.
[0m16:34:16.352121 [info ] [Thread-4 (]: 23 of 66 SKIP relation analytics_staging.stg_creators .......................... [[33mSKIP[0m]
[0m16:34:16.375660 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_creators
[0m16:34:16.381894 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_patrons
[0m16:34:16.382867 [debug] [Thread-6 (]: Database Error in test source_unique_raw_patreon_tiers_tier_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_tiers_tier_id.sql
[0m16:34:16.383094 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_creators' to be skipped because of status 'skipped'. 
[0m16:34:16.389875 [info ] [Thread-4 (]: 24 of 66 SKIP relation analytics_staging.stg_patrons ........................... [[33mSKIP[0m]
[0m16:34:16.409199 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1a-166f-1a9a-99a3-9b473b05a379) - Created
[0m16:34:16.420217 [error] [Thread-6 (]: 21 of 66 ERROR source_unique_raw_patreon_tiers_tier_id ......................... [[31mERROR[0m in 1.70s]
[0m16:34:16.432613 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_patrons
[0m16:34:16.474456 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958
[0m16:34:16.493047 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_engagement_events
[0m16:34:16.504504 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_patrons' to be skipped because of status 'skipped'. 
[0m16:34:16.516641 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_pledges
[0m16:34:16.528855 [info ] [Thread-4 (]: 25 of 66 SKIP relation analytics_staging.stg_engagement_events ................. [[33mSKIP[0m]
[0m16:34:16.535763 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_unique_raw_patreon_tiers_tier_id.9d03e86958' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_raw_patreon_tiers_tier_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_tiers_tier_id.sql.
[0m16:34:16.566069 [info ] [Thread-6 (]: 26 of 66 SKIP relation analytics_staging.stg_pledges ........................... [[33mSKIP[0m]
[0m16:34:16.572123 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_engagement_events
[0m16:34:16.603272 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_pledges
[0m16:34:16.622221 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_posts
[0m16:34:16.640536 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_engagement_events' to be skipped because of status 'skipped'. 
[0m16:34:16.640904 [debug] [Thread-6 (]: Began running node test.patreon_analytics.accepted_values_stg_creators_plan_type__lite__pro__premium.3492742776
[0m16:34:16.666169 [info ] [Thread-4 (]: 27 of 66 SKIP relation analytics_staging.stg_posts ............................. [[33mSKIP[0m]
[0m16:34:16.672910 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_pledges' to be skipped because of status 'skipped'. 
[0m16:34:16.691138 [info ] [Thread-6 (]: 28 of 66 SKIP test accepted_values_stg_creators_plan_type__lite__pro__premium .. [[33mSKIP[0m]
[0m16:34:16.697773 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_posts
[0m16:34:16.728514 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.accepted_values_stg_creators_plan_type__lite__pro__premium.3492742776
[0m16:34:16.735198 [debug] [Thread-4 (]: Began running node test.patreon_analytics.accepted_values_stg_creators_status__active__paused__suspended__deleted.7a24db0237
[0m16:34:16.754207 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_posts' to be skipped because of status 'skipped'. 
[0m16:34:16.754444 [debug] [Thread-6 (]: Began running node test.patreon_analytics.not_null_stg_creators_creator_id.a4d196188a
[0m16:34:16.767211 [info ] [Thread-4 (]: 29 of 66 SKIP test accepted_values_stg_creators_status__active__paused__suspended__deleted  [[33mSKIP[0m]
[0m16:34:16.773929 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.accepted_values_stg_creators_plan_type__lite__pro__premium.3492742776' to be skipped because of status 'skipped'. 
[0m16:34:16.786037 [info ] [Thread-6 (]: 30 of 66 SKIP test not_null_stg_creators_creator_id ............................ [[33mSKIP[0m]
[0m16:34:16.786377 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.accepted_values_stg_creators_status__active__paused__suspended__deleted.7a24db0237
[0m16:34:16.823862 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.not_null_stg_creators_creator_id.a4d196188a
[0m16:34:16.835352 [debug] [Thread-4 (]: Began running node test.patreon_analytics.unique_stg_creators_creator_id.78aaa0dba4
[0m16:34:16.847859 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.accepted_values_stg_creators_status__active__paused__suspended__deleted.7a24db0237' to be skipped because of status 'skipped'. 
[0m16:34:16.854706 [debug] [Thread-6 (]: Began running node test.patreon_analytics.accepted_values_stg_patrons_patron_value_tier__whale__high_value__regular__casual.3f5dae4aa6
[0m16:34:16.879330 [info ] [Thread-4 (]: 31 of 66 SKIP test unique_stg_creators_creator_id .............................. [[33mSKIP[0m]
[0m16:34:16.896494 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_creators_creator_id.a4d196188a' to be skipped because of status 'skipped'. 
[0m16:34:16.903196 [info ] [Thread-6 (]: 32 of 66 SKIP test accepted_values_stg_patrons_patron_value_tier__whale__high_value__regular__casual  [[33mSKIP[0m]
[0m16:34:16.903562 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.unique_stg_creators_creator_id.78aaa0dba4
[0m16:34:16.941442 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.accepted_values_stg_patrons_patron_value_tier__whale__high_value__regular__casual.3f5dae4aa6
[0m16:34:16.960325 [debug] [Thread-4 (]: Began running node test.patreon_analytics.not_null_stg_patrons_patron_id.9a81327dee
[0m16:34:16.973710 [debug] [Thread-5 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    transaction_id as unique_field,
    count(*) as n_records

from `patreon_dev`.`raw`.`transactions`
where transaction_id is not null
group by transaction_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0dd1a-1698-1d77-9eb4-f0b44f02de31
[0m16:34:16.985318 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.unique_stg_creators_creator_id.78aaa0dba4' to be skipped because of status 'skipped'. 
[0m16:34:16.992117 [debug] [Thread-6 (]: Began running node test.patreon_analytics.unique_stg_patrons_patron_id.b6b0671565
[0m16:34:17.011009 [info ] [Thread-4 (]: 33 of 66 SKIP test not_null_stg_patrons_patron_id .............................. [[33mSKIP[0m]
[0m16:34:17.024292 [debug] [Thread-5 (]: On test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6: Close
[0m16:34:17.043105 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.accepted_values_stg_patrons_patron_value_tier__whale__high_value__regular__casual.3f5dae4aa6' to be skipped because of status 'skipped'. 
[0m16:34:17.068328 [info ] [Thread-6 (]: 34 of 66 SKIP test unique_stg_patrons_patron_id ................................ [[33mSKIP[0m]
[0m16:34:17.068644 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.not_null_stg_patrons_patron_id.9a81327dee
[0m16:34:17.080534 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1a-166f-1a9a-99a3-9b473b05a379) - Closing
[0m16:34:17.117594 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.unique_stg_patrons_patron_id.b6b0671565
[0m16:34:17.141950 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_tiers
[0m16:34:17.154462 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_patrons_patron_id.9a81327dee' to be skipped because of status 'skipped'. 
[0m16:34:17.167280 [debug] [Thread-6 (]: Began running node test.patreon_analytics.accepted_values_stg_engagement_events_event_type__view__like__unlike__comment__share.6f1a4c0604
[0m16:34:17.192308 [info ] [Thread-4 (]: 35 of 66 SKIP relation analytics_staging.stg_tiers ............................. [[33mSKIP[0m]
[0m16:34:17.199002 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.unique_stg_patrons_patron_id.b6b0671565' to be skipped because of status 'skipped'. 
[0m16:34:17.217923 [info ] [Thread-6 (]: 36 of 66 SKIP test accepted_values_stg_engagement_events_event_type__view__like__unlike__comment__share  [[33mSKIP[0m]
[0m16:34:17.237121 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_tiers
[0m16:34:17.275081 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.accepted_values_stg_engagement_events_event_type__view__like__unlike__comment__share.6f1a4c0604
[0m16:34:17.293190 [debug] [Thread-4 (]: Began running node test.patreon_analytics.not_null_stg_engagement_events_event_id.a363dfdd90
[0m16:34:17.305165 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_tiers' to be skipped because of status 'skipped'. 
[0m16:34:17.311920 [debug] [Thread-6 (]: Began running node test.patreon_analytics.unique_stg_engagement_events_event_id.04a2222875
[0m16:34:17.361122 [info ] [Thread-4 (]: 37 of 66 SKIP test not_null_stg_engagement_events_event_id ..................... [[33mSKIP[0m]
[0m16:34:17.361597 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.accepted_values_stg_engagement_events_event_type__view__like__unlike__comment__share.6f1a4c0604' to be skipped because of status 'skipped'. 
[0m16:34:17.375416 [debug] [Thread-5 (]: Database Error in test source_unique_raw_patreon_transactions_transaction_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_transactions_transaction_id.sql
[0m16:34:17.382198 [info ] [Thread-6 (]: 38 of 66 SKIP test unique_stg_engagement_events_event_id ....................... [[33mSKIP[0m]
[0m16:34:17.394232 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.not_null_stg_engagement_events_event_id.a363dfdd90
[0m16:34:17.423616 [error] [Thread-5 (]: 22 of 66 ERROR source_unique_raw_patreon_transactions_transaction_id ........... [[31mERROR[0m in 1.86s]
[0m16:34:17.423977 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.unique_stg_engagement_events_event_id.04a2222875
[0m16:34:17.443051 [debug] [Thread-4 (]: Began running node test.patreon_analytics.accepted_values_stg_pledges_pledge_status__active__paused__churned__declined.a38baa5be6
[0m16:34:17.455561 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_engagement_events_event_id.a363dfdd90' to be skipped because of status 'skipped'. 
[0m16:34:17.474339 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6
[0m16:34:17.493100 [debug] [Thread-6 (]: Began running node test.patreon_analytics.not_null_stg_pledges_creator_id.7ac5bc19ef
[0m16:34:17.512096 [info ] [Thread-4 (]: 39 of 66 SKIP test accepted_values_stg_pledges_pledge_status__active__paused__churned__declined  [[33mSKIP[0m]
[0m16:34:17.518879 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.unique_stg_engagement_events_event_id.04a2222875' to be skipped because of status 'skipped'. 
[0m16:34:17.537765 [debug] [Thread-5 (]: Began running node test.patreon_analytics.not_null_stg_pledges_patron_id.45079734da
[0m16:34:17.550373 [info ] [Thread-6 (]: 40 of 66 SKIP test not_null_stg_pledges_creator_id ............................. [[33mSKIP[0m]
[0m16:34:17.569560 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.accepted_values_stg_pledges_pledge_status__active__paused__churned__declined.a38baa5be6
[0m16:34:17.588545 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.source_unique_raw_patreon_transactions_transaction_id.abd5c64ba6' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_raw_patreon_transactions_transaction_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_transactions_transaction_id.sql.
[0m16:34:17.613278 [info ] [Thread-5 (]: 41 of 66 SKIP test not_null_stg_pledges_patron_id .............................. [[33mSKIP[0m]
[0m16:34:17.613561 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.not_null_stg_pledges_creator_id.7ac5bc19ef
[0m16:34:17.631210 [debug] [Thread-4 (]: Began running node test.patreon_analytics.not_null_stg_pledges_pledge_id.c6c8840915
[0m16:34:17.648717 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.accepted_values_stg_pledges_pledge_status__active__paused__churned__declined.a38baa5be6' to be skipped because of status 'skipped'. 
[0m16:34:17.673795 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.not_null_stg_pledges_patron_id.45079734da
[0m16:34:17.686379 [debug] [Thread-6 (]: Began running node test.patreon_analytics.relationships_stg_pledges_creator_id__creator_id__ref_stg_creators_.a296b68aa7
[0m16:34:17.703676 [info ] [Thread-4 (]: 42 of 66 SKIP test not_null_stg_pledges_pledge_id .............................. [[33mSKIP[0m]
[0m16:34:17.710392 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_pledges_creator_id.7ac5bc19ef' to be skipped because of status 'skipped'. 
[0m16:34:17.735522 [debug] [Thread-5 (]: Began running node test.patreon_analytics.relationships_stg_pledges_patron_id__patron_id__ref_stg_patrons_.1aea46a3f2
[0m16:34:17.753349 [info ] [Thread-6 (]: 43 of 66 SKIP test relationships_stg_pledges_creator_id__creator_id__ref_stg_creators_  [[33mSKIP[0m]
[0m16:34:17.765515 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.not_null_stg_pledges_pledge_id.c6c8840915
[0m16:34:17.784104 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_pledges_patron_id.45079734da' to be skipped because of status 'skipped'. 
[0m16:34:17.802085 [info ] [Thread-5 (]: 44 of 66 SKIP test relationships_stg_pledges_patron_id__patron_id__ref_stg_patrons_  [[33mSKIP[0m]
[0m16:34:17.808725 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.relationships_stg_pledges_creator_id__creator_id__ref_stg_creators_.a296b68aa7
[0m16:34:17.825488 [debug] [Thread-4 (]: Began running node test.patreon_analytics.unique_stg_pledges_pledge_id.d71e65eb4c
[0m16:34:17.843693 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_pledges_pledge_id.c6c8840915' to be skipped because of status 'skipped'. 
[0m16:34:17.862570 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.relationships_stg_pledges_patron_id__patron_id__ref_stg_patrons_.1aea46a3f2
[0m16:34:17.881471 [debug] [Thread-6 (]: Began running node test.patreon_analytics.not_null_stg_posts_creator_id.44240e5ecf
[0m16:34:17.900152 [info ] [Thread-4 (]: 45 of 66 SKIP test unique_stg_pledges_pledge_id ................................ [[33mSKIP[0m]
[0m16:34:17.906810 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.relationships_stg_pledges_creator_id__creator_id__ref_stg_creators_.a296b68aa7' to be skipped because of status 'skipped'. 
[0m16:34:17.913461 [debug] [Thread-5 (]: Began running node test.patreon_analytics.not_null_stg_posts_post_id.cd5975e2d9
[0m16:34:17.938829 [info ] [Thread-6 (]: 46 of 66 SKIP test not_null_stg_posts_creator_id ............................... [[33mSKIP[0m]
[0m16:34:17.939179 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.unique_stg_pledges_pledge_id.d71e65eb4c
[0m16:34:17.962652 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.relationships_stg_pledges_patron_id__patron_id__ref_stg_patrons_.1aea46a3f2' to be skipped because of status 'skipped'. 
[0m16:34:17.981231 [info ] [Thread-5 (]: 47 of 66 SKIP test not_null_stg_posts_post_id .................................. [[33mSKIP[0m]
[0m16:34:17.981610 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.not_null_stg_posts_creator_id.44240e5ecf
[0m16:34:18.006234 [debug] [Thread-4 (]: Began running node test.patreon_analytics.relationships_stg_posts_creator_id__creator_id__ref_stg_creators_.ef48439d1e
[0m16:34:18.024424 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.unique_stg_pledges_pledge_id.d71e65eb4c' to be skipped because of status 'skipped'. 
[0m16:34:18.048675 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.not_null_stg_posts_post_id.cd5975e2d9
[0m16:34:18.067520 [debug] [Thread-6 (]: Began running node test.patreon_analytics.unique_stg_posts_post_id.ec6994c39f
[0m16:34:18.086413 [info ] [Thread-4 (]: 48 of 66 SKIP test relationships_stg_posts_creator_id__creator_id__ref_stg_creators_  [[33mSKIP[0m]
[0m16:34:18.086731 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_posts_creator_id.44240e5ecf' to be skipped because of status 'skipped'. 
[0m16:34:18.105308 [debug] [Thread-5 (]: Began running node test.patreon_analytics.not_null_stg_tiers_creator_id.03c2bb8892
[0m16:34:18.123199 [info ] [Thread-6 (]: 49 of 66 SKIP test unique_stg_posts_post_id .................................... [[33mSKIP[0m]
[0m16:34:18.129233 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.relationships_stg_posts_creator_id__creator_id__ref_stg_creators_.ef48439d1e
[0m16:34:18.154375 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_posts_post_id.cd5975e2d9' to be skipped because of status 'skipped'. 
[0m16:34:18.161028 [info ] [Thread-5 (]: 50 of 66 SKIP test not_null_stg_tiers_creator_id ............................... [[33mSKIP[0m]
[0m16:34:18.174040 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.unique_stg_posts_post_id.ec6994c39f
[0m16:34:18.192897 [debug] [Thread-4 (]: Began running node test.patreon_analytics.not_null_stg_tiers_price_usd.41c4bc4c96
[0m16:34:18.217474 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.relationships_stg_posts_creator_id__creator_id__ref_stg_creators_.ef48439d1e' to be skipped because of status 'skipped'. 
[0m16:34:18.217762 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.not_null_stg_tiers_creator_id.03c2bb8892
[0m16:34:18.236132 [debug] [Thread-6 (]: Began running node test.patreon_analytics.not_null_stg_tiers_tier_id.a8da115175
[0m16:34:18.248717 [info ] [Thread-4 (]: 51 of 66 SKIP test not_null_stg_tiers_price_usd ................................ [[33mSKIP[0m]
[0m16:34:18.249060 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.unique_stg_posts_post_id.ec6994c39f' to be skipped because of status 'skipped'. 
[0m16:34:18.261771 [debug] [Thread-5 (]: Began running node test.patreon_analytics.relationships_stg_tiers_creator_id__creator_id__ref_stg_creators_.d7715cdf0a
[0m16:34:18.286330 [info ] [Thread-6 (]: 52 of 66 SKIP test not_null_stg_tiers_tier_id .................................. [[33mSKIP[0m]
[0m16:34:18.292521 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.not_null_stg_tiers_price_usd.41c4bc4c96
[0m16:34:18.311511 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_tiers_creator_id.03c2bb8892' to be skipped because of status 'skipped'. 
[0m16:34:18.323000 [info ] [Thread-5 (]: 53 of 66 SKIP test relationships_stg_tiers_creator_id__creator_id__ref_stg_creators_  [[33mSKIP[0m]
[0m16:34:18.323297 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.not_null_stg_tiers_tier_id.a8da115175
[0m16:34:18.342316 [debug] [Thread-4 (]: Began running node test.patreon_analytics.unique_stg_tiers_tier_id.edaf87f122
[0m16:34:18.361150 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_tiers_price_usd.41c4bc4c96' to be skipped because of status 'skipped'. 
[0m16:34:18.379983 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.relationships_stg_tiers_creator_id__creator_id__ref_stg_creators_.d7715cdf0a
[0m16:34:18.405041 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_transactions
[0m16:34:18.423954 [info ] [Thread-4 (]: 54 of 66 SKIP test unique_stg_tiers_tier_id .................................... [[33mSKIP[0m]
[0m16:34:18.429826 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_tiers_tier_id.a8da115175' to be skipped because of status 'skipped'. 
[0m16:34:18.466855 [info ] [Thread-6 (]: 55 of 66 SKIP relation analytics_staging.stg_transactions ...................... [[33mSKIP[0m]
[0m16:34:18.473549 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.unique_stg_tiers_tier_id.edaf87f122
[0m16:34:18.492435 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.relationships_stg_tiers_creator_id__creator_id__ref_stg_creators_.d7715cdf0a' to be skipped because of status 'skipped'. 
[0m16:34:18.511317 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_transactions
[0m16:34:18.548923 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.unique_stg_tiers_tier_id.edaf87f122' to be skipped because of status 'skipped'. 
[0m16:34:18.592029 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.stg_transactions' to be skipped because of status 'skipped'. 
[0m16:34:18.617748 [debug] [Thread-5 (]: Began running node test.patreon_analytics.accepted_values_stg_transactions_transaction_status__pending__succeeded__failed__refunded.93b1b6f227
[0m16:34:18.624360 [debug] [Thread-4 (]: Began running node test.patreon_analytics.accepted_values_stg_transactions_transaction_type__pledge_payment__refund__chargeback.1ac4aaca05
[0m16:34:18.624610 [info ] [Thread-5 (]: 56 of 66 SKIP test accepted_values_stg_transactions_transaction_status__pending__succeeded__failed__refunded  [[33mSKIP[0m]
[0m16:34:18.624821 [debug] [Thread-6 (]: Began running node test.patreon_analytics.not_null_stg_transactions_pledge_id.176713ed98
[0m16:34:18.631353 [info ] [Thread-4 (]: 57 of 66 SKIP test accepted_values_stg_transactions_transaction_type__pledge_payment__refund__chargeback  [[33mSKIP[0m]
[0m16:34:18.638036 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.accepted_values_stg_transactions_transaction_status__pending__succeeded__failed__refunded.93b1b6f227
[0m16:34:18.650640 [info ] [Thread-6 (]: 58 of 66 SKIP test not_null_stg_transactions_pledge_id ......................... [[33mSKIP[0m]
[0m16:34:18.668535 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.accepted_values_stg_transactions_transaction_type__pledge_payment__refund__chargeback.1ac4aaca05
[0m16:34:18.685799 [debug] [Thread-5 (]: Began running node test.patreon_analytics.not_null_stg_transactions_transaction_id.1d00a8e2e4
[0m16:34:18.697913 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.accepted_values_stg_transactions_transaction_status__pending__succeeded__failed__refunded.93b1b6f227' to be skipped because of status 'skipped'. 
[0m16:34:18.760580 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.accepted_values_stg_transactions_transaction_type__pledge_payment__refund__chargeback.1ac4aaca05' to be skipped because of status 'skipped'. 
[0m16:34:18.728379 [debug] [Thread-4 (]: Began running node test.patreon_analytics.unique_stg_transactions_transaction_id.ee9a78a396
[0m16:34:18.746926 [info ] [Thread-5 (]: 59 of 66 SKIP test not_null_stg_transactions_transaction_id .................... [[33mSKIP[0m]
[0m16:34:18.710064 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.not_null_stg_transactions_pledge_id.176713ed98
[0m16:34:18.781012 [info ] [Thread-4 (]: 60 of 66 SKIP test unique_stg_transactions_transaction_id ...................... [[33mSKIP[0m]
[0m16:34:18.781427 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.not_null_stg_transactions_transaction_id.1d00a8e2e4
[0m16:34:18.796398 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_transactions_pledge_id.176713ed98' to be skipped because of status 'skipped'. 
[0m16:34:18.803170 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.unique_stg_transactions_transaction_id.ee9a78a396
[0m16:34:18.846052 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_stg_transactions_transaction_id.1d00a8e2e4' to be skipped because of status 'skipped'. 
[0m16:34:18.874813 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:34:18.875261 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.unique_stg_transactions_transaction_id.ee9a78a396' to be skipped because of status 'skipped'. 
[0m16:34:18.881209 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:34:18.887894 [debug] [Thread-6 (]: Began running node model.patreon_analytics.fct_creator_monthly_performance
[0m16:34:18.888107 [info ] [Thread-6 (]: 61 of 66 SKIP relation analytics_marts.fct_creator_monthly_performance ......... [[33mSKIP[0m]
[0m16:34:18.888320 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.fct_creator_monthly_performance
[0m16:34:18.888555 [debug] [Thread-9 (]: Marking all children of 'model.patreon_analytics.fct_creator_monthly_performance' to be skipped because of status 'skipped'. 
[0m16:34:18.906995 [debug] [Thread-5 (]: Began running node test.patreon_analytics.accepted_values_fct_creator_monthly_performance_plan_type__lite__pro__premium.1487829c6d
[0m16:34:18.907410 [debug] [Thread-4 (]: Began running node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_id.4643f34720
[0m16:34:18.920388 [debug] [Thread-6 (]: Began running node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_month_key.1c715c9d68
[0m16:34:18.920070 [info ] [Thread-5 (]: 62 of 66 SKIP test accepted_values_fct_creator_monthly_performance_plan_type__lite__pro__premium  [[33mSKIP[0m]
[0m16:34:18.936428 [info ] [Thread-4 (]: 63 of 66 SKIP test not_null_fct_creator_monthly_performance_creator_id ......... [[33mSKIP[0m]
[0m16:34:18.938114 [info ] [Thread-6 (]: 64 of 66 SKIP test not_null_fct_creator_monthly_performance_creator_month_key .. [[33mSKIP[0m]
[0m16:34:18.938377 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.accepted_values_fct_creator_monthly_performance_plan_type__lite__pro__premium.1487829c6d
[0m16:34:18.938696 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_id.4643f34720
[0m16:34:18.939048 [debug] [Thread-6 (]: Finished running node test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_month_key.1c715c9d68
[0m16:34:18.939327 [debug] [Thread-5 (]: Began running node test.patreon_analytics.not_null_fct_creator_monthly_performance_month_start_date.ed79aa56db
[0m16:34:18.939531 [debug] [Thread-4 (]: Began running node test.patreon_analytics.unique_fct_creator_monthly_performance_creator_month_key.e5e4345d05
[0m16:34:18.939688 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.accepted_values_fct_creator_monthly_performance_plan_type__lite__pro__premium.1487829c6d' to be skipped because of status 'skipped'. 
[0m16:34:18.939930 [info ] [Thread-5 (]: 65 of 66 SKIP test not_null_fct_creator_monthly_performance_month_start_date ... [[33mSKIP[0m]
[0m16:34:18.940170 [info ] [Thread-4 (]: 66 of 66 SKIP test unique_fct_creator_monthly_performance_creator_month_key .... [[33mSKIP[0m]
[0m16:34:18.940454 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_id.4643f34720' to be skipped because of status 'skipped'. 
[0m16:34:18.940726 [debug] [Thread-5 (]: Finished running node test.patreon_analytics.not_null_fct_creator_monthly_performance_month_start_date.ed79aa56db
[0m16:34:18.940928 [debug] [Thread-4 (]: Finished running node test.patreon_analytics.unique_fct_creator_monthly_performance_creator_month_key.e5e4345d05
[0m16:34:18.941098 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_fct_creator_monthly_performance_creator_month_key.1c715c9d68' to be skipped because of status 'skipped'. 
[0m16:34:18.941333 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.not_null_fct_creator_monthly_performance_month_start_date.ed79aa56db' to be skipped because of status 'skipped'. 
[0m16:34:18.941582 [debug] [Thread-9 (]: Marking all children of 'test.patreon_analytics.unique_fct_creator_monthly_performance_creator_month_key.e5e4345d05' to be skipped because of status 'skipped'. 
[0m16:34:21.864315 [debug] [Thread-3 (]: SQL status: OK in 2.980 seconds
[0m16:34:21.865479 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-181e-150a-a54b-09c47f7f5ffc) - Closing
[0m16:34:25.533967 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:34:25.540120 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:34:28.762542 [debug] [Thread-3 (]: SQL status: OK in 3.220 seconds
[0m16:34:28.763073 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-1c0b-182e-95eb-a0501dce542e) - Closing
[0m16:34:32.862858 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:34:32.870789 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:34:35.818429 [debug] [Thread-3 (]: SQL status: OK in 2.940 seconds
[0m16:34:35.820009 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-2069-1eae-a0ff-45c99a9e527a) - Closing
[0m16:34:42.432266 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:34:42.441287 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:34:45.359302 [debug] [Thread-3 (]: SQL status: OK in 2.920 seconds
[0m16:34:45.359745 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-261e-123b-b3d9-cd715204b6a0) - Closing
[0m16:34:49.328313 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:34:49.334919 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:34:52.105746 [debug] [Thread-3 (]: SQL status: OK in 2.770 seconds
[0m16:34:52.106582 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-2a3d-1cf6-a956-bc2442ed95bd) - Closing
[0m16:34:58.285360 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:34:58.300140 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:35:01.333140 [debug] [Thread-3 (]: SQL status: OK in 3.030 seconds
[0m16:35:01.334934 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-2fad-1119-950a-65c31a6fb0fe) - Closing
[0m16:35:05.089413 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:35:05.096196 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:35:09.045527 [debug] [Thread-3 (]: SQL status: OK in 3.950 seconds
[0m16:35:09.046141 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-339a-1b88-a221-0f27cb08bee6) - Closing
[0m16:35:12.614390 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:35:12.621643 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:35:16.193879 [debug] [Thread-3 (]: SQL status: OK in 3.570 seconds
[0m16:35:16.194284 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-3820-187d-a409-200f5091216c) - Closing
[0m16:35:19.902708 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:35:19.909244 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:35:23.157114 [debug] [Thread-3 (]: SQL status: OK in 3.250 seconds
[0m16:35:23.158396 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-3c6e-1e41-89f2-3fadb062a697) - Closing
[0m16:35:27.439464 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:35:27.447560 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:35:31.132852 [debug] [Thread-3 (]: SQL status: OK in 3.680 seconds
[0m16:35:31.133867 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-40f3-1a6e-bcc6-4b7357632100) - Closing
[0m16:35:35.550375 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:35:35.557063 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:35:39.019368 [debug] [Thread-3 (]: SQL status: OK in 3.460 seconds
[0m16:35:39.020762 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-45c4-1e79-a121-6ec91fecbe25) - Closing
[0m16:35:44.672286 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:35:44.681148 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:35:48.249813 [debug] [Thread-3 (]: SQL status: OK in 3.570 seconds
[0m16:35:48.250622 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-4b36-1e60-894c-7d6cf6fd983d) - Closing
[0m16:35:55.494024 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:35:55.501787 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:35:59.178725 [debug] [Thread-3 (]: SQL status: OK in 3.680 seconds
[0m16:35:59.179656 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-51b8-165f-a429-4cace3307fa2) - Closing
[0m16:36:01.913126 [debug] [Thread-3 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:36:01.919022 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%...
[0m16:36:04.853145 [debug] [Thread-3 (]: SQL status: OK in 2.930 seconds
[0m16:36:04.854821 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae, command-id=01f0dd1a-557e-1bb0-9978-fad7c56db308) - Closing
[0m16:36:04.870539 [debug] [Thread-3 (]: Writing runtime SQL for node "seed.patreon_analytics.transactions"
[0m16:36:04.889427 [debug] [Thread-3 (]: On seed.patreon_analytics.transactions: Close
[0m16:36:04.889958 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd1a-0dad-1339-a90f-8e52f476e6ae) - Closing
[0m16:36:05.074343 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bed0c9e1-2619-4a72-a6a1-858d85c3523d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16d321310>]}
[0m16:36:05.077303 [info ] [Thread-3 (]: 8 of 66 OK loaded seed file analytics_raw.transactions ......................... [[32mINSERT 146464[0m in 126.93s]
[0m16:36:05.077935 [debug] [Thread-3 (]: Finished running node seed.patreon_analytics.transactions
[0m16:36:05.082392 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:36:05.082700 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:36:05.083352 [info ] [MainThread]: 
[0m16:36:05.084413 [info ] [MainThread]: Finished running 7 seeds, 2 table models, 50 data tests, 7 view models in 0 hours 2 minutes and 34.97 seconds (154.97s).
[0m16:36:05.091248 [debug] [MainThread]: Command end result
[0m16:36:05.145672 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m16:36:05.150856 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m16:36:05.161430 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m16:36:05.161796 [info ] [MainThread]: 
[0m16:36:05.162520 [info ] [MainThread]: [31mCompleted with 14 errors, 0 partial successes, and 0 warnings:[0m
[0m16:36:05.163196 [info ] [MainThread]: 
[0m16:36:05.163525 [error] [MainThread]: [31mFailure in test source_not_null_raw_patreon_creators_creator_id (models/staging/sources.yml)[0m
[0m16:36:05.163805 [error] [MainThread]:   Database Error in test source_not_null_raw_patreon_creators_creator_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_creators_creator_id.sql
[0m16:36:05.164064 [info ] [MainThread]: 
[0m16:36:05.164527 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_creators_creator_id.sql
[0m16:36:05.164740 [info ] [MainThread]: 
[0m16:36:05.165148 [error] [MainThread]: [31mFailure in test source_not_null_raw_patreon_engagement_events_event_id (models/staging/sources.yml)[0m
[0m16:36:05.166004 [error] [MainThread]:   Database Error in test source_not_null_raw_patreon_engagement_events_event_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_engagement_events_event_id.sql
[0m16:36:05.168486 [info ] [MainThread]: 
[0m16:36:05.168912 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_engagement_events_event_id.sql
[0m16:36:05.169128 [info ] [MainThread]: 
[0m16:36:05.172570 [error] [MainThread]: [31mFailure in test source_not_null_raw_patreon_patrons_patron_id (models/staging/sources.yml)[0m
[0m16:36:05.173202 [error] [MainThread]:   Database Error in test source_not_null_raw_patreon_patrons_patron_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_patrons_patron_id.sql
[0m16:36:05.173662 [info ] [MainThread]: 
[0m16:36:05.173961 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_patrons_patron_id.sql
[0m16:36:05.174466 [info ] [MainThread]: 
[0m16:36:05.174812 [error] [MainThread]: [31mFailure in test source_not_null_raw_patreon_pledges_pledge_id (models/staging/sources.yml)[0m
[0m16:36:05.175642 [error] [MainThread]:   Database Error in test source_not_null_raw_patreon_pledges_pledge_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_pledges_pledge_id.sql
[0m16:36:05.176491 [info ] [MainThread]: 
[0m16:36:05.177153 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_pledges_pledge_id.sql
[0m16:36:05.177648 [info ] [MainThread]: 
[0m16:36:05.178039 [error] [MainThread]: [31mFailure in test source_not_null_raw_patreon_tiers_tier_id (models/staging/sources.yml)[0m
[0m16:36:05.178569 [error] [MainThread]:   Database Error in test source_not_null_raw_patreon_tiers_tier_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_tiers_tier_id.sql
[0m16:36:05.179139 [info ] [MainThread]: 
[0m16:36:05.179770 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_tiers_tier_id.sql
[0m16:36:05.180665 [info ] [MainThread]: 
[0m16:36:05.181238 [error] [MainThread]: [31mFailure in test source_not_null_raw_patreon_posts_post_id (models/staging/sources.yml)[0m
[0m16:36:05.181907 [error] [MainThread]:   Database Error in test source_not_null_raw_patreon_posts_post_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_posts_post_id.sql
[0m16:36:05.182129 [info ] [MainThread]: 
[0m16:36:05.182307 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_posts_post_id.sql
[0m16:36:05.182456 [info ] [MainThread]: 
[0m16:36:05.182629 [error] [MainThread]: [31mFailure in test source_unique_raw_patreon_creators_creator_id (models/staging/sources.yml)[0m
[0m16:36:05.182926 [error] [MainThread]:   Database Error in test source_unique_raw_patreon_creators_creator_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`creators` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_creators_creator_id.sql
[0m16:36:05.183121 [info ] [MainThread]: 
[0m16:36:05.183298 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_creators_creator_id.sql
[0m16:36:05.183583 [info ] [MainThread]: 
[0m16:36:05.183899 [error] [MainThread]: [31mFailure in test source_not_null_raw_patreon_transactions_transaction_id (models/staging/sources.yml)[0m
[0m16:36:05.184154 [error] [MainThread]:   Database Error in test source_not_null_raw_patreon_transactions_transaction_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_transactions_transaction_id.sql
[0m16:36:05.184348 [info ] [MainThread]: 
[0m16:36:05.184524 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_not_null_raw_patreon_transactions_transaction_id.sql
[0m16:36:05.184671 [info ] [MainThread]: 
[0m16:36:05.184839 [error] [MainThread]: [31mFailure in test source_unique_raw_patreon_patrons_patron_id (models/staging/sources.yml)[0m
[0m16:36:05.185264 [error] [MainThread]:   Database Error in test source_unique_raw_patreon_patrons_patron_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`patrons` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_patrons_patron_id.sql
[0m16:36:05.185521 [info ] [MainThread]: 
[0m16:36:05.186508 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_patrons_patron_id.sql
[0m16:36:05.187177 [info ] [MainThread]: 
[0m16:36:05.187390 [error] [MainThread]: [31mFailure in test source_unique_raw_patreon_engagement_events_event_id (models/staging/sources.yml)[0m
[0m16:36:05.187595 [error] [MainThread]:   Database Error in test source_unique_raw_patreon_engagement_events_event_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`engagement_events` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_engagement_events_event_id.sql
[0m16:36:05.187753 [info ] [MainThread]: 
[0m16:36:05.187920 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_engagement_events_event_id.sql
[0m16:36:05.188060 [info ] [MainThread]: 
[0m16:36:05.188288 [error] [MainThread]: [31mFailure in test source_unique_raw_patreon_pledges_pledge_id (models/staging/sources.yml)[0m
[0m16:36:05.188469 [error] [MainThread]:   Database Error in test source_unique_raw_patreon_pledges_pledge_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`pledges` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_pledges_pledge_id.sql
[0m16:36:05.188905 [info ] [MainThread]: 
[0m16:36:05.189204 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_pledges_pledge_id.sql
[0m16:36:05.189375 [info ] [MainThread]: 
[0m16:36:05.189565 [error] [MainThread]: [31mFailure in test source_unique_raw_patreon_posts_post_id (models/staging/sources.yml)[0m
[0m16:36:05.190021 [error] [MainThread]:   Database Error in test source_unique_raw_patreon_posts_post_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`posts` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_posts_post_id.sql
[0m16:36:05.190843 [info ] [MainThread]: 
[0m16:36:05.191423 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_posts_post_id.sql
[0m16:36:05.192265 [info ] [MainThread]: 
[0m16:36:05.193570 [error] [MainThread]: [31mFailure in test source_unique_raw_patreon_tiers_tier_id (models/staging/sources.yml)[0m
[0m16:36:05.194018 [error] [MainThread]:   Database Error in test source_unique_raw_patreon_tiers_tier_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`tiers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_tiers_tier_id.sql
[0m16:36:05.194251 [info ] [MainThread]: 
[0m16:36:05.194630 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_tiers_tier_id.sql
[0m16:36:05.194819 [info ] [MainThread]: 
[0m16:36:05.195137 [error] [MainThread]: [31mFailure in test source_unique_raw_patreon_transactions_transaction_id (models/staging/sources.yml)[0m
[0m16:36:05.195433 [error] [MainThread]:   Database Error in test source_unique_raw_patreon_transactions_transaction_id (models/staging/sources.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `patreon_dev`.`raw`.`transactions` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_transactions_transaction_id.sql
[0m16:36:05.195703 [info ] [MainThread]: 
[0m16:36:05.195978 [info ] [MainThread]:   compiled code at target/compiled/patreon_analytics/models/staging/sources.yml/source_unique_raw_patreon_transactions_transaction_id.sql
[0m16:36:05.196202 [info ] [MainThread]: 
[0m16:36:05.196421 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=14 SKIP=44 NO-OP=0 TOTAL=66
[0m16:36:05.202665 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": false, "command_wall_clock_time": 157.54503, "process_in_blocks": "0", "process_kernel_time": 2.896348, "process_mem_max_rss": "550551552", "process_out_blocks": "0", "process_user_time": 81.79695}
[0m16:36:05.203190 [debug] [MainThread]: Command `dbt build` failed at 16:36:05.203122 after 157.55 seconds
[0m16:36:05.203663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b20980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16dad38f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a3ecf0>]}
[0m16:36:05.203910 [debug] [MainThread]: Flushing usage events
[0m16:36:05.628330 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:39:33.054113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c9f0320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ca3b380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ca3acf0>]}


============================== 16:39:33.057592 | 15e42fcc-7f31-4787-adc7-7de569b8d527 ==============================
[0m16:39:33.057592 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:39:33.057982 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'warn_error': 'None', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'printer_width': '80', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'target_path': 'None', 'empty': 'None', 'indirect_selection': 'eager', 'debug': 'False', 'introspect': 'True', 'no_print': 'None', 'version_check': 'True', 'invocation_command': 'dbt seed', 'quiet': 'False', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False'}
[0m16:39:33.569889 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:39:33.570347 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:39:33.570542 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:39:34.374918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f369dc0>]}
[0m16:39:34.407581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c1fdb80>]}
[0m16:39:34.408045 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:39:34.498305 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:39:34.498852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12da84650>]}
[0m16:39:34.519375 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m16:39:34.604383 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m16:39:34.604864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c1b82c0>]}
[0m16:39:34.620694 [warn ] [MainThread]: [[33mWARNING[0m][PropertyMovedToConfigDeprecation]: Deprecated functionality
Found `meta` as a top-level property of `models[0].columns[0]` in file
`models/marts/core/schema.yml`. The `meta` top-level property should be moved
into the `config` of `models[0].columns[0]`.
[0m16:39:34.620994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ca66660>]}
[0m16:39:36.450119 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on 'stg_creators' in
package 'patreon_analytics' (models/staging/schema.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m16:39:36.450459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e3963f0>]}
[0m16:39:36.740623 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.intermediate
- models.patreon_analytics.marts.finance
[0m16:39:36.747478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e618260>]}
[0m16:39:36.837005 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m16:39:36.840239 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m16:39:36.854133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f1221e0>]}
[0m16:39:36.854496 [info ] [MainThread]: Found 9 models, 7 seeds, 50 data tests, 7 sources, 4 metrics, 1126 macros, 1 semantic model
[0m16:39:36.854691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f16ff20>]}
[0m16:39:36.856519 [info ] [MainThread]: 
[0m16:39:36.856838 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:39:36.857037 [info ] [MainThread]: 
[0m16:39:36.857334 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:39:36.857484 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:39:36.860807 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m16:39:36.861084 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m16:39:36.865919 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m16:39:36.866194 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m16:39:36.866344 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:39:37.626138 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d5e8-1ea4-a817-bee4529eacde) - Created
[0m16:39:38.015962 [debug] [ThreadPool]: SQL status: OK in 1.150 seconds
[0m16:39:38.021130 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1a-d5e8-1ea4-a817-bee4529eacde, command-id=01f0dd1a-d5ff-1d0b-a61c-b560656a748c) - Closing
[0m16:39:38.021518 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m16:39:38.021694 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d5e8-1ea4-a817-bee4529eacde) - Closing
[0m16:39:38.178929 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_patreon_dev_analytics_analytics_raw) - Creating connection
[0m16:39:38.179234 [debug] [ThreadPool]: Acquiring new databricks connection 'create_patreon_dev_analytics_analytics_raw'
[0m16:39:38.179484 [debug] [ThreadPool]: Creating schema "database: "patreon_dev"
schema: "analytics_analytics_raw"
"
[0m16:39:38.186846 [debug] [ThreadPool]: Using databricks connection "create_patreon_dev_analytics_analytics_raw"
[0m16:39:38.187402 [debug] [ThreadPool]: On create_patreon_dev_analytics_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "create_patreon_dev_analytics_analytics_raw"} */
create schema if not exists `patreon_dev`.`analytics_analytics_raw`
  
[0m16:39:38.187770 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:39:38.787656 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d69a-107f-a785-b7656ebf5cce) - Created
[0m16:39:39.352594 [debug] [ThreadPool]: SQL status: OK in 1.160 seconds
[0m16:39:39.353253 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1a-d69a-107f-a785-b7656ebf5cce, command-id=01f0dd1a-d6b1-10a4-a6b2-f95715f14f4d) - Closing
[0m16:39:39.353499 [debug] [ThreadPool]: On create_patreon_dev_analytics_analytics_raw: Close
[0m16:39:39.353656 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d69a-107f-a785-b7656ebf5cce) - Closing
[0m16:39:39.578617 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics) - Creating connection
[0m16:39:39.579021 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_staging) - Creating connection
[0m16:39:39.579204 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics'
[0m16:39:39.579404 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_staging'
[0m16:39:39.579603 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_marts) - Creating connection
[0m16:39:39.582685 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics"
[0m16:39:39.583000 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_analytics_raw) - Creating connection
[0m16:39:39.584668 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_staging"
[0m16:39:39.584869 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_marts'
[0m16:39:39.585050 [debug] [ThreadPool]: On list_patreon_dev_analytics: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics'

  
[0m16:39:39.585209 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_analytics_raw'
[0m16:39:39.585375 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_staging'

  
[0m16:39:39.588713 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_marts"
[0m16:39:39.589047 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:39:39.591574 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_analytics_raw"
[0m16:39:39.591871 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:39:39.592210 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_marts'

  
[0m16:39:39.592727 [debug] [ThreadPool]: On list_patreon_dev_analytics_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_analytics_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_analytics_raw'

  
[0m16:39:39.593236 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:39:39.594042 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:39:40.342851 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d788-11da-84b5-85bd21788046) - Created
[0m16:39:40.355077 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d78a-1d1c-91bf-1f82a0a569f1) - Created
[0m16:39:40.369066 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d78c-19b5-aad0-a4411ae96056) - Created
[0m16:39:40.379395 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d78e-1550-a2a5-4268780c98bb) - Created
[0m16:39:40.835635 [debug] [ThreadPool]: SQL status: OK in 1.240 seconds
[0m16:39:40.837161 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1a-d78e-1550-a2a5-4268780c98bb, command-id=01f0dd1a-d7a3-14b1-8d5d-6d37c3ea3346) - Closing
[0m16:39:40.837566 [debug] [ThreadPool]: On list_patreon_dev_analytics_analytics_raw: Close
[0m16:39:40.837735 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d78e-1550-a2a5-4268780c98bb) - Closing
[0m16:39:40.938204 [debug] [ThreadPool]: SQL status: OK in 1.340 seconds
[0m16:39:40.955280 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1a-d78c-19b5-aad0-a4411ae96056, command-id=01f0dd1a-d7a2-1666-9c6f-96a10a9f9bf7) - Closing
[0m16:39:40.959464 [debug] [ThreadPool]: SQL status: OK in 1.370 seconds
[0m16:39:40.960933 [debug] [ThreadPool]: SQL status: OK in 1.370 seconds
[0m16:39:40.962877 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1a-d788-11da-84b5-85bd21788046, command-id=01f0dd1a-d79d-1d3b-8adb-4da450ad5653) - Closing
[0m16:39:40.964552 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1a-d78a-1d1c-91bf-1f82a0a569f1, command-id=01f0dd1a-d7a0-133d-9451-cfd99513f925) - Closing
[0m16:39:41.000077 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: Close
[0m16:39:41.000366 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d78c-19b5-aad0-a4411ae96056) - Closing
[0m16:39:41.160649 [debug] [ThreadPool]: On list_patreon_dev_analytics: Close
[0m16:39:41.161607 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d788-11da-84b5-85bd21788046) - Closing
[0m16:39:41.308925 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: Close
[0m16:39:41.309164 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1a-d78a-1d1c-91bf-1f82a0a569f1) - Closing
[0m16:39:41.462805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12df3a570>]}
[0m16:39:41.465123 [debug] [Thread-4 (]: Began running node seed.patreon_analytics.creators
[0m16:39:41.465340 [debug] [Thread-5 (]: Began running node seed.patreon_analytics.engagement_events
[0m16:39:41.466008 [debug] [Thread-6 (]: Began running node seed.patreon_analytics.patrons
[0m16:39:41.466299 [debug] [Thread-7 (]: Began running node seed.patreon_analytics.pledges
[0m16:39:41.465687 [info ] [Thread-4 (]: 1 of 7 START seed file analytics_analytics_raw.creators ........................ [RUN]
[0m16:39:41.466603 [info ] [Thread-5 (]: 2 of 7 START seed file analytics_analytics_raw.engagement_events ............... [RUN]
[0m16:39:41.466855 [info ] [Thread-6 (]: 3 of 7 START seed file analytics_analytics_raw.patrons ......................... [RUN]
[0m16:39:41.467064 [info ] [Thread-7 (]: 4 of 7 START seed file analytics_analytics_raw.pledges ......................... [RUN]
[0m16:39:41.467354 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.creators) - Creating connection
[0m16:39:41.467568 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.engagement_events) - Creating connection
[0m16:39:41.467767 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.patrons) - Creating connection
[0m16:39:41.468026 [debug] [Thread-7 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.pledges) - Creating connection
[0m16:39:41.468247 [debug] [Thread-4 (]: Acquiring new databricks connection 'seed.patreon_analytics.creators'
[0m16:39:41.468415 [debug] [Thread-5 (]: Acquiring new databricks connection 'seed.patreon_analytics.engagement_events'
[0m16:39:41.468583 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.patreon_analytics.patrons'
[0m16:39:41.468737 [debug] [Thread-7 (]: Acquiring new databricks connection 'seed.patreon_analytics.pledges'
[0m16:39:41.468911 [debug] [Thread-4 (]: Began compiling node seed.patreon_analytics.creators
[0m16:39:41.469072 [debug] [Thread-5 (]: Began compiling node seed.patreon_analytics.engagement_events
[0m16:39:41.469228 [debug] [Thread-6 (]: Began compiling node seed.patreon_analytics.patrons
[0m16:39:41.469382 [debug] [Thread-7 (]: Began compiling node seed.patreon_analytics.pledges
[0m16:39:41.469573 [debug] [Thread-4 (]: Began executing node seed.patreon_analytics.creators
[0m16:39:41.469726 [debug] [Thread-5 (]: Began executing node seed.patreon_analytics.engagement_events
[0m16:39:41.469882 [debug] [Thread-6 (]: Began executing node seed.patreon_analytics.patrons
[0m16:39:41.470033 [debug] [Thread-7 (]: Began executing node seed.patreon_analytics.pledges
[0m16:39:41.473417 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:39:41.475348 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:39:41.477227 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:39:41.479129 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f076e70>]}
[0m16:39:41.479931 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f2c9d00>]}
[0m16:39:41.480917 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f2acf20>]}
[0m16:39:41.867277 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.creators"
[0m16:39:41.904045 [debug] [Thread-4 (]: On seed.patreon_analytics.creators: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.creators"} */

    create  table `patreon_dev`.`analytics_analytics_raw`.`creators` (creator_id string ,creator_name string ,email string ,category string ,subcategory string ,country_code string ,currency_code string ,plan_type string ,is_nsfw boolean ,is_verified boolean ,created_at timestamp ,first_pledge_received_at timestamp ,last_post_at timestamp ,status string )
    
    using delta
  
    
    
    
    
    
  
[0m16:39:41.904450 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:39:41.999880 [debug] [Thread-7 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:39:42.000509 [debug] [Thread-7 (]: On seed.patreon_analytics.pledges: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.pledges"} */

    create  table `patreon_dev`.`analytics_analytics_raw`.`pledges` (pledge_id string ,patron_id string ,creator_id string ,tier_id string ,pledge_amount_usd decimal(10,2) ,pledge_status string ,is_first_pledge boolean ,started_at timestamp ,ended_at timestamp ,pause_started_at bigint ,churn_reason string )
    
    using delta
  
    
    
    
    
    
  
[0m16:39:42.000859 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m16:39:42.275622 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m16:39:42.282260 [debug] [Thread-6 (]: On seed.patreon_analytics.patrons: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.patrons"} */

    create  table `patreon_dev`.`analytics_analytics_raw`.`patrons` (patron_id string ,patron_name string ,email string ,country_code string ,created_at timestamp ,first_pledge_at timestamp ,lifetime_spend_usd double ,status string )
    
    using delta
  
    
    
    
    
    
  
[0m16:39:42.282474 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:39:42.615406 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:39:42.615748 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.engagement_events"} */

    create  table `patreon_dev`.`analytics_analytics_raw`.`engagement_events` (event_id string ,patron_id string ,creator_id string ,post_id string ,event_type string ,event_at timestamp )
    
    using delta
  
    
    
    
    
    
  
[0m16:39:42.615932 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m16:39:42.718889 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-d8ee-1445-8edf-ede619e8f085) - Created
[0m16:39:42.855975 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0dd1a-d908-1eca-90a5-5bfa517505c1) - Created
[0m16:39:42.960328 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-d918-1d8a-b17b-3ca887d11fc1) - Created
[0m16:39:43.242611 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1a-d93a-17eb-af6d-286d70d5e9cd) - Created
[0m16:39:44.997952 [debug] [Thread-7 (]: SQL status: OK in 3.000 seconds
[0m16:39:44.998650 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d908-1eca-90a5-5bfa517505c1, command-id=01f0dd1a-d91d-1bdd-b718-433ffde82a6c) - Closing
[0m16:39:45.069397 [debug] [Thread-6 (]: SQL status: OK in 2.790 seconds
[0m16:39:45.076426 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d918-1d8a-b17b-3ca887d11fc1, command-id=01f0dd1a-d92d-19c5-a26c-189dc53b9f25) - Closing
[0m16:39:45.102963 [debug] [Thread-4 (]: SQL status: OK in 3.200 seconds
[0m16:39:45.109885 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d8ee-1445-8edf-ede619e8f085, command-id=01f0dd1a-d908-187e-8c0e-dec56f9c7e20) - Closing
[0m16:39:45.233581 [debug] [Thread-5 (]: SQL status: OK in 2.620 seconds
[0m16:39:45.240441 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d93a-17eb-af6d-286d70d5e9cd, command-id=01f0dd1a-d958-1961-85cd-2f295de97995) - Closing
[0m16:39:45.743101 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.creators"
[0m16:39:45.750037 [debug] [Thread-4 (]: On seed.patreon_analytics.creators: 
          insert overwrite `patreon_dev`.`analytics_analytics_raw`.`creators` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%...
[0m16:39:47.460791 [debug] [Thread-4 (]: SQL status: OK in 1.710 seconds
[0m16:39:47.483841 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d8ee-1445-8edf-ede619e8f085, command-id=01f0dd1a-daeb-19a1-bb2d-b3544e0d4acb) - Closing
[0m16:39:47.486823 [debug] [Thread-4 (]: Writing runtime SQL for node "seed.patreon_analytics.creators"
[0m16:39:47.573727 [debug] [Thread-4 (]: On seed.patreon_analytics.creators: Close
[0m16:39:47.591941 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-d8ee-1445-8edf-ede619e8f085) - Closing
[0m16:39:47.939746 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ab2d80>]}
[0m16:39:47.970336 [info ] [Thread-4 (]: 1 of 7 OK loaded seed file analytics_analytics_raw.creators .................... [[32mINSERT 500[0m in 6.32s]
[0m16:39:48.000549 [debug] [Thread-4 (]: Finished running node seed.patreon_analytics.creators
[0m16:39:48.018143 [debug] [Thread-4 (]: Began running node seed.patreon_analytics.posts
[0m16:39:48.037489 [info ] [Thread-4 (]: 5 of 7 START seed file analytics_analytics_raw.posts ........................... [RUN]
[0m16:39:48.063110 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.posts) - Creating connection
[0m16:39:48.074859 [debug] [Thread-4 (]: Acquiring new databricks connection 'seed.patreon_analytics.posts'
[0m16:39:48.075058 [debug] [Thread-4 (]: Began compiling node seed.patreon_analytics.posts
[0m16:39:48.087803 [debug] [Thread-4 (]: Began executing node seed.patreon_analytics.posts
[0m16:39:48.486849 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.posts"
[0m16:39:48.506087 [debug] [Thread-4 (]: On seed.patreon_analytics.posts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.posts"} */

    create  table `patreon_dev`.`analytics_analytics_raw`.`posts` (post_id string ,creator_id string ,title string ,post_type string ,access_level string ,minimum_tier_id string ,published_at timestamp ,is_pinned boolean )
    
    using delta
  
    
    
    
    
    
  
[0m16:39:48.506372 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:39:49.321450 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-dcdf-197a-a1d7-99467f563aa1) - Created
[0m16:39:49.958456 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:39:49.966906 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: 
          insert overwrite `patreon_dev`.`analytics_analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:39:51.369482 [debug] [Thread-4 (]: SQL status: OK in 2.860 seconds
[0m16:39:51.375901 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1a-dcdf-197a-a1d7-99467f563aa1, command-id=01f0dd1a-dcfd-1fb8-bcc4-e2d3eeb8266e) - Closing
[0m16:39:51.555151 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m16:39:51.568605 [debug] [Thread-6 (]: On seed.patreon_analytics.patrons: 
          insert overwrite `patreon_dev`.`analytics_analytics_raw`.`patrons` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(...
[0m16:39:52.916968 [debug] [Thread-5 (]: SQL status: OK in 2.940 seconds
[0m16:39:52.928993 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d93a-17eb-af6d-286d70d5e9cd, command-id=01f0dd1a-ddc5-18ce-9fcb-62e3841f7237) - Closing
[0m16:39:52.958587 [debug] [Thread-7 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:39:52.969939 [debug] [Thread-7 (]: On seed.patreon_analytics.pledges: 
          insert overwrite `patreon_dev`.`analytics_analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%...
[0m16:39:55.203687 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.posts"
[0m16:39:55.213397 [debug] [Thread-4 (]: On seed.patreon_analytics.posts: 
          insert overwrite `patreon_dev`.`analytics_analytics_raw`.`posts` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s...
[0m16:39:55.709130 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:39:55.712288 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m16:39:55.823945 [debug] [Thread-6 (]: SQL status: OK in 4.240 seconds
[0m16:39:55.824278 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d918-1d8a-b17b-3ca887d11fc1, command-id=01f0dd1a-dede-141b-881f-6f6004cb28da) - Closing
[0m16:39:56.575340 [debug] [Thread-7 (]: SQL status: OK in 3.590 seconds
[0m16:39:56.582061 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d908-1eca-90a5-5bfa517505c1, command-id=01f0dd1a-df96-1a26-a01c-fcad93ba550b) - Closing
[0m16:39:57.521350 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.patrons"
[0m16:39:57.530144 [debug] [Thread-6 (]: On seed.patreon_analytics.patrons: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`patrons` values
          (%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s...
[0m16:39:58.292138 [debug] [Thread-5 (]: SQL status: OK in 2.580 seconds
[0m16:39:58.305302 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d93a-17eb-af6d-286d70d5e9cd, command-id=01f0dd1a-e0df-1059-959a-5f269908e68f) - Closing
[0m16:39:58.495026 [debug] [Thread-4 (]: SQL status: OK in 3.270 seconds
[0m16:39:58.501749 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1a-dcdf-197a-a1d7-99467f563aa1, command-id=01f0dd1a-e0de-1abb-b427-3dcb7e9cfa02) - Closing
[0m16:39:58.515088 [debug] [Thread-4 (]: Writing runtime SQL for node "seed.patreon_analytics.posts"
[0m16:39:58.584993 [debug] [Thread-4 (]: On seed.patreon_analytics.posts: Close
[0m16:39:58.591169 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-dcdf-197a-a1d7-99467f563aa1) - Closing
[0m16:39:58.783283 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1584c70e0>]}
[0m16:39:58.808925 [info ] [Thread-4 (]: 5 of 7 OK loaded seed file analytics_analytics_raw.posts ....................... [[32mINSERT 8000[0m in 10.72s]
[0m16:39:58.821997 [debug] [Thread-4 (]: Finished running node seed.patreon_analytics.posts
[0m16:39:58.835047 [debug] [Thread-4 (]: Began running node seed.patreon_analytics.tiers
[0m16:39:58.859369 [info ] [Thread-4 (]: 6 of 7 START seed file analytics_analytics_raw.tiers ........................... [RUN]
[0m16:39:58.871822 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.tiers) - Creating connection
[0m16:39:58.884765 [debug] [Thread-4 (]: Acquiring new databricks connection 'seed.patreon_analytics.tiers'
[0m16:39:58.903735 [debug] [Thread-4 (]: Began compiling node seed.patreon_analytics.tiers
[0m16:39:58.909506 [debug] [Thread-4 (]: Began executing node seed.patreon_analytics.tiers
[0m16:39:59.031355 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.tiers"
[0m16:39:59.038180 [debug] [Thread-4 (]: On seed.patreon_analytics.tiers: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.tiers"} */

    create  table `patreon_dev`.`analytics_analytics_raw`.`tiers` (tier_id string ,creator_id string ,tier_name string ,tier_rank bigint ,price_usd decimal(10,2) ,description string ,is_active boolean ,created_at timestamp ,archived_at bigint )
    
    using delta
  
    
    
    
    
    
  
[0m16:39:59.062886 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:39:59.803775 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-e321-176e-a13a-c2b8c066e18f) - Created
[0m16:39:59.890892 [debug] [Thread-6 (]: SQL status: OK in 2.350 seconds
[0m16:39:59.903107 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d918-1d8a-b17b-3ca887d11fc1, command-id=01f0dd1a-e1ee-1a33-908c-9e31fb8867ae) - Closing
[0m16:39:59.910473 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.patreon_analytics.patrons"
[0m16:39:59.985350 [debug] [Thread-6 (]: On seed.patreon_analytics.patrons: Close
[0m16:39:59.998397 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-d918-1d8a-b17b-3ca887d11fc1) - Closing
[0m16:40:00.392209 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15891bce0>]}
[0m16:40:00.430442 [info ] [Thread-6 (]: 3 of 7 OK loaded seed file analytics_analytics_raw.patrons ..................... [[32mINSERT 15000[0m in 18.92s]
[0m16:40:00.461267 [debug] [Thread-6 (]: Finished running node seed.patreon_analytics.patrons
[0m16:40:00.474264 [debug] [Thread-6 (]: Began running node seed.patreon_analytics.transactions
[0m16:40:00.481499 [info ] [Thread-6 (]: 7 of 7 START seed file analytics_analytics_raw.transactions .................... [RUN]
[0m16:40:00.494310 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=seed.patreon_analytics.transactions) - Creating connection
[0m16:40:00.507293 [debug] [Thread-6 (]: Acquiring new databricks connection 'seed.patreon_analytics.transactions'
[0m16:40:00.520266 [debug] [Thread-6 (]: Began compiling node seed.patreon_analytics.transactions
[0m16:40:00.526895 [debug] [Thread-6 (]: Began executing node seed.patreon_analytics.transactions
[0m16:40:01.946079 [debug] [Thread-4 (]: SQL status: OK in 2.880 seconds
[0m16:40:01.952902 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e321-176e-a13a-c2b8c066e18f, command-id=01f0dd1a-e33f-16e0-a6fb-c0d0cc5a4c83) - Closing
[0m16:40:02.763901 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:40:02.801889 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "seed.patreon_analytics.transactions"} */

    create  table `patreon_dev`.`analytics_analytics_raw`.`transactions` (transaction_id string ,pledge_id string ,patron_id string ,creator_id string ,transaction_type string ,transaction_status string ,gross_amount_usd decimal(10,2) ,platform_fee_usd decimal(10,2) ,processing_fee_usd decimal(10,2) ,net_amount_usd decimal(10,2) ,payment_method string ,failure_reason string ,transaction_at timestamp )
    
    using delta
  
    
    
    
    
    
  
[0m16:40:02.814774 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:40:03.152506 [debug] [Thread-4 (]: Using databricks connection "seed.patreon_analytics.tiers"
[0m16:40:03.166076 [debug] [Thread-4 (]: On seed.patreon_analytics.tiers: 
          insert overwrite `patreon_dev`.`analytics_analytics_raw`.`tiers` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,...
[0m16:40:03.204412 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:40:03.213479 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m16:40:03.472138 [debug] [Thread-7 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:40:03.477463 [debug] [Thread-7 (]: On seed.patreon_analytics.pledges: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%...
[0m16:40:03.692919 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-e56f-1782-9389-f67369182454) - Created
[0m16:40:05.075001 [debug] [Thread-4 (]: SQL status: OK in 1.900 seconds
[0m16:40:05.075421 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e321-176e-a13a-c2b8c066e18f, command-id=01f0dd1a-e55f-100f-9704-9de9c1a3e22d) - Closing
[0m16:40:05.079126 [debug] [Thread-4 (]: Writing runtime SQL for node "seed.patreon_analytics.tiers"
[0m16:40:05.081956 [debug] [Thread-4 (]: On seed.patreon_analytics.tiers: Close
[0m16:40:05.082278 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1a-e321-176e-a13a-c2b8c066e18f) - Closing
[0m16:40:05.279512 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1588c11f0>]}
[0m16:40:05.284397 [info ] [Thread-4 (]: 6 of 7 OK loaded seed file analytics_analytics_raw.tiers ....................... [[32mINSERT 1403[0m in 6.41s]
[0m16:40:05.285477 [debug] [Thread-4 (]: Finished running node seed.patreon_analytics.tiers
[0m16:40:05.795659 [debug] [Thread-6 (]: SQL status: OK in 2.980 seconds
[0m16:40:05.796422 [debug] [Thread-5 (]: SQL status: OK in 2.570 seconds
[0m16:40:05.797011 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1a-e588-162a-8aef-40383f5b3d6d) - Closing
[0m16:40:05.797238 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d93a-17eb-af6d-286d70d5e9cd, command-id=01f0dd1a-e55f-1ae9-964f-f9f10cf79f62) - Closing
[0m16:40:06.550337 [debug] [Thread-7 (]: SQL status: OK in 3.070 seconds
[0m16:40:06.556922 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d908-1eca-90a5-5bfa517505c1, command-id=01f0dd1a-e57e-1d1b-91bc-01b3d217992e) - Closing
[0m16:40:06.832707 [debug] [Thread-7 (]: Using databricks connection "seed.patreon_analytics.pledges"
[0m16:40:06.838876 [debug] [Thread-7 (]: On seed.patreon_analytics.pledges: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`pledges` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%...
[0m16:40:08.219676 [debug] [Thread-7 (]: SQL status: OK in 1.370 seconds
[0m16:40:08.226028 [debug] [Thread-7 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d908-1eca-90a5-5bfa517505c1, command-id=01f0dd1a-e76d-1b1f-8a77-f53c657363a2) - Closing
[0m16:40:08.232245 [debug] [Thread-7 (]: Writing runtime SQL for node "seed.patreon_analytics.pledges"
[0m16:40:08.320401 [debug] [Thread-7 (]: On seed.patreon_analytics.pledges: Close
[0m16:40:08.333341 [debug] [Thread-7 (]: Databricks adapter: Connection(session-id=01f0dd1a-d908-1eca-90a5-5bfa517505c1) - Closing
[0m16:40:08.614501 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15f959f10>]}
[0m16:40:08.627515 [info ] [Thread-7 (]: 4 of 7 OK loaded seed file analytics_analytics_raw.pledges ..................... [[32mINSERT 20302[0m in 27.15s]
[0m16:40:08.646115 [debug] [Thread-7 (]: Finished running node seed.patreon_analytics.pledges
[0m16:40:09.185896 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:40:09.195540 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m16:40:11.019060 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:40:11.025425 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert overwrite `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%...
[0m16:40:11.618670 [debug] [Thread-5 (]: SQL status: OK in 2.420 seconds
[0m16:40:11.619086 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d93a-17eb-af6d-286d70d5e9cd, command-id=01f0dd1a-e8e8-1616-afe1-14a8a961d1c7) - Closing
[0m16:40:12.541977 [debug] [Thread-5 (]: Using databricks connection "seed.patreon_analytics.engagement_events"
[0m16:40:12.544077 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`engagement_events` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s...
[0m16:40:14.595553 [debug] [Thread-5 (]: SQL status: OK in 2.050 seconds
[0m16:40:14.595922 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd1a-d93a-17eb-af6d-286d70d5e9cd, command-id=01f0dd1a-ead9-1cfb-834c-688857b82bae) - Closing
[0m16:40:14.597479 [debug] [Thread-5 (]: Writing runtime SQL for node "seed.patreon_analytics.engagement_events"
[0m16:40:14.600379 [debug] [Thread-5 (]: On seed.patreon_analytics.engagement_events: Close
[0m16:40:14.600606 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1a-d93a-17eb-af6d-286d70d5e9cd) - Closing
[0m16:40:14.728332 [debug] [Thread-6 (]: SQL status: OK in 3.700 seconds
[0m16:40:14.728689 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1a-ea49-1f56-ad8c-61c641651683) - Closing
[0m16:40:14.767338 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15de892e0>]}
[0m16:40:14.774177 [info ] [Thread-5 (]: 2 of 7 OK loaded seed file analytics_analytics_raw.engagement_events ........... [[32mINSERT 45541[0m in 33.30s]
[0m16:40:14.780895 [debug] [Thread-5 (]: Finished running node seed.patreon_analytics.engagement_events
[0m16:40:18.254961 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:40:18.262149 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:40:21.531176 [debug] [Thread-6 (]: SQL status: OK in 3.270 seconds
[0m16:40:21.531969 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1a-ee58-1841-80c5-f107be2f52fb) - Closing
[0m16:40:24.980687 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:40:24.987119 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:40:27.325297 [debug] [Thread-6 (]: SQL status: OK in 2.340 seconds
[0m16:40:27.325643 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1a-f248-1af1-80e1-69da3e3f23d6) - Closing
[0m16:40:30.961116 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:40:30.968445 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:40:33.350312 [debug] [Thread-6 (]: SQL status: OK in 2.380 seconds
[0m16:40:33.350642 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1a-f5db-1b87-beaa-51a2154f20ec) - Closing
[0m16:40:37.504459 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:40:37.511858 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:40:39.954202 [debug] [Thread-6 (]: SQL status: OK in 2.440 seconds
[0m16:40:39.954804 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1a-f9bd-126c-9275-81f7d6fe7ab6) - Closing
[0m16:40:43.317720 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:40:43.323942 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:40:45.956767 [debug] [Thread-6 (]: SQL status: OK in 2.540 seconds
[0m16:40:46.009428 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1a-fd3a-1263-8e2e-0c74c018cd0f) - Closing
[0m16:40:50.970450 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:40:50.977328 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:40:53.521546 [debug] [Thread-6 (]: SQL status: OK in 2.540 seconds
[0m16:40:53.524117 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1b-01c9-1795-a150-e2e2e342b28e) - Closing
[0m16:40:57.822761 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:40:57.832131 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:41:01.385112 [debug] [Thread-6 (]: SQL status: OK in 3.550 seconds
[0m16:41:01.386207 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1b-05e0-1b3d-aa51-ca5b4fbaced9) - Closing
[0m16:41:04.849954 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:41:04.856333 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:41:07.973100 [debug] [Thread-6 (]: SQL status: OK in 3.120 seconds
[0m16:41:07.973909 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1b-0a09-139a-bce0-9b56b796b893) - Closing
[0m16:41:11.884440 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:41:11.894549 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:41:15.291519 [debug] [Thread-6 (]: SQL status: OK in 3.390 seconds
[0m16:41:15.292963 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1b-0e44-1201-b743-961f787f5cd9) - Closing
[0m16:41:18.940336 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:41:18.946764 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:41:21.817150 [debug] [Thread-6 (]: SQL status: OK in 2.870 seconds
[0m16:41:21.818858 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1b-126f-1abf-86d0-42ab175f57dc) - Closing
[0m16:41:25.552848 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:41:25.559362 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:41:28.630830 [debug] [Thread-6 (]: SQL status: OK in 3.070 seconds
[0m16:41:28.631581 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1b-1667-1e03-8e18-76988982ee05) - Closing
[0m16:41:32.391878 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:41:32.399451 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:41:35.681060 [debug] [Thread-6 (]: SQL status: OK in 3.280 seconds
[0m16:41:35.682910 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1b-1a80-15b7-8a87-4859dba6fdce) - Closing
[0m16:41:39.212007 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:41:39.218612 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:41:42.284345 [debug] [Thread-6 (]: SQL status: OK in 3.070 seconds
[0m16:41:42.285122 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1b-1e90-14a0-b828-96ba62cf2365) - Closing
[0m16:41:44.781026 [debug] [Thread-6 (]: Using databricks connection "seed.patreon_analytics.transactions"
[0m16:41:44.786434 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: 
          insert into `patreon_dev`.`analytics_analytics_raw`.`transactions` values
          (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s),(%s,%s,...
[0m16:41:47.376679 [debug] [Thread-6 (]: SQL status: OK in 2.590 seconds
[0m16:41:47.377106 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1a-e56f-1782-9389-f67369182454, command-id=01f0dd1b-21db-155e-bbdd-ff59fed28eb9) - Closing
[0m16:41:47.380143 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.patreon_analytics.transactions"
[0m16:41:47.387499 [debug] [Thread-6 (]: On seed.patreon_analytics.transactions: Close
[0m16:41:47.387771 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1a-e56f-1782-9389-f67369182454) - Closing
[0m16:41:47.560320 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15e42fcc-7f31-4787-adc7-7de569b8d527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x177107fb0>]}
[0m16:41:47.561883 [info ] [Thread-6 (]: 7 of 7 OK loaded seed file analytics_analytics_raw.transactions ................ [[32mINSERT 146464[0m in 107.07s]
[0m16:41:47.562398 [debug] [Thread-6 (]: Finished running node seed.patreon_analytics.transactions
[0m16:41:47.564959 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:41:47.565445 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:41:47.565894 [info ] [MainThread]: 
[0m16:41:47.566105 [info ] [MainThread]: Finished running 7 seeds in 0 hours 2 minutes and 10.71 seconds (130.71s).
[0m16:41:47.566954 [debug] [MainThread]: Command end result
[0m16:41:47.614240 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m16:41:47.619019 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m16:41:47.623524 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m16:41:47.623764 [info ] [MainThread]: 
[0m16:41:47.624053 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:41:47.624222 [info ] [MainThread]: 
[0m16:41:47.624432 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=7
[0m16:41:47.625003 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- PropertyMovedToConfigDeprecation: 42 occurrences
- MissingArgumentsPropertyInGenericTestDeprecation: 12 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m16:41:47.629493 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 134.61824, "process_in_blocks": "0", "process_kernel_time": 1.730883, "process_mem_max_rss": "599212032", "process_out_blocks": "0", "process_user_time": 78.03663}
[0m16:41:47.629944 [debug] [MainThread]: Command `dbt seed` succeeded at 16:41:47.629887 after 134.62 seconds
[0m16:41:47.630307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c9db920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f1cb9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10534ef90>]}
[0m16:41:47.630613 [debug] [MainThread]: Flushing usage events
[0m16:41:48.164095 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:42:43.720141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1195b8f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c24950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c24860>]}


============================== 16:42:43.723833 | 35e7f40e-ef28-426a-96a7-96ffa8d671ca ==============================
[0m16:42:43.723833 [info ] [MainThread]: Running with dbt=1.11.0-rc3
[0m16:42:43.724191 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'warn_error': 'None', 'target_path': 'None', 'static_parser': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'write_json': 'True', 'empty': 'False', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'log_cache_events': 'False', 'partial_parse': 'True', 'quiet': 'False', 'version_check': 'True', 'profiles_dir': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project', 'no_print': 'None', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/logs', 'use_colors': 'True', 'invocation_command': 'dbt run', 'printer_width': '80', 'introspect': 'True'}
[0m16:42:44.217292 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:42:44.217720 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:42:44.217938 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:42:44.951501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e252000>]}
[0m16:42:44.982915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128244aa0>]}
[0m16:42:44.983358 [info ] [MainThread]: Registered adapter: databricks=1.11.3
[0m16:42:45.075147 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:42:45.075661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e2c71a0>]}
[0m16:42:45.097419 [debug] [MainThread]: checksum: 0c6350755389a6ed721b17ac809f023d953b5a3e5903ed71b682e7badc0c55b0, vars: {}, profile: , target: , version: 1.11.0rc3
[0m16:42:45.255541 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:42:45.255846 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m16:42:45.255987 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:42:45.260953 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.patreon_analytics.marts.finance
- models.patreon_analytics.intermediate
[0m16:42:45.295641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e458e00>]}
[0m16:42:45.377091 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m16:42:45.380300 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m16:42:45.390983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e83cd70>]}
[0m16:42:45.391305 [info ] [MainThread]: Found 9 models, 7 seeds, 50 data tests, 7 sources, 4 metrics, 1126 macros, 1 semantic model
[0m16:42:45.391492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e633200>]}
[0m16:42:45.393209 [info ] [MainThread]: 
[0m16:42:45.393461 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:42:45.393618 [info ] [MainThread]: 
[0m16:42:45.393963 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:42:45.394171 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:42:45.397641 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m16:42:45.397844 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m16:42:45.405166 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m16:42:45.405577 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m16:42:45.405949 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m16:42:45.406110 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:42:45.406433 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev) - Creating connection
[0m16:42:45.406629 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m16:42:45.406951 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev'
[0m16:42:45.408919 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m16:42:45.410837 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev"
[0m16:42:45.411213 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m16:42:45.411510 [debug] [ThreadPool]: On list_patreon_dev: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev"} */

    

  SHOW SCHEMAS IN `patreon_dev`


  
[0m16:42:45.411811 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:42:45.411975 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:42:46.131228 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4643-1d71-a565-ed9559e48c49) - Created
[0m16:42:46.131547 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4643-1cd5-8934-d9aa06e3f3ce) - Created
[0m16:42:46.132418 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4645-108a-9a1b-6ebc44e27770) - Created
[0m16:42:46.616745 [debug] [ThreadPool]: SQL status: OK in 1.210 seconds
[0m16:42:46.617026 [debug] [ThreadPool]: SQL status: OK in 1.210 seconds
[0m16:42:46.617215 [debug] [ThreadPool]: SQL status: OK in 1.210 seconds
[0m16:42:46.621751 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1b-4643-1d71-a565-ed9559e48c49, command-id=01f0dd1b-4667-1c83-84c2-c97593d92d7b) - Closing
[0m16:42:46.622462 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1b-4645-108a-9a1b-6ebc44e27770, command-id=01f0dd1b-4668-14dc-8644-2759c5523474) - Closing
[0m16:42:46.623055 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1b-4643-1cd5-8934-d9aa06e3f3ce, command-id=01f0dd1b-4668-10d8-ac72-be8030f98292) - Closing
[0m16:42:46.623342 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m16:42:46.623682 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4643-1d71-a565-ed9559e48c49) - Closing
[0m16:42:46.779067 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m16:42:46.779621 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4645-108a-9a1b-6ebc44e27770) - Closing
[0m16:42:46.941785 [debug] [ThreadPool]: On list_patreon_dev: Close
[0m16:42:46.942542 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4643-1cd5-8934-d9aa06e3f3ce) - Closing
[0m16:42:47.101663 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_analytics_raw) - Creating connection
[0m16:42:47.102059 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_staging) - Creating connection
[0m16:42:47.102253 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_analytics_raw'
[0m16:42:47.102532 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics) - Creating connection
[0m16:42:47.102916 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_patreon_dev_analytics_marts) - Creating connection
[0m16:42:47.103096 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_staging'
[0m16:42:47.106370 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_analytics_raw"
[0m16:42:47.106586 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics'
[0m16:42:47.106742 [debug] [ThreadPool]: Acquiring new databricks connection 'list_patreon_dev_analytics_marts'
[0m16:42:47.108531 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_staging"
[0m16:42:47.108810 [debug] [ThreadPool]: On list_patreon_dev_analytics_analytics_raw: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_analytics_raw"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_analytics_raw'

  
[0m16:42:47.110120 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics"
[0m16:42:47.111087 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_staging'

  
[0m16:42:47.111801 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:42:47.112487 [debug] [ThreadPool]: On list_patreon_dev_analytics: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics'

  
[0m16:42:47.115496 [debug] [ThreadPool]: Using databricks connection "list_patreon_dev_analytics_marts"
[0m16:42:47.115736 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:42:47.115984 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:42:47.116246 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "connection_name": "list_patreon_dev_analytics_marts"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'patreon_dev' 
  AND table_schema = 'analytics_marts'

  
[0m16:42:47.117024 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:42:47.944585 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4759-171a-9636-d683296ef7f6) - Created
[0m16:42:47.945295 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4759-1c8b-a1f7-a2f5f10d2595) - Created
[0m16:42:47.945889 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4759-18ef-8fa0-f99162aa59ca) - Created
[0m16:42:47.947091 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4759-1a88-bdbd-c7706b7d7529) - Created
[0m16:42:48.472829 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m16:42:48.476418 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1b-4759-1c8b-a1f7-a2f5f10d2595, command-id=01f0dd1b-476f-19c6-9412-036f4ab63dc6) - Closing
[0m16:42:48.477160 [debug] [ThreadPool]: On list_patreon_dev_analytics_marts: Close
[0m16:42:48.477608 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4759-1c8b-a1f7-a2f5f10d2595) - Closing
[0m16:42:48.478621 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m16:42:48.481113 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1b-4759-1a88-bdbd-c7706b7d7529, command-id=01f0dd1b-476f-1dca-b72c-85d0f055bcc7) - Closing
[0m16:42:48.485584 [debug] [ThreadPool]: SQL status: OK in 1.370 seconds
[0m16:42:48.487247 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1b-4759-18ef-8fa0-f99162aa59ca, command-id=01f0dd1b-476f-1b0c-99cc-e717b22583cf) - Closing
[0m16:42:48.509596 [debug] [ThreadPool]: SQL status: OK in 1.390 seconds
[0m16:42:48.510965 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0dd1b-4759-171a-9636-d683296ef7f6, command-id=01f0dd1b-476f-1658-82e9-cb8ba222de0d) - Closing
[0m16:42:48.634231 [debug] [ThreadPool]: On list_patreon_dev_analytics_staging: Close
[0m16:42:48.634488 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4759-1a88-bdbd-c7706b7d7529) - Closing
[0m16:42:48.847304 [debug] [ThreadPool]: On list_patreon_dev_analytics_analytics_raw: Close
[0m16:42:48.848248 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4759-18ef-8fa0-f99162aa59ca) - Closing
[0m16:42:49.009740 [debug] [ThreadPool]: On list_patreon_dev_analytics: Close
[0m16:42:49.011896 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0dd1b-4759-171a-9636-d683296ef7f6) - Closing
[0m16:42:49.163849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e81a000>]}
[0m16:42:49.165607 [debug] [Thread-3 (]: Began running node model.patreon_analytics.metricflow_time_spine
[0m16:42:49.165825 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_creators
[0m16:42:49.166276 [debug] [Thread-5 (]: Began running node model.patreon_analytics.stg_engagement_events
[0m16:42:49.166450 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_patrons
[0m16:42:49.166126 [info ] [Thread-3 (]: 1 of 9 START sql table model analytics.metricflow_time_spine ................... [RUN]
[0m16:42:49.166719 [info ] [Thread-4 (]: 2 of 9 START sql view model analytics_staging.stg_creators ..................... [RUN]
[0m16:42:49.166957 [info ] [Thread-5 (]: 3 of 9 START sql view model analytics_staging.stg_engagement_events ............ [RUN]
[0m16:42:49.167462 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.metricflow_time_spine) - Creating connection
[0m16:42:49.167153 [info ] [Thread-6 (]: 4 of 9 START sql view model analytics_staging.stg_patrons ...................... [RUN]
[0m16:42:49.167745 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_creators) - Creating connection
[0m16:42:49.167984 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_engagement_events) - Creating connection
[0m16:42:49.168159 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.patreon_analytics.metricflow_time_spine'
[0m16:42:49.168382 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_patrons) - Creating connection
[0m16:42:49.168542 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_creators'
[0m16:42:49.168703 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_engagement_events'
[0m16:42:49.168874 [debug] [Thread-3 (]: Began compiling node model.patreon_analytics.metricflow_time_spine
[0m16:42:49.169039 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_patrons'
[0m16:42:49.169190 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.stg_creators
[0m16:42:49.169343 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.stg_engagement_events
[0m16:42:49.181229 [debug] [Thread-6 (]: Began compiling node model.patreon_analytics.stg_patrons
[0m16:42:49.191497 [debug] [Thread-3 (]: Using databricks connection "model.patreon_analytics.metricflow_time_spine"
[0m16:42:49.193719 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.stg_creators"
[0m16:42:49.195624 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.stg_engagement_events"
[0m16:42:49.197254 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.metricflow_time_spine"} */


        select timestampdiff(day, date_trunc('day', cast('2020-01-01' as timestamp)), date_trunc('day', cast('2030-12-31' as timestamp)))
[0m16:42:49.203037 [debug] [Thread-6 (]: Writing injected SQL for node "model.patreon_analytics.stg_patrons"
[0m16:42:49.203428 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:42:49.204042 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.stg_creators
[0m16:42:49.280565 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.stg_engagement_events
[0m16:42:49.287068 [debug] [Thread-6 (]: Began executing node model.patreon_analytics.stg_patrons
[0m16:42:49.289371 [debug] [Thread-5 (]: MATERIALIZING VIEW
[0m16:42:49.290799 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m16:42:49.292172 [debug] [Thread-6 (]: MATERIALIZING VIEW
[0m16:42:49.293283 [warn ] [Thread-5 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:42:49.293580 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:42:49.293864 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m16:42:49.294119 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12edf60f0>]}
[0m16:42:49.294317 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12edf5a30>]}
[0m16:42:49.294486 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12edf4b00>]}
[0m16:42:49.301786 [debug] [Thread-4 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_creators`
[0m16:42:49.302185 [debug] [Thread-6 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_patrons`
[0m16:42:49.303684 [debug] [Thread-5 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_engagement_events`
[0m16:42:49.309097 [debug] [Thread-4 (]: Writing runtime sql for node "model.patreon_analytics.stg_creators"
[0m16:42:49.309685 [debug] [Thread-6 (]: Writing runtime sql for node "model.patreon_analytics.stg_patrons"
[0m16:42:49.310148 [debug] [Thread-5 (]: Writing runtime sql for node "model.patreon_analytics.stg_engagement_events"
[0m16:42:49.310988 [debug] [Thread-4 (]: Using databricks connection "model.patreon_analytics.stg_creators"
[0m16:42:49.311262 [debug] [Thread-4 (]: On model.patreon_analytics.stg_creators: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_creators"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_creators`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`creators`
),

staged as (
    select
        creator_id,
        creator_name,
        email,
        category,
        subcategory,
        country_code,
        coalesce(currency_code, 'USD') as currency_code,
        coalesce(plan_type, 'pro') as plan_type,
        coalesce(is_nsfw, false) as is_nsfw,
        coalesce(is_verified, false) as is_verified,
        created_at,
        first_pledge_received_at,
        last_post_at,
        coalesce(status, 'active') as status,
        
        -- Derived fields
        datediff(day, created_at, coalesce(first_pledge_received_at, current_timestamp())) as days_to_first_pledge,
        datediff(month, created_at, current_timestamp()) as account_age_months,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m16:42:49.311451 [debug] [Thread-6 (]: Using databricks connection "model.patreon_analytics.stg_patrons"
[0m16:42:49.311626 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:42:49.311770 [debug] [Thread-5 (]: Using databricks connection "model.patreon_analytics.stg_engagement_events"
[0m16:42:49.311970 [debug] [Thread-6 (]: On model.patreon_analytics.stg_patrons: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_patrons"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_patrons`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`patrons`
),

staged as (
    select
        patron_id,
        patron_name,
        email,
        country_code,
        created_at,
        first_pledge_at,
        coalesce(lifetime_spend_usd, 0) as lifetime_spend_usd,
        coalesce(status, 'active') as status,
        
        -- Derived fields
        datediff(month, created_at, current_timestamp()) as account_age_months,
        case 
            when lifetime_spend_usd >= 1000 then 'whale'
            when lifetime_spend_usd >= 500 then 'high_value'
            when lifetime_spend_usd >= 100 then 'regular'
            else 'casual'
        end as patron_value_tier,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m16:42:49.312398 [debug] [Thread-5 (]: On model.patreon_analytics.stg_engagement_events: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_engagement_events"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_engagement_events`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`engagement_events`
),

staged as (
    select
        event_id,
        patron_id,
        creator_id,
        post_id,
        event_type,
        event_at,
        
        -- Derived fields
        date_trunc('month', event_at) as event_month,
        date_trunc('day', event_at) as event_date,
        
        -- Engagement weighting (for composite scores)
        case event_type
            when 'view' then 1
            when 'like' then 3
            when 'comment' then 5
            when 'share' then 7
            else 1
        end as engagement_weight,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m16:42:49.312634 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:42:49.312859 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m16:42:49.898138 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd1b-4880-1898-8976-6108a91123c3) - Created
[0m16:42:50.033966 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1b-4899-12a4-a2c2-96a0f042307e) - Created
[0m16:42:50.038456 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1b-4899-1e86-9c26-d55eb2b16f56) - Created
[0m16:42:50.039475 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1b-4899-1203-94b6-bede51bc2c92) - Created
[0m16:42:50.171150 [debug] [Thread-3 (]: SQL status: OK in 0.970 seconds
[0m16:42:50.172664 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1b-4880-1898-8976-6108a91123c3, command-id=01f0dd1b-4899-1955-af89-39abb86ddd0d) - Closing
[0m16:42:50.201622 [debug] [Thread-3 (]: Writing injected SQL for node "model.patreon_analytics.metricflow_time_spine"
[0m16:42:50.202848 [debug] [Thread-3 (]: Began executing node model.patreon_analytics.metricflow_time_spine
[0m16:42:50.212649 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m16:42:50.403545 [debug] [Thread-3 (]: Writing runtime sql for node "model.patreon_analytics.metricflow_time_spine"
[0m16:42:50.405242 [debug] [Thread-3 (]: Using databricks connection "model.patreon_analytics.metricflow_time_spine"
[0m16:42:50.406067 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.metricflow_time_spine"} */

  
    
        create or replace table `patreon_dev`.`analytics`.`metricflow_time_spine`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      with days as (
    
    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
    
    

    )

    select *
    from unioned
    where generated_number <= 4017
    order by generated_number



),

all_periods as (

    select (
        timestampadd(day, (row_number() over (order by 1) - 1), cast('2020-01-01' as timestamp))
    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2030-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(timestampadd(year, -1, d.date_day) as date) as prior_year_date_day,
        cast(timestampadd(day, -364, d.date_day) as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(timestampadd(day, -1, d.date_day) as date) as prior_date_day,
    cast(timestampadd(day, 1, d.date_day) as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    date_part('dayofweek', d.date_day) as day_of_week,
    date_part('dayofweek_iso', d.date_day) as day_of_week_iso,
    date_format(d.date_day, 'EEEE') as day_of_week_name,
    date_format(d.date_day, 'E') as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    dayofyear(d.date_day) as day_of_year,

    cast(date_trunc('week', d.date_day) as date) as week_start_date,
    cast(
        timestampadd(day, -1, timestampadd(week, 1, date_trunc('week', d.date_day)))
        as date) as week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_week_start_date,
    cast(
        timestampadd(day, -1, timestampadd(week, 1, date_trunc('week', d.prior_year_over_year_date_day)))
        as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(timestampadd(day, 6, cast(date_trunc('week', d.date_day) as date)) as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(timestampadd(day, 6, cast(date_trunc('week', d.prior_year_over_year_date_day) as date)) as date) as prior_year_iso_week_end_date,
    cast(date_part('week', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    date_format(d.date_day, 'MMMM')  as month_name,
    date_format(d.date_day, 'MMM')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(month, 1, date_trunc('month', d.date_day)))
        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(month, 1, date_trunc('month', d.prior_year_date_day)))
        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(quarter, 1, date_trunc('quarter', d.date_day)))
        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        timestampadd(day, -1, timestampadd(year, 1, date_trunc('year', d.date_day)))
        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


)

select
    date_day as date_day
from days
  
[0m16:42:51.058583 [debug] [Thread-5 (]: SQL status: OK in 1.750 seconds
[0m16:42:51.059203 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd1b-4899-1203-94b6-bede51bc2c92, command-id=01f0dd1b-48af-1511-bbaf-33ae082bb327) - Closing
[0m16:42:51.065058 [debug] [Thread-5 (]: Applying tags to relation None
[0m16:42:51.066855 [debug] [Thread-5 (]: On model.patreon_analytics.stg_engagement_events: Close
[0m16:42:51.067062 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1b-4899-1203-94b6-bede51bc2c92) - Closing
[0m16:42:51.068366 [debug] [Thread-6 (]: SQL status: OK in 1.760 seconds
[0m16:42:51.068874 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1b-4899-12a4-a2c2-96a0f042307e, command-id=01f0dd1b-48ae-18f2-9aaa-4b59d94b458c) - Closing
[0m16:42:51.069169 [debug] [Thread-6 (]: Applying tags to relation None
[0m16:42:51.083384 [debug] [Thread-4 (]: SQL status: OK in 1.770 seconds
[0m16:42:51.083968 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1b-4899-1e86-9c26-d55eb2b16f56, command-id=01f0dd1b-48af-11df-8832-bc3e3d84a230) - Closing
[0m16:42:51.084292 [debug] [Thread-4 (]: Applying tags to relation None
[0m16:42:51.228689 [debug] [Thread-6 (]: On model.patreon_analytics.stg_patrons: Close
[0m16:42:51.229447 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1b-4899-12a4-a2c2-96a0f042307e) - Closing
[0m16:42:51.467446 [debug] [Thread-4 (]: On model.patreon_analytics.stg_creators: Close
[0m16:42:51.468136 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1b-4899-1e86-9c26-d55eb2b16f56) - Closing
[0m16:42:51.632010 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d4b500>]}
[0m16:42:51.632287 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12edf4770>]}
[0m16:42:51.632473 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ed39e50>]}
[0m16:42:51.632823 [info ] [Thread-5 (]: 3 of 9 OK created sql view model analytics_staging.stg_engagement_events ....... [[32mOK[0m in 2.46s]
[0m16:42:51.633140 [info ] [Thread-6 (]: 4 of 9 OK created sql view model analytics_staging.stg_patrons ................. [[32mOK[0m in 2.46s]
[0m16:42:51.633496 [info ] [Thread-4 (]: 2 of 9 OK created sql view model analytics_staging.stg_creators ................ [[32mOK[0m in 2.46s]
[0m16:42:51.633838 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.stg_engagement_events
[0m16:42:51.634084 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_patrons
[0m16:42:51.634307 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_creators
[0m16:42:51.634535 [debug] [Thread-5 (]: Began running node model.patreon_analytics.stg_pledges
[0m16:42:51.634835 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_posts
[0m16:42:51.635074 [debug] [Thread-4 (]: Began running node model.patreon_analytics.stg_tiers
[0m16:42:51.635342 [info ] [Thread-5 (]: 5 of 9 START sql view model analytics_staging.stg_pledges ...................... [RUN]
[0m16:42:51.635815 [info ] [Thread-6 (]: 6 of 9 START sql view model analytics_staging.stg_posts ........................ [RUN]
[0m16:42:51.636138 [info ] [Thread-4 (]: 7 of 9 START sql view model analytics_staging.stg_tiers ........................ [RUN]
[0m16:42:51.636465 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_pledges) - Creating connection
[0m16:42:51.636737 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_posts) - Creating connection
[0m16:42:51.636966 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_tiers) - Creating connection
[0m16:42:51.637145 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_pledges'
[0m16:42:51.637308 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_posts'
[0m16:42:51.637465 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_tiers'
[0m16:42:51.637627 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.stg_pledges
[0m16:42:51.637782 [debug] [Thread-6 (]: Began compiling node model.patreon_analytics.stg_posts
[0m16:42:51.637929 [debug] [Thread-4 (]: Began compiling node model.patreon_analytics.stg_tiers
[0m16:42:51.640354 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.stg_pledges"
[0m16:42:51.643190 [debug] [Thread-6 (]: Writing injected SQL for node "model.patreon_analytics.stg_posts"
[0m16:42:51.644958 [debug] [Thread-4 (]: Writing injected SQL for node "model.patreon_analytics.stg_tiers"
[0m16:42:51.645541 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.stg_pledges
[0m16:42:51.647208 [debug] [Thread-5 (]: MATERIALIZING VIEW
[0m16:42:51.647786 [debug] [Thread-5 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_pledges`
[0m16:42:51.648117 [debug] [Thread-5 (]: Writing runtime sql for node "model.patreon_analytics.stg_pledges"
[0m16:42:51.648379 [debug] [Thread-6 (]: Began executing node model.patreon_analytics.stg_posts
[0m16:42:51.649924 [debug] [Thread-6 (]: MATERIALIZING VIEW
[0m16:42:51.650452 [debug] [Thread-6 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_posts`
[0m16:42:51.650749 [debug] [Thread-6 (]: Writing runtime sql for node "model.patreon_analytics.stg_posts"
[0m16:42:51.651106 [debug] [Thread-5 (]: Using databricks connection "model.patreon_analytics.stg_pledges"
[0m16:42:51.651349 [debug] [Thread-4 (]: Began executing node model.patreon_analytics.stg_tiers
[0m16:42:51.651606 [debug] [Thread-5 (]: On model.patreon_analytics.stg_pledges: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_pledges"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_pledges`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`pledges`
),

staged as (
    select
        pledge_id,
        patron_id,
        creator_id,
        tier_id,
        pledge_amount_usd,
        pledge_status,
        coalesce(is_first_pledge, false) as is_first_pledge,
        started_at,
        ended_at,
        pause_started_at,
        churn_reason,
        
        -- Derived fields
        date_trunc('month', started_at) as pledge_month,
        case 
            when pledge_status = 'active' then null
            else datediff(day, started_at, coalesce(ended_at, current_timestamp()))
        end as pledge_duration_days,
        
        -- Is currently active (for point-in-time analysis)
        case 
            when pledge_status = 'active' and pause_started_at is null then true
            else false
        end as is_currently_active,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m16:42:51.653192 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m16:42:51.653420 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m16:42:51.653587 [debug] [Thread-6 (]: Using databricks connection "model.patreon_analytics.stg_posts"
[0m16:42:51.654031 [debug] [Thread-4 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_tiers`
[0m16:42:51.654329 [debug] [Thread-6 (]: On model.patreon_analytics.stg_posts: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_posts"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_posts`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`posts`
),

staged as (
    select
        post_id,
        creator_id,
        title,
        post_type,
        access_level,
        minimum_tier_id,
        published_at,
        coalesce(is_pinned, false) as is_pinned,
        
        -- Derived fields
        date_trunc('month', published_at) as published_month,
        date_trunc('day', published_at) as published_date,
        
        -- Content categorization
        case 
            when access_level = 'public' then 'free'
            when access_level = 'patrons_only' then 'paywalled'
            when access_level = 'tier_specific' then 'premium'
            else 'unknown'
        end as content_access_type,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m16:42:51.654723 [debug] [Thread-4 (]: Writing runtime sql for node "model.patreon_analytics.stg_tiers"
[0m16:42:51.654977 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:42:51.655720 [debug] [Thread-4 (]: Using databricks connection "model.patreon_analytics.stg_tiers"
[0m16:42:51.656022 [debug] [Thread-4 (]: On model.patreon_analytics.stg_tiers: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_tiers"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_tiers`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`tiers`
),

staged as (
    select
        tier_id,
        creator_id,
        tier_name,
        tier_rank,
        price_usd,
        description,
        coalesce(is_active, true) as is_active,
        created_at,
        archived_at,
        
        -- Price bucket for analysis
        case 
            when price_usd <= 5 then 'micro'
            when price_usd <= 15 then 'standard'
            when price_usd <= 30 then 'premium'
            else 'whale'
        end as price_bucket,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m16:42:51.656277 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:42:52.342028 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1b-49f9-19fe-a942-273666362020) - Created
[0m16:42:52.349863 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1b-49fa-169c-827e-8ae06f0bf265) - Created
[0m16:42:52.357675 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1b-49fa-1b15-95c0-2c39ed1f7d42) - Created
[0m16:42:53.239075 [debug] [Thread-6 (]: SQL status: OK in 1.580 seconds
[0m16:42:53.240274 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1b-49f9-19fe-a942-273666362020, command-id=01f0dd1b-4a0e-107b-b282-109ae57feb04) - Closing
[0m16:42:53.240742 [debug] [Thread-6 (]: Applying tags to relation None
[0m16:42:53.241202 [debug] [Thread-6 (]: On model.patreon_analytics.stg_posts: Close
[0m16:42:53.241391 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1b-49f9-19fe-a942-273666362020) - Closing
[0m16:42:53.316752 [debug] [Thread-5 (]: SQL status: OK in 1.660 seconds
[0m16:42:53.317379 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd1b-49fa-1b15-95c0-2c39ed1f7d42, command-id=01f0dd1b-4a11-10cd-9652-5a186d3cc729) - Closing
[0m16:42:53.317727 [debug] [Thread-5 (]: Applying tags to relation None
[0m16:42:53.329493 [debug] [Thread-4 (]: SQL status: OK in 1.670 seconds
[0m16:42:53.330647 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0dd1b-49fa-169c-827e-8ae06f0bf265, command-id=01f0dd1b-4a0f-1503-b298-4f37775135aa) - Closing
[0m16:42:53.331059 [debug] [Thread-4 (]: Applying tags to relation None
[0m16:42:53.405256 [debug] [Thread-5 (]: On model.patreon_analytics.stg_pledges: Close
[0m16:42:53.406075 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1b-49fa-1b15-95c0-2c39ed1f7d42) - Closing
[0m16:42:53.578581 [debug] [Thread-4 (]: On model.patreon_analytics.stg_tiers: Close
[0m16:42:53.579279 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0dd1b-49fa-169c-827e-8ae06f0bf265) - Closing
[0m16:42:53.685050 [debug] [Thread-3 (]: SQL status: OK in 3.280 seconds
[0m16:42:53.685978 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0dd1b-4880-1898-8976-6108a91123c3, command-id=01f0dd1b-48e8-10e2-8a41-653167de7c49) - Closing
[0m16:42:53.687878 [debug] [Thread-3 (]: Applying tags to relation None
[0m16:42:53.738764 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12eeaff50>]}
[0m16:42:53.739072 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ee96840>]}
[0m16:42:53.739300 [debug] [Thread-3 (]: On model.patreon_analytics.metricflow_time_spine: Close
[0m16:42:53.739640 [info ] [Thread-6 (]: 6 of 9 OK created sql view model analytics_staging.stg_posts ................... [[32mOK[0m in 2.10s]
[0m16:42:53.740197 [info ] [Thread-5 (]: 5 of 9 OK created sql view model analytics_staging.stg_pledges ................. [[32mOK[0m in 2.10s]
[0m16:42:53.740450 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0dd1b-4880-1898-8976-6108a91123c3) - Closing
[0m16:42:53.740702 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_posts
[0m16:42:53.740925 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.stg_pledges
[0m16:42:53.741275 [debug] [Thread-6 (]: Began running node model.patreon_analytics.stg_transactions
[0m16:42:53.741574 [info ] [Thread-6 (]: 8 of 9 START sql view model analytics_staging.stg_transactions ................. [RUN]
[0m16:42:53.888272 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ebc50a0>]}
[0m16:42:53.889082 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.stg_transactions) - Creating connection
[0m16:42:53.890351 [info ] [Thread-4 (]: 7 of 9 OK created sql view model analytics_staging.stg_tiers ................... [[32mOK[0m in 2.25s]
[0m16:42:53.890904 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e233230>]}
[0m16:42:53.891224 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.patreon_analytics.stg_transactions'
[0m16:42:53.891511 [debug] [Thread-4 (]: Finished running node model.patreon_analytics.stg_tiers
[0m16:42:53.891867 [info ] [Thread-3 (]: 1 of 9 OK created sql table model analytics.metricflow_time_spine .............. [[32mOK[0m in 4.72s]
[0m16:42:53.892076 [debug] [Thread-6 (]: Began compiling node model.patreon_analytics.stg_transactions
[0m16:42:53.892496 [debug] [Thread-3 (]: Finished running node model.patreon_analytics.metricflow_time_spine
[0m16:42:53.898353 [debug] [Thread-6 (]: Writing injected SQL for node "model.patreon_analytics.stg_transactions"
[0m16:42:53.899827 [debug] [Thread-6 (]: Began executing node model.patreon_analytics.stg_transactions
[0m16:42:53.901324 [debug] [Thread-6 (]: MATERIALIZING VIEW
[0m16:42:53.901869 [debug] [Thread-6 (]: Creating view `patreon_dev`.`analytics_staging`.`stg_transactions`
[0m16:42:53.902200 [debug] [Thread-6 (]: Writing runtime sql for node "model.patreon_analytics.stg_transactions"
[0m16:42:53.902562 [debug] [Thread-6 (]: Using databricks connection "model.patreon_analytics.stg_transactions"
[0m16:42:53.902807 [debug] [Thread-6 (]: On model.patreon_analytics.stg_transactions: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.stg_transactions"} */

  
  
  create or replace view `patreon_dev`.`analytics_staging`.`stg_transactions`
  
  as (
    with source as (
    select * from `patreon_dev`.`analytics_raw`.`transactions`
),

staged as (
    select
        transaction_id,
        pledge_id,
        patron_id,
        creator_id,
        transaction_type,
        transaction_status,
        gross_amount_usd,
        platform_fee_usd,
        processing_fee_usd,
        net_amount_usd,
        payment_method,
        failure_reason,
        transaction_at,
        
        -- Derived fields
        date_trunc('month', transaction_at) as transaction_month,
        date_trunc('day', transaction_at) as transaction_date,
        
        -- Fee analysis
        case 
            when gross_amount_usd > 0 and transaction_status = 'succeeded'
            then round((platform_fee_usd / gross_amount_usd) * 100, 2)
            else null
        end as platform_fee_rate_pct,
        
        case 
            when gross_amount_usd > 0 and transaction_status = 'succeeded'
            then round((processing_fee_usd / gross_amount_usd) * 100, 2)
            else null
        end as processing_fee_rate_pct,
        
        -- Success flag
        case when transaction_status = 'succeeded' then 1 else 0 end as is_successful,
        
        current_timestamp() as _stg_loaded_at
        
    from source
)

select * from staged
  )

[0m16:42:53.903008 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:42:54.455315 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1b-4b3c-18fd-b1ca-a7ce5aee43e7) - Created
[0m16:42:55.269504 [debug] [Thread-6 (]: SQL status: OK in 1.370 seconds
[0m16:42:55.271871 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0dd1b-4b3c-18fd-b1ca-a7ce5aee43e7, command-id=01f0dd1b-4b51-140d-8066-bbf595e09ecb) - Closing
[0m16:42:55.272542 [debug] [Thread-6 (]: Applying tags to relation None
[0m16:42:55.273044 [debug] [Thread-6 (]: On model.patreon_analytics.stg_transactions: Close
[0m16:42:55.273239 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0dd1b-4b3c-18fd-b1ca-a7ce5aee43e7) - Closing
[0m16:42:55.423473 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ee8e1b0>]}
[0m16:42:55.424919 [info ] [Thread-6 (]: 8 of 9 OK created sql view model analytics_staging.stg_transactions ............ [[32mOK[0m in 1.68s]
[0m16:42:55.425981 [debug] [Thread-6 (]: Finished running node model.patreon_analytics.stg_transactions
[0m16:42:55.427005 [debug] [Thread-5 (]: Began running node model.patreon_analytics.fct_creator_monthly_performance
[0m16:42:55.427276 [info ] [Thread-5 (]: 9 of 9 START sql table model analytics_marts.fct_creator_monthly_performance ... [RUN]
[0m16:42:55.427634 [debug] [Thread-5 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.patreon_analytics.fct_creator_monthly_performance) - Creating connection
[0m16:42:55.427858 [debug] [Thread-5 (]: Acquiring new databricks connection 'model.patreon_analytics.fct_creator_monthly_performance'
[0m16:42:55.428042 [debug] [Thread-5 (]: Began compiling node model.patreon_analytics.fct_creator_monthly_performance
[0m16:42:55.438723 [debug] [Thread-5 (]: Writing injected SQL for node "model.patreon_analytics.fct_creator_monthly_performance"
[0m16:42:55.440950 [debug] [Thread-5 (]: Began executing node model.patreon_analytics.fct_creator_monthly_performance
[0m16:42:55.442649 [debug] [Thread-5 (]: MATERIALIZING TABLE
[0m16:42:55.443385 [debug] [Thread-5 (]: Writing runtime sql for node "model.patreon_analytics.fct_creator_monthly_performance"
[0m16:42:55.444212 [debug] [Thread-5 (]: Using databricks connection "model.patreon_analytics.fct_creator_monthly_performance"
[0m16:42:55.444830 [debug] [Thread-5 (]: On model.patreon_analytics.fct_creator_monthly_performance: /* {"app": "dbt", "dbt_version": "1.11.0rc3", "dbt_databricks_version": "1.11.3", "databricks_sql_connector_version": "4.1.3", "profile_name": "patreon_databricks", "target_name": "dev", "node_id": "model.patreon_analytics.fct_creator_monthly_performance"} */

  
    
        create or replace table `patreon_dev`.`analytics_marts`.`fct_creator_monthly_performance`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      with creators as (
    select * from `patreon_dev`.`analytics_staging`.`stg_creators`
),

pledges as (
    select * from `patreon_dev`.`analytics_staging`.`stg_pledges`
),

transactions as (
    select * from `patreon_dev`.`analytics_staging`.`stg_transactions`
),

tiers as (
    select * from `patreon_dev`.`analytics_staging`.`stg_tiers`
),

posts as (
    select * from `patreon_dev`.`analytics_staging`.`stg_posts`
),

engagement as (
    select * from `patreon_dev`.`analytics_staging`.`stg_engagement_events`
),

-- Generate month spine from earliest pledge to current month
months as (
    select distinct date_trunc('month', transaction_at)::date as month_start_date
    from transactions
),

-- Create creator-month combinations
creator_months as (
    select 
        c.creator_id,
        m.month_start_date,
        md5(cast(concat(coalesce(cast(c.creator_id as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(m.month_start_date as string), '_dbt_utils_surrogate_key_null_')) as string)) as creator_month_key
    from creators c
    cross join months m
    where m.month_start_date >= date_trunc('month', c.first_pledge_received_at)
),

-- Monthly pledge metrics
pledge_metrics as (
    select
        p.creator_id,
        date_trunc('month', t.transaction_at)::date as month_start_date,
        
        -- Patron counts
        count(distinct p.patron_id) as total_patrons,
        count(distinct case when p.pledge_status = 'active' then p.patron_id end) as active_patrons,
        count(distinct case when p.is_first_pledge = true then p.patron_id end) as new_patrons,
        count(distinct case when p.pledge_status = 'churned' then p.patron_id end) as churned_patrons,
        
        -- MRR
        sum(case when p.pledge_status = 'active' then p.pledge_amount_usd else 0 end) as gross_mrr_usd,
        sum(case when p.is_first_pledge = true then p.pledge_amount_usd else 0 end) as new_mrr_usd,
        sum(case when p.pledge_status = 'churned' then p.pledge_amount_usd else 0 end) as churned_mrr_usd,
        
        -- Tier distribution
        count(distinct case when ti.tier_rank = 1 then p.patron_id end) as tier_1_patrons,
        count(distinct case when ti.tier_rank = 2 then p.patron_id end) as tier_2_patrons,
        count(distinct case when ti.tier_rank >= 3 then p.patron_id end) as tier_3_plus_patrons,
        
        -- ARPP
        avg(p.pledge_amount_usd) as avg_pledge_amount_usd
        
    from pledges p
    inner join transactions t on p.pledge_id = t.pledge_id
    left join tiers ti on p.tier_id = ti.tier_id
    group by 1, 2
),

-- Monthly revenue (actual collections)
revenue_metrics as (
    select
        creator_id,
        date_trunc('month', transaction_at)::date as month_start_date,
        
        sum(case when transaction_status = 'succeeded' then gross_amount_usd else 0 end) as gross_revenue_usd,
        sum(case when transaction_status = 'succeeded' then platform_fee_usd else 0 end) as platform_fees_usd,
        sum(case when transaction_status = 'succeeded' then processing_fee_usd else 0 end) as processing_fees_usd,
        sum(case when transaction_status = 'succeeded' then net_amount_usd else 0 end) as net_creator_earnings_usd,
        
        count(case when transaction_status = 'succeeded' then 1 end) as successful_transactions,
        count(case when transaction_status = 'failed' then 1 end) as failed_transactions,
        sum(case when transaction_status = 'failed' then gross_amount_usd else 0 end) as declined_amount_usd
        
    from transactions
    where transaction_type = 'pledge_payment'
    group by 1, 2
),

-- Monthly content metrics
content_metrics as (
    select
        creator_id,
        published_month as month_start_date,
        
        count(distinct post_id) as posts_published,
        count(distinct case when content_access_type = 'paywalled' then post_id end) as paywalled_posts,
        count(distinct case when content_access_type = 'free' then post_id end) as free_posts
        
    from posts
    group by 1, 2
),

-- Monthly engagement metrics
engagement_metrics as (
    select
        creator_id,
        event_month as month_start_date,
        
        count(case when event_type = 'view' then 1 end) as total_views,
        count(case when event_type = 'like' then 1 end) as total_likes,
        count(case when event_type = 'comment' then 1 end) as total_comments,
        count(distinct patron_id) as engaged_patrons,
        sum(engagement_weight) as total_engagement_score
        
    from engagement
    group by 1, 2
),

-- Previous month for growth calculations
lagged as (
    select
        creator_id,
        month_start_date,
        lag(gross_mrr_usd) over (partition by creator_id order by month_start_date) as prev_mrr,
        lag(active_patrons) over (partition by creator_id order by month_start_date) as prev_patrons
    from pledge_metrics
)

select
    cm.creator_month_key,
    cm.creator_id,
    cm.month_start_date,
    
    -- Creator attributes
    c.creator_name,
    c.category as creator_category,
    c.plan_type,
    c.country_code as creator_country,
    
    -- Patron metrics
    coalesce(pm.total_patrons, 0) as total_patrons,
    coalesce(pm.active_patrons, 0) as active_patrons,
    coalesce(pm.new_patrons, 0) as new_patrons,
    coalesce(pm.churned_patrons, 0) as churned_patrons,
    coalesce(pm.active_patrons, 0) - coalesce(l.prev_patrons, 0) as net_patron_change,
    
    case 
        when coalesce(l.prev_patrons, 0) > 0 
        then round((pm.churned_patrons * 100.0 / l.prev_patrons), 2)
        else 0 
    end as patron_churn_rate_pct,
    
    -- MRR metrics
    coalesce(pm.gross_mrr_usd, 0) as gross_mrr_usd,
    coalesce(pm.new_mrr_usd, 0) as new_mrr_usd,
    coalesce(pm.churned_mrr_usd, 0) as churned_mrr_usd,
    coalesce(pm.gross_mrr_usd, 0) - coalesce(l.prev_mrr, 0) as mrr_change_usd,
    
    case 
        when coalesce(l.prev_mrr, 0) > 0 
        then round(((pm.gross_mrr_usd - l.prev_mrr) * 100.0 / l.prev_mrr), 2)
        else null 
    end as mrr_growth_rate_pct,
    
    -- Revenue metrics
    coalesce(rm.gross_revenue_usd, 0) as gross_revenue_usd,
    coalesce(rm.platform_fees_usd, 0) as platform_fees_usd,
    coalesce(rm.processing_fees_usd, 0) as processing_fees_usd,
    coalesce(rm.net_creator_earnings_usd, 0) as net_creator_earnings_usd,
    
    case 
        when coalesce(pm.gross_mrr_usd, 0) > 0 
        then round((rm.gross_revenue_usd * 100.0 / pm.gross_mrr_usd), 2)
        else null 
    end as collection_rate_pct,
    
    -- Payment health
    coalesce(rm.successful_transactions, 0) as successful_transactions,
    coalesce(rm.failed_transactions, 0) as failed_transactions,
    coalesce(rm.declined_amount_usd, 0) as declined_amount_usd,
    
    case 
        when coalesce(rm.successful_transactions, 0) + coalesce(rm.failed_transactions, 0) > 0
        then round((rm.failed_transactions * 100.0 / (rm.successful_transactions + rm.failed_transactions)), 2)
        else 0 
    end as decline_rate_pct,
    
    -- Tier distribution
    coalesce(pm.tier_1_patrons, 0) as tier_1_patrons,
    coalesce(pm.tier_2_patrons, 0) as tier_2_patrons,
    coalesce(pm.tier_3_plus_patrons, 0) as tier_3_plus_patrons,
    coalesce(pm.avg_pledge_amount_usd, 0) as avg_pledge_amount_usd,
    
    -- Content metrics
    coalesce(cnt.posts_published, 0) as posts_published,
    coalesce(cnt.paywalled_posts, 0) as paywalled_posts,
    coalesce(cnt.free_posts, 0) as free_posts,
    
    -- Engagement metrics
    coalesce(eng.total_views, 0) as total_views,
    coalesce(eng.total_likes, 0) as total_likes,
    coalesce(eng.total_comments, 0) as total_comments,
    coalesce(eng.engaged_patrons, 0) as engaged_patrons,
    coalesce(eng.total_engagement_score, 0) as total_engagement_score,
    
    case 
        when coalesce(pm.active_patrons, 0) > 0 
        then round((eng.engaged_patrons * 100.0 / pm.active_patrons), 2)
        else null 
    end as patron_engagement_rate_pct,
    
    current_timestamp() as updated_at

from creator_months cm
left join creators c on cm.creator_id = c.creator_id
left join pledge_metrics pm on cm.creator_id = pm.creator_id and cm.month_start_date = pm.month_start_date
left join revenue_metrics rm on cm.creator_id = rm.creator_id and cm.month_start_date = rm.month_start_date
left join content_metrics cnt on cm.creator_id = cnt.creator_id and cm.month_start_date = cnt.month_start_date
left join engagement_metrics eng on cm.creator_id = eng.creator_id and cm.month_start_date = eng.month_start_date
left join lagged l on cm.creator_id = l.creator_id and cm.month_start_date = l.month_start_date
  
[0m16:42:55.445240 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m16:42:56.041790 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1b-4c2e-104e-94a2-848744243308) - Created
[0m16:43:05.135596 [debug] [Thread-5 (]: SQL status: OK in 9.690 seconds
[0m16:43:05.138942 [debug] [Thread-5 (]: Databricks adapter: Cursor(session-id=01f0dd1b-4c2e-104e-94a2-848744243308, command-id=01f0dd1b-4c43-122c-8cc7-62e86b96eade) - Closing
[0m16:43:05.359611 [debug] [Thread-5 (]: Applying tags to relation None
[0m16:43:05.363318 [debug] [Thread-5 (]: On model.patreon_analytics.fct_creator_monthly_performance: Close
[0m16:43:05.363597 [debug] [Thread-5 (]: Databricks adapter: Connection(session-id=01f0dd1b-4c2e-104e-94a2-848744243308) - Closing
[0m16:43:05.529412 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35e7f40e-ef28-426a-96a7-96ffa8d671ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12edf7590>]}
[0m16:43:05.531176 [info ] [Thread-5 (]: 9 of 9 OK created sql table model analytics_marts.fct_creator_monthly_performance  [[32mOK[0m in 10.10s]
[0m16:43:05.532158 [debug] [Thread-5 (]: Finished running node model.patreon_analytics.fct_creator_monthly_performance
[0m16:43:05.535926 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m16:43:05.536207 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:43:05.537077 [info ] [MainThread]: 
[0m16:43:05.537492 [info ] [MainThread]: Finished running 2 table models, 7 view models in 0 hours 0 minutes and 20.14 seconds (20.14s).
[0m16:43:05.539332 [debug] [MainThread]: Command end result
[0m16:43:05.605370 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/manifest.json
[0m16:43:05.609805 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/semantic_manifest.json
[0m16:43:05.614549 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/tarik/codeAlpine/sp-demo/patreon_dbt_project/target/run_results.json
[0m16:43:05.614846 [info ] [MainThread]: 
[0m16:43:05.615097 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:43:05.615263 [info ] [MainThread]: 
[0m16:43:05.615477 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=9
[0m16:43:05.619173 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 21.942377, "process_in_blocks": "0", "process_kernel_time": 0.545151, "process_mem_max_rss": "277823488", "process_out_blocks": "0", "process_user_time": 6.104302}
[0m16:43:05.619709 [debug] [MainThread]: Command `dbt run` succeeded at 16:43:05.619643 after 21.94 seconds
[0m16:43:05.620050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c24950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1180e3bf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12edaac90>]}
[0m16:43:05.620275 [debug] [MainThread]: Flushing usage events
[0m16:43:06.141963 [debug] [MainThread]: An error was encountered while trying to flush usage events
